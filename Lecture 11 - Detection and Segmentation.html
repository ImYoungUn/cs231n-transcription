<html>
	<head>
		<link rel="stylesheet" href="css/style.css">
		<script>
			var url="https://www.youtube.com/watch?v=nDPWywWRIRo&index=11&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv"
		</script>
		<script src="js/script.js"></script>
	</head>
	<body>
		<div class="lecture">
<h1>Lecture 11 - Detection and Segmentation</h1>
<br/>
<p>Hello, hi. <span onclick="goVideo(this)" class="timestamp">00:08</span></p>
<p>So I want to get started. <span onclick="goVideo(this)" class="timestamp">00:10</span></p>
<p>Welcome to CS 231N Lecture 11. <span onclick="goVideo(this)" class="timestamp">00:12</span></p>
<p>We're going to talk about today detection segmentation and a whole bunch of other really exciting topics around core computer vision tasks. <span onclick="goVideo(this)" class="timestamp">00:15</span></p>
<p>But as usual, a couple administrative notes. <span onclick="goVideo(this)" class="timestamp">00:23</span></p>
<p>So last time you obviously took the midterm, we didn't have lecture, hopefully that went okay for all of you but so we're going to work on grading the midterm this week, but as a reminder please don't make any public discussions about the midterm questions or answers or whatever until at least tomorrow because there are still some people taking makeup midterms today and throughout the rest of the week so we just ask you that you refrain from talking publicly about midterm questions. <span onclick="goVideo(this)" class="timestamp">00:25</span></p>
<p>Why don't you wait until Monday? <span onclick="goVideo(this)" class="timestamp">00:56</span></p>
<p>[laughing] Okay, great. <span onclick="goVideo(this)" class="timestamp">00:57</span></p>
<p>So we're also starting to work on midterm grading. <span onclick="goVideo(this)" class="timestamp">01:02</span></p>
<p>We'll get those back to you as soon as you can, as soon as we can. <span onclick="goVideo(this)" class="timestamp">01:04</span></p>
<p>We're also starting to work on grading assignment two so there's a lot of grading being done this week. <span onclick="goVideo(this)" class="timestamp">01:07</span></p>
<p>The TA's are pretty busy. <span onclick="goVideo(this)" class="timestamp">01:11</span></p>
<p>Also a reminder for you guys, hopefully you've been working hard on your projects now that most of you are done with the midterm so your project milestones will be due on Tuesday so any sort of last minute changes that you had in your projects, I know some people decided to switch projects after the proposal, some teams reshuffled a little bit, that's fine but your milestone should reflect the project that you're actually doing for the rest of the quarter. <span onclick="goVideo(this)" class="timestamp">01:14</span></p>
<p>So hopefully that's going out well. <span onclick="goVideo(this)" class="timestamp">01:37</span></p>
<p>I know there's been a lot of worry and stress on Piazza, wondering about assignment three. <span onclick="goVideo(this)" class="timestamp">01:39</span></p>
<p>So we're working on that as hard as we can but that's actually a bit of a new assignment, it's changing a bit from last year so it will be out as soon as possible, hopefully today or tomorrow. <span onclick="goVideo(this)" class="timestamp">01:43</span></p>
<p>Although we promise that whenever it comes out you'll have two weeks to finish it so try not to stress out about that too much. <span onclick="goVideo(this)" class="timestamp">01:53</span></p>
<p>But I'm pretty excited, I think assignment three will be really cool, has a lot of cool, it'll cover a lot of really cool material. <span onclick="goVideo(this)" class="timestamp">02:01</span></p>
<p>So another thing, last time in lecture we mentioned this thing called the Train Game which is this really cool thing we've been working on sort of as a side project a little bit. <span onclick="goVideo(this)" class="timestamp">02:09</span></p>
<p>So this is an interactive tool that you guys can go on and use to explore a little bit the process of tuning hyperparameters in practice so we hope that, so this is again totally not required for the course. <span onclick="goVideo(this)" class="timestamp">02:17</span></p>
<p>Totally optional, but if you do we will offer a small amount of extra credit for those of you who want to do well and participate on this. <span onclick="goVideo(this)" class="timestamp">02:30</span></p>
<p>And we'll send out exactly some more details later this afternoon on Piazza. <span onclick="goVideo(this)" class="timestamp">02:37</span></p>
<p>But just a bit of a demo for what exactly is this thing. <span onclick="goVideo(this)" class="timestamp">02:42</span></p>
<p>So you'll get to go in and we've changed the name from Train Game to HyperQuest because you're questing to solve, to find the best hyperparameters for your model so this is really cool, it'll be an interactive tool that you can use to explore the training of hyperparameters interactively in your browser. <span onclick="goVideo(this)" class="timestamp">02:45</span></p>
<p>So you'll login with your student ID and name. <span onclick="goVideo(this)" class="timestamp">03:01</span></p>
<p>You'll fill out a little survey with some of your experience on deep learning then you'll read some instructions. <span onclick="goVideo(this)" class="timestamp">03:04</span></p>
<p>So in this game you'll be shown some random data set on every trial. <span onclick="goVideo(this)" class="timestamp">03:11</span></p>
<p>This data set might be images or it might be vectors and your goal is to train a model by picking the right hyperparameters interactively to perform as well as you can on the validation set of this random data set. <span onclick="goVideo(this)" class="timestamp">03:16</span></p>
<p>And it'll sort of keep track of your performance over time and there'll be a leaderboard, it'll be really cool. <span onclick="goVideo(this)" class="timestamp">03:28</span></p>
<p>So every time you play the game, you'll get some statistics about your data set. <span onclick="goVideo(this)" class="timestamp">03:33</span></p>
<p>In this case we're doing a classification problem with 10 classes. <span onclick="goVideo(this)" class="timestamp">03:38</span></p>
<p>You can see down at the bottom you have these statistics about random data set, we have 10 classes. <span onclick="goVideo(this)" class="timestamp">03:43</span></p>
<p>The input data size is three by 32 by 32 so this is some image data set and we can see that in this case we have 8500 examples in the training set and 1500 examples in the validation set. <span onclick="goVideo(this)" class="timestamp">03:47</span></p>
<p>These are all random, they'll change a little bit every time. <span onclick="goVideo(this)" class="timestamp">03:58</span></p>
<p>Based on these data set statistics you'll make some choices on your initial learning rate, your initial network size, and your initial dropout rate. <span onclick="goVideo(this)" class="timestamp">04:01</span></p>
<p>Then you'll see a screen like this where it'll run one epoch with those chosen hyperparameters, show you on the right here you'll see two plots. <span onclick="goVideo(this)" class="timestamp">04:08</span></p>
<p>One is your training and validation loss for that first epoch. <span onclick="goVideo(this)" class="timestamp">04:17</span></p>
<p>Then you'll see your training and validation accuracy for that first epoch and based on the gaps that you see in these two graphs you can make choices interactively to change the learning rates and hyperparameters for the next epoch. <span onclick="goVideo(this)" class="timestamp">04:21</span></p>
<p>So then you can either choose to continue training with the current or changed hyperparameters, you can also stop training, or you can revert to go back to the previous checkpoint in case things got really messed up. <span onclick="goVideo(this)" class="timestamp">04:32</span></p>
<p>So then you'll get to make some choice, so here we'll decide to continue training and in this case you could go and set new learning rates and new hyperparameters for the next epoch of training. <span onclick="goVideo(this)" class="timestamp">04:43</span></p>
<p>You can also, kind of interesting here, you can actually grow the network interactively during training in this demo. <span onclick="goVideo(this)" class="timestamp">04:54</span></p>
<p>There's this cool trick from a couple recent papers where you can either take existing layers and make them wider or add new layers to the network in the middle of training while still maintaining the same function in the network so you can do that to increase the size of your network in the middle of training here which is kind of cool. <span onclick="goVideo(this)" class="timestamp">05:01</span></p>
<p>So then you'll make choices over several epochs and eventually your final validation accuracy will be recorded and we'll have some leaderboard that compares your score on that data set to some simple baseline models. <span onclick="goVideo(this)" class="timestamp">05:20</span></p>
<p>And depending on how well you do on this leaderboard we'll again offer some small amounts of extra credit for those of you who choose to participate. <span onclick="goVideo(this)" class="timestamp">05:33</span></p>
<p>So this is again, totally optional, but I think it can be a really cool learning experience for you guys to play around with and explore how hyperparameters affect the learning process. <span onclick="goVideo(this)" class="timestamp">05:39</span></p>
<p>Also, it's really useful for us. <span onclick="goVideo(this)" class="timestamp">05:49</span></p>
<p>You'll help science out by participating in this experiment. <span onclick="goVideo(this)" class="timestamp">05:50</span></p>
<p>We're pretty interested in seeing how people behave when they train neural networks so you'll be helping us out as well if you decide to play this. <span onclick="goVideo(this)" class="timestamp">05:54</span></p>
<p>But again, totally optional, up to you. <span onclick="goVideo(this)" class="timestamp">06:04</span></p>
<p>Any questions on that? <span onclick="goVideo(this)" class="timestamp">06:08</span></p>
<p>Hopefully at some point but it's. <span onclick="goVideo(this)" class="timestamp">06:15</span></p>
<p>So the question was will this be a paper or whatever eventually? <span onclick="goVideo(this)" class="timestamp">06:16</span></p>
<p>Hopefully but it's really early stages of this project so I can't make any promises but I hope so. <span onclick="goVideo(this)" class="timestamp">06:20</span></p>
<p>But I think it'll be really cool. <span onclick="goVideo(this)" class="timestamp">06:26</span></p>
<p>[laughing] Yeah, so the question is how can you add layers during training? <span onclick="goVideo(this)" class="timestamp">06:33</span></p>
<p>I don't really want to get into that right now but the paper to read is Net2Net by Ian Goodfellow's one of the authors and there's another paper from Microsoft called Network Morphism. <span onclick="goVideo(this)" class="timestamp">06:37</span></p>
<p>So if you read those two papers you can see how this works. <span onclick="goVideo(this)" class="timestamp">06:48</span></p>
<p>Okay, so last time, a bit of a reminder before we had the midterm last time we talked about recurrent neural networks. <span onclick="goVideo(this)" class="timestamp">06:53</span></p>
<p>We saw that recurrent neural networks can be used for different types of problems. <span onclick="goVideo(this)" class="timestamp">06:59</span></p>
<p>In addition to one to one we can do one to many, many to one, many to many. <span onclick="goVideo(this)" class="timestamp">07:03</span></p>
<p>We saw how this can apply to language modeling and we saw some cool examples of applying neural networks to model different sorts of languages at the character level and we sampled these artificial math and Shakespeare and C source code. <span onclick="goVideo(this)" class="timestamp">07:07</span></p>
<p>We also saw how similar things could be applied to image captioning by connecting a CNN feature extractor together with an RNN language model. <span onclick="goVideo(this)" class="timestamp">07:20</span></p>
<p>And we saw some really cool examples of that. <span onclick="goVideo(this)" class="timestamp">07:28</span></p>
<p>We also talked about the different types of RNN's. <span onclick="goVideo(this)" class="timestamp">07:31</span></p>
<p>We talked about this Vanilla RNN. <span onclick="goVideo(this)" class="timestamp">07:33</span></p>
<p>I also want to mention that this is sometimes called a Simple RNN or an Elman RNN so you'll see all of these different terms in literature. <span onclick="goVideo(this)" class="timestamp">07:36</span></p>
<p>We also talked about the Long Short Term Memory or LSTM. <span onclick="goVideo(this)" class="timestamp">07:42</span></p>
<p>And we talked about how the gradient, the LSTM has this crazy set of equations but it makes sense because it helps improve gradient flow during back propagation and helps this thing model more longer term dependencies in our sequences. <span onclick="goVideo(this)" class="timestamp">07:44</span></p>
<p>So today we're going to switch gears and talk about a whole bunch of different exciting tasks. <span onclick="goVideo(this)" class="timestamp">07:59</span></p>
<p>We're going to talk about, so so far we've been talking about mostly the image classification problem. <span onclick="goVideo(this)" class="timestamp">08:03</span></p>
<p>Today we're going to talk about various types of other computer vision tasks where you actually want to go in and say things about the spatial pixels inside your images so we'll see segmentation, localization, detection, a couple other different computer vision tasks and how you can approach these with convolutional neural networks. <span onclick="goVideo(this)" class="timestamp">08:08</span></p>
<p>So as a bit of refresher, so far the main thing we've been talking about in this class is image classification so here we're going to have some input image come in. <span onclick="goVideo(this)" class="timestamp">08:25</span></p>
<p>That input image will go through some deep convolutional network, that network will give us some feature vector of maybe 4096 dimensions in the case of AlexNet RGB and then from that final feature vector we'll have some fully-connected, some final fully-connected layer that gives us 1000 numbers for the different class scores that we care about where 1000 is maybe the number of classes in ImageNet in this example. <span onclick="goVideo(this)" class="timestamp">08:33</span></p>
<p>And then at the end of the day what the network does is we input an image and then we output a single category label saying what is the content of this entire image as a whole. <span onclick="goVideo(this)" class="timestamp">08:55</span></p>
<p>But this is maybe the most basic possible task in computer vision and there's a whole bunch of other interesting types of tasks that we might want to solve using deep learning. <span onclick="goVideo(this)" class="timestamp">09:05</span></p>
<p>So today we're going to talk about several of these different tasks and step through each of these and see how they all work with deep learning. <span onclick="goVideo(this)" class="timestamp">09:14</span></p>
<p>So we'll talk about these more in detail about what each problem is as we get to it but this is kind of a summary slide that we'll talk first about semantic segmentation. <span onclick="goVideo(this)" class="timestamp">09:21</span></p>
<p>We'll talk about classification and localization, then we'll talk about object detection, and finally a couple brief words about instance segmentation. <span onclick="goVideo(this)" class="timestamp">09:31</span></p>
<p>So first is the problem of semantic segmentation. <span onclick="goVideo(this)" class="timestamp">09:39</span></p>
<p>In the problem of semantic segmentation, we want to input an image and then output a decision of a category for every pixel in that image so for every pixel in this, so this input image for example is this cat walking through the field, he's very cute. <span onclick="goVideo(this)" class="timestamp">09:44</span></p>
<p>And in the output we want to say for every pixel is that pixel a cat or grass or sky or trees or background or some other set of categories. <span onclick="goVideo(this)" class="timestamp">09:58</span></p>
<p>So we're going to have some set of categories just like we did in the image classification case but now rather than assigning a single category labeled to the entire image, we want to produce a category label for each pixel of the input image. <span onclick="goVideo(this)" class="timestamp">10:07</span></p>
<p>And this is called semantic segmentation. <span onclick="goVideo(this)" class="timestamp">10:19</span></p>
<p>So one interesting thing about semantic segmentation is that it does not differentiate instances so in this example on the right we have this image with two cows where they're standing right next to each other and when we're talking about semantic segmentation we're just labeling all the pixels independently for what is the category of that pixel. <span onclick="goVideo(this)" class="timestamp">10:22</span></p>
<p>So in the case like this where we have two cows right next to each other the output does not make any distinguishing, does not distinguish between these two cows. <span onclick="goVideo(this)" class="timestamp">10:39</span></p>
<p>Instead we just get a whole mass of pixels that are all labeled as cow. <span onclick="goVideo(this)" class="timestamp">10:48</span></p>
<p>So this is a bit of a shortcoming of semantic segmentation and we'll see how we can fix this later when we move to instance segmentation. <span onclick="goVideo(this)" class="timestamp">10:51</span></p>
<p>But at least for now we'll just talk about semantic segmentation first. <span onclick="goVideo(this)" class="timestamp">10:58</span></p>
<p>So you can imagine maybe using a class, so one potential approach for attacking semantic segmentation might be through classification. <span onclick="goVideo(this)" class="timestamp">11:04</span></p>
<p>So there's this, you could use this idea of a sliding window approach to semantic segmentation. <span onclick="goVideo(this)" class="timestamp">11:12</span></p>
<p>So you might imagine that we take our input image and we break it up into many many small, tiny local crops of the image so in this example we've taken maybe three crops from around the head of this cow and then you could imagine taking each of those crops and now treating this as a classification problem. <span onclick="goVideo(this)" class="timestamp">11:17</span></p>
<p>Saying for this crop, what is the category of the central pixel of the crop? <span onclick="goVideo(this)" class="timestamp">11:36</span></p>
<p>And then we could use all the same machinery that we've developed for classifying entire images but now just apply it on crops rather than on the entire image. <span onclick="goVideo(this)" class="timestamp">11:41</span></p>
<p>And this would probably work to some extent but it's probably not a very good idea. <span onclick="goVideo(this)" class="timestamp">11:51</span></p>
<p>So this would end up being super super computationally expensive because we want to label every pixel in the image, we would need a separate crop for every pixel in that image and this would be super super expensive to run forward and backward passes through. <span onclick="goVideo(this)" class="timestamp">11:56</span></p>
<p>And moreover, we're actually, if you think about this we can actually share computation between different patches so if you're trying to classify two patches that are right next to each other and actually overlap then the convolutional features of those patches will end up going through the same convolutional layers and we can actually share a lot of the computation when applying this to separate passes or when applying this type of approach to separate patches in the image. <span onclick="goVideo(this)" class="timestamp">12:10</span></p>
<p>So this is actually a terrible idea and nobody does this and you should probably not do this but it's at least the first thing you might think of if you were trying to think about semantic segmentation. <span onclick="goVideo(this)" class="timestamp">12:37</span></p>
<p>Then the next idea that works a bit better is this idea of a fully convolutional network right. <span onclick="goVideo(this)" class="timestamp">12:48</span></p>
<p>So rather than extracting individual patches from the image and classifying these patches independently, we can imagine just having our network be a whole giant stack of convolutional layers with no fully connected layers or anything so in this case we just have a bunch of convolutional layers that are all maybe three by three with zero padding or something like that so that each convolutional layer preserves the spatial size of the input and now if we pass our image through a whole stack of these convolutional layers, then the final convolutional layer could just output a tensor of something by C by H by W where C is the number of categories that we care about and you could see this tensor as just giving our classification scores for every pixel in the input image at every location in the input image. <span onclick="goVideo(this)" class="timestamp">12:53</span></p>
<p>And we could compute this all at once with just some giant stack of convolutional layers. <span onclick="goVideo(this)" class="timestamp">13:38</span></p>
<p>And then you could imagine training this thing by putting a classification loss at every pixel of this output, taking an average over those pixels in space, and just training this kind of network through normal, regular back propagation. <span onclick="goVideo(this)" class="timestamp">13:43</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">13:55</span></p>
<p>Oh, the question is how do you develop training data for this? <span onclick="goVideo(this)" class="timestamp">13:58</span></p>
<p>It's very expensive right. <span onclick="goVideo(this)" class="timestamp">14:01</span></p>
<p>So the training data for this would be we need to label every pixel in those input images so there's tools that people sometimes have online where you can go in and sort of draw contours around the objects and then fill in regions but in general getting this kind of training data is very expensive. <span onclick="goVideo(this)" class="timestamp">14:02</span></p>
<p>Yeah, the question is what is the loss function? <span onclick="goVideo(this)" class="timestamp">14:29</span></p>
<p>So here since we're making a classification decision per pixel then we put a cross entropy loss on every pixel of the output. <span onclick="goVideo(this)" class="timestamp">14:31</span></p>
<p>So we have the ground truth category label for every pixel in the output, then we compute across entropy loss between every pixel in the output and the ground truth pixels and then take either a sum or an average over space and then sum or average over the mini-batch. <span onclick="goVideo(this)" class="timestamp">14:39</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">14:52</span></p>
<p>Yeah, yeah. <span onclick="goVideo(this)" class="timestamp">15:18</span></p>
<p>Yeah, the question is do we assume that we know the categories? <span onclick="goVideo(this)" class="timestamp">15:24</span></p>
<p>So yes, we do assume that we know the categories up front so this is just like the image classification case. <span onclick="goVideo(this)" class="timestamp">15:28</span></p>
<p>So an image classification we know at the start of training based on our data set that maybe there's 10 or 20 or 100 or 1000 classes that we care about for this data set and then here we are fixed to that set of classes that are fixed for the data set. <span onclick="goVideo(this)" class="timestamp">15:33</span></p>
<p>So this model is relatively simple and you can imagine this working reasonably well assuming that you tuned all the hyperparameters right but it's kind of a problem right. <span onclick="goVideo(this)" class="timestamp">15:51</span></p>
<p>So in this setup, since we're applying a bunch of convolutions that are all keeping the same spatial size of the input image, this would be super super expensive right. <span onclick="goVideo(this)" class="timestamp">16:00</span></p>
<p>If you wanted to do convolutions that maybe have 64 or 128 or 256 channels for those convolutional filters which is pretty common in a lot of these networks, then running those convolutions on this high resolution input image over a sequence of layers would be extremely computationally expensive and would take a ton of memory. <span onclick="goVideo(this)" class="timestamp">16:09</span></p>
<p>So in practice, you don't usually see networks with this architecture. <span onclick="goVideo(this)" class="timestamp">16:27</span></p>
<p>Instead you tend to see networks that look something like this where we have some downsampling and then some upsampling of the feature map inside the image. <span onclick="goVideo(this)" class="timestamp">16:31</span></p>
<p>So rather than doing all the convolutions of the full spatial resolution of the image, we'll maybe go through a small number of convolutional layers at the original resolution then downsample that feature map using something like max pooling or strided convolutions and sort of downsample, downsample, so we have convolutions in downsampling and convolutions in downsampling that look much like a lot of the classification networks that you see but now the difference is that rather than transitioning to a fully connected layer like you might do in an image classification setup, instead we want to increase the spatial resolution of our predictions in the second half of the network so that our output image can now be the same size as our input image and this ends up being much more computationally efficient because you can make the network very deep and work at a lower spatial resolution for many of the layers at the inside of the network. <span onclick="goVideo(this)" class="timestamp">16:40</span></p>
<p>So we've already seen examples of downsampling when it comes to convolutional networks. <span onclick="goVideo(this)" class="timestamp">17:29</span></p>
<p>We've seen that you can do strided convolutions or various types of pooling to reduce the spatial size of the image inside a network but we haven't really talked about upsampling and the question you might be wondering is what are these upsampling layers actually look like inside the network? <span onclick="goVideo(this)" class="timestamp">17:36</span></p>
<p>And what are our strategies for increasing the size of a feature map inside the network? <span onclick="goVideo(this)" class="timestamp">17:51</span></p>
<p>Sorry, was there a question in the back? <span onclick="goVideo(this)" class="timestamp">17:55</span></p>
<p>Yeah, so the question is how do we upsample? <span onclick="goVideo(this)" class="timestamp">18:07</span></p>
<p>And the answer is that's the topic of the next couple slides. <span onclick="goVideo(this)" class="timestamp">18:09</span></p>
<p>[laughing] So one strategy for upsampling is something like unpooling so we have this notion of pooling to downsample so we talked about average pooling or max pooling so when we talked about average pooling we're kind of taking a spatial average within a receptive field of each pooling region. <span onclick="goVideo(this)" class="timestamp">18:11</span></p>
<p>One kind of analog for upsampling is this idea of nearest neighbor unpooling. <span onclick="goVideo(this)" class="timestamp">18:30</span></p>
<p>So here on the left we see this example of nearest neighbor unpooling where our input is maybe some two by two grid and our output is a four by four grid and now in our output we've done a two by two stride two nearest neighbor unpooling or upsampling where we've just duplicated that element for every point in our two by two receptive field of the unpooling region. <span onclick="goVideo(this)" class="timestamp">18:34</span></p>
<p>Another thing you might see is this bed of nails unpooling or bed of nails upsampling where you'll just take, again we have a two by two receptive field for our unpooling regions and then you'll take the, in this case you make it all zeros except for one element of the unpooling region so in this case we've taken all of our inputs and always put them in the upper left hand corner of this unpooling region and everything else is zeros. <span onclick="goVideo(this)" class="timestamp">18:56</span></p>
<p>And this is kind of like a bed of nails because the zeros are very flat, then you've got these things poking up for the values at these various non-zero regions. <span onclick="goVideo(this)" class="timestamp">19:23</span></p>
<p>Another thing that you see sometimes which was alluded to by the question a minute ago is this idea of max unpooling so in a lot of these networks they tend to be symmetrical where we have a downsampling portion of the network and then an upsampling portion of the network with a symmetry between those two portions of the network. <span onclick="goVideo(this)" class="timestamp">19:33</span></p>
<p>So sometimes what you'll see is this idea of max unpooling where for each unpooling, for each upsampling layer, it is associated with one of the pooling layers in the first half of the network and now in the first half, in the downsampling when we do max pooling we'll actually remember which element of the receptive field during max pooling was used to do the max pooling and now when we go through the rest of the network then we'll do something that looks like this bed of nails upsampling except rather than always putting the elements in the same position, instead we'll stick it into the position that was used in the corresponding max pooling step earlier in the network. <span onclick="goVideo(this)" class="timestamp">19:52</span></p>
<p>I'm not sure if that explanation was clear but hopefully the picture makes sense. <span onclick="goVideo(this)" class="timestamp">20:33</span></p>
<p>Yeah, so then you just end up filling the rest with zeros. <span onclick="goVideo(this)" class="timestamp">20:39</span></p>
<p>So then you fill the rest with zeros and then you stick the elements from the low resolution patch up into the high resolution patch at the points where the max pooling took place at the corresponding max pooling there. <span onclick="goVideo(this)" class="timestamp">20:42</span></p>
<p>Okay, so that's kind of an interesting idea. <span onclick="goVideo(this)" class="timestamp">20:56</span></p>
<p>Sorry, question? <span onclick="goVideo(this)" class="timestamp">21:00</span></p>
<p>Oh yeah, so the question is why is this a good idea? <span onclick="goVideo(this)" class="timestamp">21:08</span></p>
<p>Why might this matter? <span onclick="goVideo(this)" class="timestamp">21:10</span></p>
<p>So the idea is that when we're doing semantic segmentation we want our predictions to be pixel perfect right. <span onclick="goVideo(this)" class="timestamp">21:11</span></p>
<p>We kind of want to get those sharp boundaries and those tiny details in our predictive segmentation so now if you're doing this max pooling, there's this sort of heterogeneity that's happening inside the feature map due to the max pooling where from the low resolution image you don't know, you're sort of losing spatial information in some sense by you don't know where that feature vector came from in the local receptive field after max pooling. <span onclick="goVideo(this)" class="timestamp">21:16</span></p>
<p>So if you actually unpool by putting the vector in the same slot you might think that that might help us handle these fine details a little bit better and help us preserve some of that spatial information that was lost during max pooling. <span onclick="goVideo(this)" class="timestamp">21:45</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">21:59</span></p>
<p>The question is does this make things easier for back prop? <span onclick="goVideo(this)" class="timestamp">22:10</span></p>
<p>Yeah, I guess, I don't think it changes the back prop dynamics too much because storing these indices is not a huge computational overhead. <span onclick="goVideo(this)" class="timestamp">22:13</span></p>
<p>They're pretty small in comparison to everything else. <span onclick="goVideo(this)" class="timestamp">22:21</span></p>
<p>So another thing that you'll see sometimes is this idea of transpose convolution. <span onclick="goVideo(this)" class="timestamp">22:24</span></p>
<p>So transpose convolution, so for these various types of unpooling that we just talked about, these bed of nails, this nearest neighbor, this max unpooling, all of these are kind of a fixed function, they're not really learning exactly how to do the upsampling so if you think about something like strided convolution, strided convolution is kind of like a learnable layer that learns the way that the network wants to perform downsampling at that layer. <span onclick="goVideo(this)" class="timestamp">22:29</span></p>
<p>And by analogy with that there's this type of layer called a transpose convolution that lets us do kind of learnable upsampling. <span onclick="goVideo(this)" class="timestamp">22:54</span></p>
<p>So it will both upsample the feature map and learn some weights about how it wants to do that upsampling. <span onclick="goVideo(this)" class="timestamp">23:02</span></p>
<p>And this is really just another type of convolution so to see how this works remember how a normal three by three stride one pad one convolution would work. <span onclick="goVideo(this)" class="timestamp">23:08</span></p>
<p>That for this kind of normal convolution that we've seen many times now in this class, our input might by four by four, our output might be four by four, and now we'll have this three by three kernel and we'll take an inner product between, we'll plop down that kernel at the corner of the image, take an inner product, and that inner product will give us the value and the activation in the upper left hand corner of our output. <span onclick="goVideo(this)" class="timestamp">23:16</span></p>
<p>And we'll repeat this for every receptive field in the image. <span onclick="goVideo(this)" class="timestamp">23:35</span></p>
<p>Now if we talk about strided convolution then strided convolution ends up looking pretty similar. <span onclick="goVideo(this)" class="timestamp">23:39</span></p>
<p>However, our input is maybe a four by four region and our output is a two by two region. <span onclick="goVideo(this)" class="timestamp">23:44</span></p>
<p>But we still have this idea of taking, of there being some three by three filter or kernel that we plop down in the corner of the image, take an inner product and use that to compute a value of the activation and the output. <span onclick="goVideo(this)" class="timestamp">23:49</span></p>
<p>But now with strided convolution the idea is that we're moving that, rather than popping down that filter at every possible point in the input, instead we're going to move the filter by two pixels in the input every time we move the filter by one pixel, every time we move by one pixel in the output. <span onclick="goVideo(this)" class="timestamp">24:00</span></p>
<p>Right so this stride of two gives us a ratio between how much do we move in the input versus how much do we move in the output. <span onclick="goVideo(this)" class="timestamp">24:16</span></p>
<p>So when you do a strided convolution with stride two this ends up downsampling the image or the feature map by a factor of two in kind of a learnable way. <span onclick="goVideo(this)" class="timestamp">24:23</span></p>
<p>And now a transpose convolution is sort of the opposite in a way so here our input will be a two by two region and our output will be a four by four region. <span onclick="goVideo(this)" class="timestamp">24:32</span></p>
<p>But now the operation that we perform with transpose convolution is a little bit different. <span onclick="goVideo(this)" class="timestamp">24:42</span></p>
<p>Now so rather than taking an inner product instead what we're going to do is we're going to take the value of our input feature map at that upper left hand corner and that'll be some scalar value in the upper left hand corner. <span onclick="goVideo(this)" class="timestamp">24:46</span></p>
<p>We're going to multiply the filter by that scalar value and then copy those values over to this three by three region in the output so rather than taking an inner product with our filter and the input, instead our input gives weights that we will use to weight the filter and then our output will be weighted copies of the filter that are weighted by the values in the input. <span onclick="goVideo(this)" class="timestamp">25:00</span></p>
<p>And now we can do this sort of same ratio trick in order to upsample so now when we move one pixel in the input now we can plop our filter down two pixels away in the output and it's the same trick that now the blue pixel in the input is some scalar value and we'll take that scalar value, multiply it by the values in the filter, and copy those weighted filter values into this new region in the output. <span onclick="goVideo(this)" class="timestamp">25:24</span></p>
<p>The tricky part is that sometimes these receptive fields in the output can overlap now and now when these receptive fields in the output overlap we just sum the results in the output. <span onclick="goVideo(this)" class="timestamp">25:49</span></p>
<p>So then you can imagine repeating this everywhere and repeating this process everywhere and this ends up doing sort of a learnable upsampling where we use these learned convolutional filter weights to upsample the image and increase the spatial size. <span onclick="goVideo(this)" class="timestamp">26:00</span></p>
<p>By the way, you'll see this operation go by a lot of different names in literature. <span onclick="goVideo(this)" class="timestamp">26:15</span></p>
<p>Sometimes this gets called things like deconvolution which I think is kind of a bad name but you'll see it out there in papers so from a signal processing perspective deconvolution means the inverse operation to convolution which this is not however you'll frequently see this type of layer called a deconvolution layer in some deep learning papers so be aware of that, watch out for that terminology. <span onclick="goVideo(this)" class="timestamp">26:19</span></p>
<p>You'll also sometimes see this called upconvolution which is kind of a cute name. <span onclick="goVideo(this)" class="timestamp">26:44</span></p>
<p>Sometimes it gets called fractionally strided convolution because if we think of the stride as the ratio in step between the input and the output then now this is something like a stride one half convolution because of this ratio of one to two between steps in the input and steps in the output. <span onclick="goVideo(this)" class="timestamp">26:48</span></p>
<p>This also sometimes gets called a backwards strided convolution because if you think about it and work through the math this ends up being the same, the forward pass of a transpose convolution ends up being the same mathematical operation as the backwards pass in a normal convolution so you might have to take my word for it, that might not be super obvious when you first look at this but that's kind of a neat fact so you'll sometimes see that name as well. <span onclick="goVideo(this)" class="timestamp">27:04</span></p>
<p>And as maybe a bit of a more concrete example of what this looks like I think it's maybe a little easier to see in one dimension so if we imagine, so here we're doing a three by three transpose convolution in one dimension. <span onclick="goVideo(this)" class="timestamp">27:28</span></p>
<p>Sorry, not three by three, a three by one transpose convolution in one dimension. <span onclick="goVideo(this)" class="timestamp">27:41</span></p>
<p>So our filter here is just three numbers. <span onclick="goVideo(this)" class="timestamp">27:46</span></p>
<p>Our input is two numbers and now you can see that in our output we've taken the values in the input, used them to weight the values of the filter and plopped down those weighted filters in the output with a stride of two and now where these receptive fields overlap in the output then we sum. <span onclick="goVideo(this)" class="timestamp">27:48</span></p>
<p>So you might be wondering, this is kind of a funny name. <span onclick="goVideo(this)" class="timestamp">28:03</span></p>
<p>Where does the name transpose convolution come from and why is that actually my preferred name for this operation? <span onclick="goVideo(this)" class="timestamp">28:06</span></p>
<p>So that comes from this kind of neat interpretation of convolution. <span onclick="goVideo(this)" class="timestamp">28:12</span></p>
<p>So it turns out that any time you do convolution you can always write convolution as a matrix multiplication. <span onclick="goVideo(this)" class="timestamp">28:15</span></p>
<p>So again, this is kind of easier to see with a one-dimensional example but here we've got some weight. <span onclick="goVideo(this)" class="timestamp">28:21</span></p>
<p>So we're doing a one-dimensional convolution of a weight vector x which has three elements, and an input vector, a vector, which has four elements, A, B, C, D. <span onclick="goVideo(this)" class="timestamp">28:29</span></p>
<p>So here we're doing a three by one convolution with stride one and you can see that we can frame this whole operation as a matrix multiplication where we take our convolutional kernel x and turn it into some matrix capital X which contains copies of that convolutional kernel that are offset by different regions. <span onclick="goVideo(this)" class="timestamp">28:38</span></p>
<p>And now we can take this giant weight matrix X and do a matrix vector multiplication between x and our input a and this just produces the same result as convolution. <span onclick="goVideo(this)" class="timestamp">28:59</span></p>
<p>And now with transpose convolution means that we're going to take this same weight matrix but now we're going to multiply by the transpose of that same weight matrix. <span onclick="goVideo(this)" class="timestamp">29:09</span></p>
<p>So here you can see the same example for this stride one convolution on the left and the corresponding stride one transpose convolution on the right. <span onclick="goVideo(this)" class="timestamp">29:17</span></p>
<p>And if you work through the details you'll see that when it comes to stride one, a stride one transpose convolution also ends up being a stride one normal convolution so there's a little bit of details in the way that the border and the padding are handled but it's fundamentally the same operation. <span onclick="goVideo(this)" class="timestamp">29:26</span></p>
<p>But now things look different when you talk about a stride of two. <span onclick="goVideo(this)" class="timestamp">29:42</span></p>
<p>So again, here on the left we can take a stride two convolution and write out this stride two convolution as a matrix multiplication. <span onclick="goVideo(this)" class="timestamp">29:45</span></p>
<p>And now the corresponding transpose convolution is no longer a convolution so if you look through this weight matrix and think about how convolutions end up getting represented in this way then now this transposed matrix for the stride two convolution is something fundamentally different from the original normal convolution operation so that's kind of the reasoning behind the name and that's why I think that's kind of the nicest name to call this operation by. <span onclick="goVideo(this)" class="timestamp">29:54</span></p>
<p>Sorry, was there a question? <span onclick="goVideo(this)" class="timestamp">30:20</span></p>
<p>Sorry? <span onclick="goVideo(this)" class="timestamp">30:27</span></p>
<p>It's very possible there's a typo in the slide so please point out on Piazza and I'll fix it but I hope the idea was clear. <span onclick="goVideo(this)" class="timestamp">30:29</span></p>
<p>Is there another question? <span onclick="goVideo(this)" class="timestamp">30:36</span></p>
<p>Okay, thank you [laughing]. <span onclick="goVideo(this)" class="timestamp">30:38</span></p>
<p>Yeah, so, oh no lots of questions. <span onclick="goVideo(this)" class="timestamp">30:40</span></p>
<p>Yeah, so the issue is why do we sum and not average? <span onclick="goVideo(this)" class="timestamp">30:53</span></p>
<p>So the reason we sum is due to this transpose convolution formula zone so that's the reason why we sum but you're right that you actually, this is kind of a problem that the magnitudes will actually vary in the output depending on how many receptive fields were in the output. <span onclick="goVideo(this)" class="timestamp">30:56</span></p>
<p>So actually in practice this is something that people started to point out very recently and somewhat switched away from this stride, so using three by three stride two transpose convolution upsampling can sometimes produce some checkerboard artifacts in the output exactly due to that problem. <span onclick="goVideo(this)" class="timestamp">31:11</span></p>
<p>So what I've seen in a couple more recent papers is maybe to use four by four stride two or two by two stride two transpose convolution for upsampling and that helps alleviate that problem a little bit. <span onclick="goVideo(this)" class="timestamp">31:26</span></p>
<p>Yeah, so the question is what is a stride half convolution and where does that terminology come from? <span onclick="goVideo(this)" class="timestamp">31:46</span></p>
<p>I think that was from my paper. <span onclick="goVideo(this)" class="timestamp">31:52</span></p>
<p>So that was actually, yes that was definitely this. <span onclick="goVideo(this)" class="timestamp">31:53</span></p>
<p>So at the time I was writing that paper I was kind of into the name fractionally strided convolution but after thinking about it a bit more I think transpose convolution is probably the right name. <span onclick="goVideo(this)" class="timestamp">31:56</span></p>
<p>So then this idea of semantic segmentation actually ends up being pretty natural. <span onclick="goVideo(this)" class="timestamp">32:07</span></p>
<p>You just have this giant convolutional network with downsampling and upsampling inside the network and now our downsampling will be by strided convolution or pooling, our upsampling will be by transpose convolution or various types of unpooling or upsampling and we can train this whole thing end to end with back propagation using this cross entropy loss over every pixel. <span onclick="goVideo(this)" class="timestamp">32:13</span></p>
<p>So this is actually pretty cool that we can take a lot of the same machinery that we already learned for image classification and now just apply it very easily to extend to new types of problems so that's super cool. <span onclick="goVideo(this)" class="timestamp">32:33</span></p>
<p>So the next task that I want to talk about is this idea of classification plus localization. <span onclick="goVideo(this)" class="timestamp">32:46</span></p>
<p>So we've talked about image classification a lot where we want to just assign a category label to the input image but sometimes you might want to know a little bit more about the image. <span onclick="goVideo(this)" class="timestamp">32:52</span></p>
<p>In addition to predicting what the category is, in this case the cat, you might also want to know where is that object in the image? <span onclick="goVideo(this)" class="timestamp">33:01</span></p>
<p>So in addition to predicting the category label cat, you might also want to draw a bounding box around the region of the cat in that image. <span onclick="goVideo(this)" class="timestamp">33:09</span></p>
<p>And classification plus localization, the distinction here between this and object detection is that in the localization scenario you assume ahead of time that you know there's exactly one object in the image that you're looking for or maybe more than one but you know ahead of time that we're going to make some classification decision about this image and we're going to produce exactly one bounding box that's going to tell us where that object is located in the image so we sometimes call that task classification plus localization. <span onclick="goVideo(this)" class="timestamp">33:17</span></p>
<p>And again, we can reuse a lot of the same machinery that we've already learned from image classification in order to tackle this problem. <span onclick="goVideo(this)" class="timestamp">33:47</span></p>
<p>So kind of a basic architecture for this problem looks something like this. <span onclick="goVideo(this)" class="timestamp">33:53</span></p>
<p>So again, we have our input image, we feed our input image through some giant convolutional network, this is Alex, this is AlexNet for example, which will give us some final vector summarizing the content of the image. <span onclick="goVideo(this)" class="timestamp">33:58</span></p>
<p>Then just like before we'll have some fully connected layer that goes from that final vector to our class scores. <span onclick="goVideo(this)" class="timestamp">34:09</span></p>
<p>But now we'll also have another fully connected layer that goes from that vector to four numbers. <span onclick="goVideo(this)" class="timestamp">34:15</span></p>
<p>Where the four numbers are something like the height, the width, and the x and y positions of that bounding box. <span onclick="goVideo(this)" class="timestamp">34:21</span></p>
<p>And now our network will produce these two different outputs, one is this set of class scores, and the other are these four numbers giving the coordinates of the bounding box in the input image. <span onclick="goVideo(this)" class="timestamp">34:28</span></p>
<p>And now during training time, when we train this network we'll actually have two losses so in this scenario we're sort of assuming a fully supervised setting so we assume that each of our training images is annotated with both a category label and also a ground truth bounding box for that category in the image. <span onclick="goVideo(this)" class="timestamp">34:39</span></p>
<p>So now we have two loss functions. <span onclick="goVideo(this)" class="timestamp">34:55</span></p>
<p>We have our favorite softmax loss that we compute using the ground truth category label and the predicted class scores, and we also have some kind of loss that gives us some measure of dissimilarity between our predicted coordinates for the bounding box and our actual coordinates for the bounding box. <span onclick="goVideo(this)" class="timestamp">34:57</span></p>
<p>So one very simple thing is to just take an L2 loss between those two and that's kind of the simplest thing that you'll see in practice although sometimes people play around with this and maybe use L1 or smooth L1 or they parametrize the bounding box a little bit differently but the idea is always the same, that you have some regression loss between your predicted bounding box coordinates and the ground truth bounding box coordinates. <span onclick="goVideo(this)" class="timestamp">35:13</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">35:35</span></p>
<p>Sorry, go ahead. <span onclick="goVideo(this)" class="timestamp">35:38</span></p>
<p>So the question is, is this a good idea to do all at the same time? <span onclick="goVideo(this)" class="timestamp">35:49</span></p>
<p>Like what happens if you misclassify, should you even look at the box coordinates? <span onclick="goVideo(this)" class="timestamp">35:52</span></p>
<p>So sometimes people get fancy with it, so in general it works okay. <span onclick="goVideo(this)" class="timestamp">35:55</span></p>
<p>It's not a big problem, you can actually train a network to do both of these things at the same time and it'll figure it out but sometimes things can get tricky in terms of misclassification so sometimes what you'll see for example is that rather than predicting a single box you might make predictions like a separate prediction of the box for each category and then only apply loss to the predicted box corresponding to the ground truth category. <span onclick="goVideo(this)" class="timestamp">35:59</span></p>
<p>So people do get a little bit fancy with these things that sometimes helps a bit in practice. <span onclick="goVideo(this)" class="timestamp">36:24</span></p>
<p>But at least this basic setup, it might not be perfect or it might not be optimal but it will work and it will do something. <span onclick="goVideo(this)" class="timestamp">36:28</span></p>
<p>Was there a question in the back? <span onclick="goVideo(this)" class="timestamp">36:34</span></p>
<p>Yeah, so that's the question is do these losses have different units, do they dominate the gradient? <span onclick="goVideo(this)" class="timestamp">36:41</span></p>
<p>So this is what we call a multi-task loss so whenever we're taking derivatives we always want to take derivative of a scalar with respect to our network parameters and use that derivative to take gradient steps. <span onclick="goVideo(this)" class="timestamp">36:46</span></p>
<p>But now we've got two scalars that we want to both minimize so what you tend to do in practice is have some additional hyperparameter that gives you some weighting between these two losses so you'll take a weighted sum of these two different loss functions to give our final scalar loss. <span onclick="goVideo(this)" class="timestamp">36:58</span></p>
<p>And then you'll take your gradients with respect to this weighted sum of the two losses. <span onclick="goVideo(this)" class="timestamp">37:11</span></p>
<p>And this ends up being really really tricky because this weighting parameter is a hyperparameter that you need to set but it's kind of different from some of the other hyperparameters that we've seen so far in the past right because this weighting hyperparameter actually changes the value of the loss function so one thing that you might often look at when you're trying to set hyperparameters is you might make different hyperparameter choices and see what happens to the loss under different choices of hyperparameters. <span onclick="goVideo(this)" class="timestamp">37:15</span></p>
<p>But in this case because the loss actually, because the hyperparameter affects the absolute value of the loss making those comparisons becomes kind of tricky. <span onclick="goVideo(this)" class="timestamp">37:43</span></p>
<p>So setting that hyperparameter is somewhat difficult. <span onclick="goVideo(this)" class="timestamp">37:51</span></p>
<p>And in practice, you kind of need to take it on a case by case basis for exactly the problem you're solving but my general strategy for this is to have some other metric of performance that you care about other than the actual loss value which then you actually use that final performance metric to make your cross validation choices rather than looking at the value of the loss to make those choices. <span onclick="goVideo(this)" class="timestamp">37:54</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">38:17</span></p>
<p>So the question is why do we do this all at once? <span onclick="goVideo(this)" class="timestamp">38:27</span></p>
<p>Why not do this separately? <span onclick="goVideo(this)" class="timestamp">38:30</span></p>
<p>Yeah, so the question is why don't we fix the big network and then just only learn separate fully connected layers for these two tasks? <span onclick="goVideo(this)" class="timestamp">38:38</span></p>
<p>People do do that sometimes and in fact that's probably the first thing you should try if you're faced with a situation like this but in general whenever you're doing transfer learning you always get better performance if you fine tune the whole system jointly because there's probably some mismatch between the features, if you train on ImageNet and then you use that network for your data set you're going to get better performance on your data set if you can also change the network. <span onclick="goVideo(this)" class="timestamp">38:45</span></p>
<p>But one trick you might see in practice sometimes is that you might freeze that network then train those two things separately until convergence and then after they converge then you go back and jointly fine tune the whole system. <span onclick="goVideo(this)" class="timestamp">39:09</span></p>
<p>So that's a trick that sometimes people do in practice in that situation. <span onclick="goVideo(this)" class="timestamp">39:20</span></p>
<p>And as I've kind of alluded to this big network is often a pre-trained network that is taken from ImageNet for example. <span onclick="goVideo(this)" class="timestamp">39:24</span></p>
<p>So a bit of an aside, this idea of predicting some fixed number of positions in the image can be applied to a lot of different problems beyond just classification plus localization. <span onclick="goVideo(this)" class="timestamp">39:31</span></p>
<p>One kind of cool example is human pose estimation. <span onclick="goVideo(this)" class="timestamp">39:41</span></p>
<p>So here we want to take an input image is a picture of a person. <span onclick="goVideo(this)" class="timestamp">39:44</span></p>
<p>We want to output the positions of the joints for that person and this actually allows the network to predict what is the pose of the human. <span onclick="goVideo(this)" class="timestamp">39:49</span></p>
<p>Where are his arms, where are his legs, stuff like that, and generally most people have the same number of joints. <span onclick="goVideo(this)" class="timestamp">39:56</span></p>
<p>That's a bit of a simplifying assumption, it might not always be true but it works for the network. <span onclick="goVideo(this)" class="timestamp">40:02</span></p>
<p>So for example one parameterization that you might see in some data sets is define a person's pose by 14 joint positions. <span onclick="goVideo(this)" class="timestamp">40:06</span></p>
<p>Their feet and their knees and their hips and something like that and now when we train the network then we're going to input this image of a person and now we're going to output 14 numbers in this case giving the x and y coordinates for each of those 14 joints. <span onclick="goVideo(this)" class="timestamp">40:15</span></p>
<p>And then you apply some kind of regression loss on each of those 14 different predicted points and just train this network with back propagation again. <span onclick="goVideo(this)" class="timestamp">40:30</span></p>
<p>Yeah, so you might see an L2 loss but people play around with other regression losses here as well. <span onclick="goVideo(this)" class="timestamp">40:40</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">40:46</span></p>
<p>So the question is what do I mean when I say regression loss? <span onclick="goVideo(this)" class="timestamp">40:50</span></p>
<p>So I mean something other than cross entropy or softmax right. <span onclick="goVideo(this)" class="timestamp">40:53</span></p>
<p>When I say regression loss I usually mean like an L2 Euclidean loss or an L1 loss or sometimes a smooth L1 loss. <span onclick="goVideo(this)" class="timestamp">40:57</span></p>
<p>But in general classification versus regression is whether your output is categorical or continuous so if you're expecting a categorical output like you ultimately want to make a classification decision over some fixed number of categories then you'll think about a cross entropy loss, softmax loss or these SVM margin type losses that we talked about already in the class. <span onclick="goVideo(this)" class="timestamp">41:04</span></p>
<p>But if your expected output is to be some continuous value, in this case the position of these points, then your output is continuous so you tend to use different types of losses in those situations. <span onclick="goVideo(this)" class="timestamp">41:25</span></p>
<p>Typically an L2, L1, different kinds of things there. <span onclick="goVideo(this)" class="timestamp">41:34</span></p>
<p>So sorry for not clarifying that earlier. <span onclick="goVideo(this)" class="timestamp">41:37</span></p>
<p>But the bigger point here is that for any time you know that you want to make some fixed number of outputs from your network, if you know for example. <span onclick="goVideo(this)" class="timestamp">41:41</span></p>
<p>Maybe you knew that you wanted to, you knew that you always are going to have pictures of a cat and a dog and you want to predict both the bounding box of the cat and the bounding box of the dog in that case you'd know that you have a fixed number of outputs for each input so you might imagine hooking up this type of regression classification plus localization framework for that problem as well. <span onclick="goVideo(this)" class="timestamp">41:51</span></p>
<p>So this idea of some fixed number of regression outputs can be applied to a lot of different problems including pose estimation. <span onclick="goVideo(this)" class="timestamp">42:10</span></p>
<p>So the next task that I want to talk about is object detection and this is a really meaty topic. <span onclick="goVideo(this)" class="timestamp">42:19</span></p>
<p>This is kind of a core problem in computer vision and you could probably teach a whole seminar class on just the history of object detection and various techniques applied there. <span onclick="goVideo(this)" class="timestamp">42:25</span></p>
<p>So I'll be relatively brief and try to go over the main big ideas of object detection plus deep learning that have been used in the last couple of years. <span onclick="goVideo(this)" class="timestamp">42:33</span></p>
<p>But the idea in object detection is that we again start with some fixed set of categories that we care about, maybe cats and dogs and fish or whatever but some fixed set of categories that we're interested in. <span onclick="goVideo(this)" class="timestamp">42:42</span></p>
<p>And now our task is that given our input image, every time one of those categories appears in the image, we want to draw a box around it and we want to predict the category of that box so this is different from classification plus localization because there might be a varying number of outputs for every input image. <span onclick="goVideo(this)" class="timestamp">42:55</span></p>
<p>You don't know ahead of time how many objects you expect to find in each image so that's, this ends up being a pretty challenging problem. <span onclick="goVideo(this)" class="timestamp">43:15</span></p>
<p>So we've seen graphs, so this is kind of interesting. <span onclick="goVideo(this)" class="timestamp">43:22</span></p>
<p>We've seen this graph many times of the ImageNet classification performance as a function of years and we saw that it just got better and better every year and there's been a similar trend with object detection because object detection has again been one of these core problems in computer vision that people have cared about for a very long time. <span onclick="goVideo(this)" class="timestamp">43:25</span></p>
<p>So this slide is due to Ross Girshick who's worked on this problem a lot and it shows the progression of object detection performance on this one particular data set called PASCAL VOC which has been relatively used for a long time in the object detection community. <span onclick="goVideo(this)" class="timestamp">43:44</span></p>
<p>And you can see that up until about 2012 performance on object detection started to stagnate and slow down a little bit and then in 2013 was when some of the first deep learning approaches to object detection came around and you could see that performance just shot up very quickly getting better and better year over year. <span onclick="goVideo(this)" class="timestamp">43:59</span></p>
<p>One thing you might notice is that this plot ends in 2015 and it's actually continued to go up since then so the current state of the art in this data set is well over 80 and in fact a lot of recent papers don't even report results on this data set anymore because it's considered too easy. <span onclick="goVideo(this)" class="timestamp">44:16</span></p>
<p>So it's a little bit hard to know, I'm not actually sure what is the state of the art number on this data set but it's off the top of this plot. <span onclick="goVideo(this)" class="timestamp">44:29</span></p>
<p>Sorry, did you have a question? <span onclick="goVideo(this)" class="timestamp">44:37</span></p>
<p>Nevermind. <span onclick="goVideo(this)" class="timestamp">44:40</span></p>
<p>Okay, so as I already said this is different from localization because there might be differing numbers of objects for each image. <span onclick="goVideo(this)" class="timestamp">44:42</span></p>
<p>So for example in this cat on the upper left there's only one object so we only need to predict four numbers but now for this image in the middle there's three animals there so we need our network to predict 12 numbers, four coordinates for each bounding box. <span onclick="goVideo(this)" class="timestamp">44:50</span></p>
<p>Or in this example of many many ducks then you want your network to predict a whole bunch of numbers. <span onclick="goVideo(this)" class="timestamp">45:05</span></p>
<p>Again, four numbers for each duck. <span onclick="goVideo(this)" class="timestamp">45:10</span></p>
<p>So this is quite different from object detection. <span onclick="goVideo(this)" class="timestamp">45:13</span></p>
<p>Sorry object detection is quite different from localization because in object detection you might have varying numbers of objects in the image and you don't know ahead of time how many you expect to find. <span onclick="goVideo(this)" class="timestamp">45:17</span></p>
<p>So as a result, it's kind of tricky if you want to think of object detection as a regression problem. <span onclick="goVideo(this)" class="timestamp">45:28</span></p>
<p>So instead, people tend to work, use kind of a different paradigm when thinking about object detection. <span onclick="goVideo(this)" class="timestamp">45:34</span></p>
<p>So one approach that's very common and has been used for a long time in computer vision is this idea of sliding window approaches to object detection. <span onclick="goVideo(this)" class="timestamp">45:40</span></p>
<p>So this is kind of similar to this idea of taking small patches and applying that for semantic segmentation and we can apply a similar idea for object detection. <span onclick="goVideo(this)" class="timestamp">45:49</span></p>
<p>So the ideas is that we'll take different crops from the input image, in this case we've got this crop in the lower left hand corner of our image and now we take that crop, feed it through our convolutional network and our convolutional network does a classification decision on that input crop. <span onclick="goVideo(this)" class="timestamp">45:59</span></p>
<p>It'll say that there's no dog here, there's no cat here, and then in addition to the categories that we care about we'll add an additional category called background and now our network can predict background in case it doesn't see any of the categories that we care about, so then when we take this crop from the lower left hand corner here then our network would hopefully predict background and say that no, there's no object here. <span onclick="goVideo(this)" class="timestamp">46:14</span></p>
<p>Now if we take a different crop then our network would predict dog yes, cat no, background no. <span onclick="goVideo(this)" class="timestamp">46:39</span></p>
<p>We take a different crop we get dog yes, cat no, background no. <span onclick="goVideo(this)" class="timestamp">46:44</span></p>
<p>Or a different crop, dog no, cat yes, background no. <span onclick="goVideo(this)" class="timestamp">46:47</span></p>
<p>Does anyone see a problem here? <span onclick="goVideo(this)" class="timestamp">46:51</span></p>
<p>Yeah, the question is how do you choose the crops? <span onclick="goVideo(this)" class="timestamp">47:00</span></p>
<p>So this is a huge problem right. <span onclick="goVideo(this)" class="timestamp">47:02</span></p>
<p>Because there could be any number of objects in this image, these objects could appear at any location in the image, these objects could appear at any size in the image, these objects could also appear at any aspect ratio in the image, so if you want to do kind of a brute force sliding window approach you'd end up having to test thousands, tens of thousands, many many many many different crops in order to tackle this problem with a brute force sliding window approach. <span onclick="goVideo(this)" class="timestamp">47:04</span></p>
<p>And in the case where every one of those crops is going to be fed through a giant convolutional network, this would be completely computationally intractable. <span onclick="goVideo(this)" class="timestamp">47:29</span></p>
<p>So in practice people don't ever do this sort of brute force sliding window approach for object detection using convolutional networks. <span onclick="goVideo(this)" class="timestamp">47:37</span></p>
<p>Instead there's this cool line of work called region proposals that comes from, this is not using deep learning typically. <span onclick="goVideo(this)" class="timestamp">47:47</span></p>
<p>These are slightly more traditional computer vision techniques but the idea is that a region proposal network kind of uses more traditional signal processing, image processing type things to make some list of proposals for where, so given an input image, a region proposal network will then give you something like a thousand boxes where an object might be present. <span onclick="goVideo(this)" class="timestamp">47:54</span></p>
<p>So you can imagine that maybe we do some local, we look for edges in the image and try to draw boxes that contain closed edges or something like that. <span onclick="goVideo(this)" class="timestamp">48:14</span></p>
<p>These various types of image processing approaches, but these region proposal networks will basically look for blobby regions in our input image and then give us some set of candidate proposal regions where objects might be potentially found. <span onclick="goVideo(this)" class="timestamp">48:22</span></p>
<p>And these are relatively fast-ish to run so one common example of a region proposal method that you might see is something called Selective Search which I think actually gives you 2000 region proposals, not the 1000 that it says on the slide. <span onclick="goVideo(this)" class="timestamp">48:35</span></p>
<p>So you kind of run this thing and then after about two seconds of turning on your CPU it'll spit out 2000 region proposals in the input image where objects are likely to be found so there'll be a lot of noise in those. <span onclick="goVideo(this)" class="timestamp">48:49</span></p>
<p>Most of them will not be true objects but there's a pretty high recall. <span onclick="goVideo(this)" class="timestamp">49:01</span></p>
<p>If there is an object in the image then it does tend to get covered by these region proposals from Selective Search. <span onclick="goVideo(this)" class="timestamp">49:05</span></p>
<p>So now rather than applying our classification network to every possible location and scale in the image instead what we can do is first apply one of these region proposal networks to get some set of proposal regions where objects are likely located and now apply a convolutional network for classification to each of these proposal regions and this will end up being much more computationally tractable than trying to do all possible locations and scales. <span onclick="goVideo(this)" class="timestamp">49:11</span></p>
<p>And this idea all came together in this paper called R-CNN from a few years ago that does exactly that. <span onclick="goVideo(this)" class="timestamp">49:36</span></p>
<p>So given our input image in this case we'll run some region proposal network to get our proposals, these are also sometimes called regions of interest or ROI's so again Selective Search gives you something like 2000 regions of interest. <span onclick="goVideo(this)" class="timestamp">49:45</span></p>
<p>Now one of the problems here is that these input, these regions in the input image could have different sizes but if we're going to run them all through a convolutional network our classification, our convolutional networks for classification all want images of the same input size typically due to the fully connected net layers and whatnot so we need to take each of these region proposals and warp them to that fixed square size that is expected as input to our downstream network. <span onclick="goVideo(this)" class="timestamp">49:59</span></p>
<p>So we'll crop out those region proposal, those regions corresponding to the region proposals, we'll warp them to that fixed size, and then we'll run each of them through a convolutional network which will then use in this case an SVM to make a classification decision for each of those, to predict categories for each of those crops. <span onclick="goVideo(this)" class="timestamp">50:26</span></p>
<p>And then I lost a slide. <span onclick="goVideo(this)" class="timestamp">50:48</span></p>
<p>But it'll also, not shown in the slide right now but in addition R-CNN also predicts a regression, like a correction to the bounding box in addition for each of these input region proposals because the problem is that your input region proposals are kind of generally in the right position for an object but they might not be perfect so in addition R-CNN will, in addition to category labels for each of these proposals, it'll also predict four numbers that are kind of an offset or a correction to the box that was predicted at the region proposal stage. <span onclick="goVideo(this)" class="timestamp">50:52</span></p>
<p>So then again, this is a multi-task loss and you would train this whole thing. <span onclick="goVideo(this)" class="timestamp">51:24</span></p>
<p>Sorry was there a question? <span onclick="goVideo(this)" class="timestamp">51:27</span></p>
<p>The question is how much does the change in aspect ratio impact accuracy? <span onclick="goVideo(this)" class="timestamp">51:35</span></p>
<p>It's a little bit hard to say. <span onclick="goVideo(this)" class="timestamp">51:40</span></p>
<p>I think there's some controlled experiments in some of these papers but I'm not sure I can give a generic answer to that. <span onclick="goVideo(this)" class="timestamp">51:41</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">51:48</span></p>
<p>The question is is it necessary for regions of interest to be rectangles? <span onclick="goVideo(this)" class="timestamp">51:53</span></p>
<p>So they typically are because it's tough to warp these non-region things but once you move to something like instant segmentation then you sometimes get proposals that are not rectangles. <span onclick="goVideo(this)" class="timestamp">51:56</span></p>
<p>If you actually do care about predicting things that are not rectangles. <span onclick="goVideo(this)" class="timestamp">52:08</span></p>
<p>Is there another question? <span onclick="goVideo(this)" class="timestamp">52:12</span></p>
<p>Yeah, so the question is are the region proposals learned so in R-CNN it's a traditional thing. <span onclick="goVideo(this)" class="timestamp">52:18</span></p>
<p>These are not learned, this is kind of some fixed algorithm that someone wrote down but we'll see in a couple minutes that we can actually, we've changed that a little bit in the last couple of years. <span onclick="goVideo(this)" class="timestamp">52:24</span></p>
<p>Is there another question? <span onclick="goVideo(this)" class="timestamp">52:33</span></p>
<p>The question is is the offset always inside the region of interest? <span onclick="goVideo(this)" class="timestamp">52:37</span></p>
<p>The answer is no, it doesn't have to be. <span onclick="goVideo(this)" class="timestamp">52:40</span></p>
<p>You might imagine that suppose the region of interest put a box around a person but missed the head then you could imagine the network inferring that oh this is a person but people usually have heads so the network showed the box should be a little bit higher. <span onclick="goVideo(this)" class="timestamp">52:42</span></p>
<p>So sometimes the final predicted boxes will be outside the region of interest. <span onclick="goVideo(this)" class="timestamp">52:55</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">52:59</span></p>
<p>Yeah. <span onclick="goVideo(this)" class="timestamp">53:08</span></p>
<p>Yeah the question is you have a lot of ROI's that don't correspond to true objects? <span onclick="goVideo(this)" class="timestamp">53:12</span></p>
<p>And like we said, in addition to the classes that you actually care about you add an additional background class so your class scores can also predict background to say that there was no object here. <span onclick="goVideo(this)" class="timestamp">53:15</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">53:26</span></p>
<p>Yeah, so the question is what kind of data do we need and yeah, this is fully supervised in the sense that our training data has each image, consists of images. <span onclick="goVideo(this)" class="timestamp">53:37</span></p>
<p>Each image has all the object categories marked with bounding boxes for each instance of that category. <span onclick="goVideo(this)" class="timestamp">53:47</span></p>
<p>There are definitely papers that try to approach this like oh what if you don't have the data. <span onclick="goVideo(this)" class="timestamp">53:53</span></p>
<p>What if you only have that data for some images? <span onclick="goVideo(this)" class="timestamp">53:56</span></p>
<p>Or what if that data is noisy but at least in the generic case you assume full supervision of all objects in the images at training time. <span onclick="goVideo(this)" class="timestamp">54:00</span></p>
<p>Okay, so I think we've kind of alluded to this but there's kind of a lot of problems with this R-CNN framework. <span onclick="goVideo(this)" class="timestamp">54:09</span></p>
<p>And actually if you look at the figure here on the right you can see that additional bounding box head so I'll put it back. <span onclick="goVideo(this)" class="timestamp">54:16</span></p>
<p>But this is kind of still computationally pretty expensive because if we've got 2000 region proposals, we're running each of those proposals independently, that can be pretty expensive. <span onclick="goVideo(this)" class="timestamp">54:21</span></p>
<p>There's also this question of relying on this fixed region proposal network, this fixed region proposals, we're not learning them so that's kind of a problem. <span onclick="goVideo(this)" class="timestamp">54:34</span></p>
<p>And just in practice it ends up being pretty slow so in the original implementation R-CNN would actually dump all the features to disk so it'd take hundreds of gigabytes of disk space to store all these features. <span onclick="goVideo(this)" class="timestamp">54:42</span></p>
<p>Then training would be super slow since you have to make all these different forward and backward passes through the image and it took something like 84 hours is one number they've recorded for training time so this is super super slow. <span onclick="goVideo(this)" class="timestamp">54:54</span></p>
<p>And now at test time it's also super slow, something like roughly 30 seconds minute per image because you need to run thousands of forward passes through the convolutional network for each of these region proposals so this ends up being pretty slow. <span onclick="goVideo(this)" class="timestamp">55:06</span></p>
<p>Thankfully we have fast R-CNN that fixed a lot of these problems so when we do fast R-CNN then it's going to look kind of the same. <span onclick="goVideo(this)" class="timestamp">55:18</span></p>
<p>We're going to start with our input image but now rather than processing each region of interest separately instead we're going to run the entire image through some convolutional layers all at once to give this high resolution convolutional feature map corresponding to the entire image. <span onclick="goVideo(this)" class="timestamp">55:27</span></p>
<p>And now we still are using some region proposals from some fixed thing like Selective Search but rather than cropping out the pixels of the image corresponding to the region proposals, instead we imagine projecting those region proposals onto this convolutional feature map and then taking crops from the convolutional feature map corresponding to each proposal rather than taking crops directly from the image. <span onclick="goVideo(this)" class="timestamp">55:41</span></p>
<p>And this allows us to reuse a lot of this expensive convolutional computation across the entire image when we have many many crops per image. <span onclick="goVideo(this)" class="timestamp">56:04</span></p>
<p>But again, if we have some fully connected layers downstream those fully connected layers are expecting some fixed-size input so now we need to do some reshaping of those crops from the convolutional feature map and they do that in a differentiable way using something they call an ROI pooling layer. <span onclick="goVideo(this)" class="timestamp">56:13</span></p>
<p>Once you have these warped crops from the convolutional feature map then you can run these things through some fully connected layers and predict your classification scores and your linear regression offsets to the bounding boxes. <span onclick="goVideo(this)" class="timestamp">56:31</span></p>
<p>And now when we train this thing then we again have a multi-task loss that trades off between these two constraints and during back propagation we can back prop through this entire thing and learn it all jointly. <span onclick="goVideo(this)" class="timestamp">56:45</span></p>
<p>This ROI pooling, it looks kind of like max pooling. <span onclick="goVideo(this)" class="timestamp">56:56</span></p>
<p>I don't really want to get into the details of that right now. <span onclick="goVideo(this)" class="timestamp">56:59</span></p>
<p>And in terms of speed if we look at R-CNN versus fast R-CNN versus this other model called SPP net which is kind of in between the two, then you can see that at training time fast R-CNN is something like 10 times faster to train because we're sharing all this computation between different feature maps. <span onclick="goVideo(this)" class="timestamp">57:03</span></p>
<p>And now at test time fast R-CNN is super fast and in fact fast R-CNN is so fast at test time that its computation time is actually dominated by computing region proposals. <span onclick="goVideo(this)" class="timestamp">57:20</span></p>
<p>So we said that computing these 2000 region proposals using Selective Search takes something like two seconds and now once we've got all these region proposals then because we're processing them all sort of in a shared way by sharing these expensive convolutions across the entire image that we can process all of these region proposals in less than a second altogether. <span onclick="goVideo(this)" class="timestamp">57:33</span></p>
<p>So fast R-CNN ends up being bottlenecked by just the computing of these region proposals. <span onclick="goVideo(this)" class="timestamp">57:53</span></p>
<p>Thankfully we've solved this problem with faster R-CNN. <span onclick="goVideo(this)" class="timestamp">57:59</span></p>
<p>So the idea in faster R-CNN is to just make, so the problem was the computing the region proposals using this fixed function was a bottleneck. <span onclick="goVideo(this)" class="timestamp">58:03</span></p>
<p>So instead we'll just make the network itself predict its own region proposals. <span onclick="goVideo(this)" class="timestamp">58:13</span></p>
<p>And so the way that this sort of works is that again, we take our input image, run the entire input image altogether through some convolutional layers to get some convolutional feature map representing the entire high resolution image and now there's a separate region proposal network which works on top of those convolutional features and predicts its own region proposals inside the network. <span onclick="goVideo(this)" class="timestamp">58:18</span></p>
<p>Now once we have those predicted region proposals then it looks just like fast R-CNN where now we take crops from those region proposals from the convolutional features, pass them up to the rest of the network. <span onclick="goVideo(this)" class="timestamp">58:39</span></p>
<p>And now we talked about multi-task losses and multi-task training networks to do multiple things at once. <span onclick="goVideo(this)" class="timestamp">58:50</span></p>
<p>Well now we're telling the network to do four things all at once so balancing out this four-way multi-task loss is kind of tricky. <span onclick="goVideo(this)" class="timestamp">58:57</span></p>
<p>But because the region proposal network needs to do two things: it needs to say for each potential proposal is it an object or not an object, it needs to actually regress the bounding box coordinates for each of those proposals, and now the final network at the end needs to do these two things again. <span onclick="goVideo(this)" class="timestamp">59:05</span></p>
<p>Make final classification decisions for what are the class scores for each of these proposals, and also have a second round of bounding box regression to again correct any errors that may have come from the region proposal stage. <span onclick="goVideo(this)" class="timestamp">59:21</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">59:34</span></p>
<p>So the question is that sometimes multi-task learning might be seen as regularization and are we getting that affect here? <span onclick="goVideo(this)" class="timestamp">59:45</span></p>
<p>I'm not sure if there's been super controlled studies on that but actually in the original version of the faster R-CNN paper they did a little bit of experimentation like what if we share the region proposal network, what if we don't share? <span onclick="goVideo(this)" class="timestamp">59:50</span></p>
<p>What if we learn separate convolutional networks for the region proposal network versus the classification network? <span onclick="goVideo(this)" class="timestamp">60:03</span></p>
<p>And I think there were minor differences but it wasn't a dramatic difference either way. <span onclick="goVideo(this)" class="timestamp">60:08</span></p>
<p>So in practice it's kind of nicer to only learn one because it's computationally cheaper. <span onclick="goVideo(this)" class="timestamp">60:12</span></p>
<p>Sorry, question? <span onclick="goVideo(this)" class="timestamp">60:18</span></p>
<p>Yeah the question is how do you train this region proposal network because you don't know, you don't have ground truth region proposals for the region proposal network. <span onclick="goVideo(this)" class="timestamp">60:33</span></p>
<p>So that's a little bit hairy. <span onclick="goVideo(this)" class="timestamp">60:41</span></p>
<p>I don't want to get too much into those details but the idea is that at any time you have a region proposal which has more than some threshold of overlap with any of the ground truth objects then you say that that is the positive region proposal and you should predict that as the region proposal and any potential proposal which has very low overlap with any ground truth objects should be predicted as a negative. <span onclick="goVideo(this)" class="timestamp">60:43</span></p>
<p>But there's a lot of dark magic hyperparameters in that process and that's a little bit hairy. <span onclick="goVideo(this)" class="timestamp">61:04</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">61:09</span></p>
<p>Yeah, so the question is what is the classification loss on the region proposal network and the answer is that it's making a binary, so I didn't want to get into too much of the details of that architecture 'cause it's a little bit hairy but it's making binary decisions. <span onclick="goVideo(this)" class="timestamp">61:15</span></p>
<p>So it has some set of potential regions that it's considering and it's making a binary decision for each one. <span onclick="goVideo(this)" class="timestamp">61:26</span></p>
<p>Is this an object or not an object? <span onclick="goVideo(this)" class="timestamp">61:32</span></p>
<p>So it's like a binary classification loss. <span onclick="goVideo(this)" class="timestamp">61:34</span></p>
<p>So once you train this thing then faster R-CNN ends up being pretty darn fast. <span onclick="goVideo(this)" class="timestamp">61:38</span></p>
<p>So now because we've eliminated this overhead from computing region proposals outside the network, now faster R-CNN ends up being very very fast compared to these other alternatives. <span onclick="goVideo(this)" class="timestamp">61:43</span></p>
<p>Also, one interesting thing is that because we're learning the region proposals here you might imagine maybe what if there was some mismatch between this fixed region proposal algorithm and my data? <span onclick="goVideo(this)" class="timestamp">61:53</span></p>
<p>So in this case once you're learning your own region proposals then you can overcome that mismatch if your region proposals are somewhat weird or different than other data sets. <span onclick="goVideo(this)" class="timestamp">62:05</span></p>
<p>So this whole family of R-CNN methods, R stands for region, so these are all region-based methods because there's some kind of region proposal and then we're doing some processing, some independent processing for each of those potential regions. <span onclick="goVideo(this)" class="timestamp">62:16</span></p>
<p>So this whole family of methods are called these region-based methods for object detection. <span onclick="goVideo(this)" class="timestamp">62:30</span></p>
<p>But there's another family of methods that you sometimes see for object detection which is sort of all feed forward in a single pass. <span onclick="goVideo(this)" class="timestamp">62:36</span></p>
<p>So one of these is YOLO for You Only Look Once. <span onclick="goVideo(this)" class="timestamp">62:43</span></p>
<p>And another is SSD for Single Shot Detection and these two came out somewhat around the same time. <span onclick="goVideo(this)" class="timestamp">62:48</span></p>
<p>But the idea is that rather than doing independent processing for each of these potential regions instead we want to try to treat this like a regression problem and just make all these predictions all at once with some big convolutional network. <span onclick="goVideo(this)" class="timestamp">62:54</span></p>
<p>So now given our input image you imagine dividing that input image into some coarse grid, in this case it's a seven by seven grid and now within each of those grid cells you imagine some set of base bounding boxes. <span onclick="goVideo(this)" class="timestamp">63:06</span></p>
<p>Here I've drawn three base bounding boxes like a tall one, a wide one, and a square one but in practice you would use more than three. <span onclick="goVideo(this)" class="timestamp">63:18</span></p>
<p>So now for each of these grid cells and for each of these base bounding boxes you want to predict several things. <span onclick="goVideo(this)" class="timestamp">63:25</span></p>
<p>One, you want to predict an offset off the base bounding box to predict what is the true location of the object off this base bounding box. <span onclick="goVideo(this)" class="timestamp">63:32</span></p>
<p>And you also want to predict classification scores so maybe a classification score for each of these base bounding boxes. <span onclick="goVideo(this)" class="timestamp">63:43</span></p>
<p>How likely is it that an object of this category appears in this bounding box. <span onclick="goVideo(this)" class="timestamp">63:51</span></p>
<p>So then at the end we end up predicting from our input image, we end up predicting this giant tensor of seven by seven grid by 5B + C. <span onclick="goVideo(this)" class="timestamp">63:55</span></p>
<p>So that's just where we have B base bounding boxes, we have five numbers for each giving our offset and our confidence for that base bounding box and C classification scores for our C categories. <span onclick="goVideo(this)" class="timestamp">64:04</span></p>
<p>So then we kind of see object detection as this input of an image, output of this three dimensional tensor and you can imagine just training this whole thing with a giant convolutional network. <span onclick="goVideo(this)" class="timestamp">64:16</span></p>
<p>And that's kind of what these single shot methods do where they just, and again matching the ground truth objects into these potential base boxes becomes a little bit hairy but that's what these methods do. <span onclick="goVideo(this)" class="timestamp">64:27</span></p>
<p>And by the way, the region proposal network that gets used in faster R-CNN ends up looking quite similar to these where they have some set of base bounding boxes over some gridded image, another region proposal network does some regression plus some classification. <span onclick="goVideo(this)" class="timestamp">64:41</span></p>
<p>So there's kind of some overlapping ideas here. <span onclick="goVideo(this)" class="timestamp">64:55</span></p>
<p>So in faster R-CNN we're kind of treating the object, the region proposal step as kind of this fixed end-to-end regression problem and then we do the separate per region processing but now with these single shot methods we only do that first step and just do all of our object detection with a single forward pass. <span onclick="goVideo(this)" class="timestamp">65:00</span></p>
<p>So object detection has a ton of different variables. <span onclick="goVideo(this)" class="timestamp">65:19</span></p>
<p>There could be different base networks like VGG, ResNet, we've seen different metastrategies for object detection including this faster R-CNN type region based family of methods, this single shot detection family of methods. <span onclick="goVideo(this)" class="timestamp">65:21</span></p>
<p>There's kind of a hybrid that I didn't talk about called R-FCN which is somewhat in between. <span onclick="goVideo(this)" class="timestamp">65:34</span></p>
<p>There's a lot of different hyperparameters like what is the image size, how many region proposals do you use. <span onclick="goVideo(this)" class="timestamp">65:38</span></p>
<p>And there's actually this really cool paper that will appear at CVPR this summer that does a really controlled experimentation around a lot of these different variables and tries to tell you how do these methods all perform under these different variables. <span onclick="goVideo(this)" class="timestamp">65:43</span></p>
<p>So if you're interested I'd encourage you to check it out but kind of one of the key takeaways is that the faster R-CNN style of region based methods tends to give higher accuracies but ends up being much slower than the single shot methods because the single shot methods don't require this per region processing. <span onclick="goVideo(this)" class="timestamp">65:56</span></p>
<p>But I encourage you to check out this paper if you want more details. <span onclick="goVideo(this)" class="timestamp">66:12</span></p>
<p>Also as a bit of aside, I had this fun paper with Andre a couple years ago that kind of combined object detection with image captioning and did this problem called dense captioning so now the idea is that rather than predicting a fixed category label for each region, instead we want to write a caption for each region. <span onclick="goVideo(this)" class="timestamp">66:17</span></p>
<p>And again, we had some data set that had this sort of data where we had a data set of regions together with captions and then we sort of trained this giant end-to-end model that just predicted these captions all jointly. <span onclick="goVideo(this)" class="timestamp">66:35</span></p>
<p>And this ends up looking somewhat like faster R-CNN where you have some region proposal stage then a bounding box, then some per region processing. <span onclick="goVideo(this)" class="timestamp">66:46</span></p>
<p>But rather than a SVM or a softmax loss instead those per region processing has a whole RNN language model that predicts a caption for each region. <span onclick="goVideo(this)" class="timestamp">66:53</span></p>
<p>So that ends up looking quite a bit like faster R-CNN. <span onclick="goVideo(this)" class="timestamp">67:03</span></p>
<p>There's a video here but I think we're running out of time so I'll skip it. <span onclick="goVideo(this)" class="timestamp">67:06</span></p>
<p>But the idea here is that once you have this, you can kind of tie together a lot of these ideas and if you have some new problem that you're interested in tackling like dense captioning, you can recycle a lot of the components that you've learned from other problems like object detection and image captioning and kind of stitch together one end-to-end network that produces the outputs that you care about for your problem. <span onclick="goVideo(this)" class="timestamp">67:11</span></p>
<p>So the last task that I want to talk about is this idea of instance segmentation. <span onclick="goVideo(this)" class="timestamp">67:32</span></p>
<p>So here instance segmentation is in some ways like the full problem We're given an input image and we want to predict one, the locations and identities of objects in that image similar to object detection, but rather than just predicting a bounding box for each of those objects, instead we want to predict a whole segmentation mask for each of those objects and predict which pixels in the input image corresponds to each object instance. <span onclick="goVideo(this)" class="timestamp">67:36</span></p>
<p>So this is kind of like a hybrid between semantic segmentation and object detection because like object detection we can handle multiple objects and we differentiate the identities of different instances so in this example since there are two dogs in the image and instance segmentation method actually distinguishes between the two dog instances and the output and kind of like semantic segmentation we have this pixel wise accuracy where for each of these objects we want to say which pixels belong to that object. <span onclick="goVideo(this)" class="timestamp">68:02</span></p>
<p>So there's been a lot of different methods that people have tackled, for instance segmentation as well, but the current state of the art is this new paper called Mask R-CNN that actually just came out on archive about a month ago so this is not yet published, this is like super fresh stuff. <span onclick="goVideo(this)" class="timestamp">68:32</span></p>
<p>And this ends up looking a lot like faster R-CNN. <span onclick="goVideo(this)" class="timestamp">68:49</span></p>
<p>So it has this multi-stage processing approach where we take our whole input image, that whole input image goes into some convolutional network and some learned region proposal network that's exactly the same as faster R-CNN and now once we have our learned region proposals then we project those proposals onto our convolutional feature map just like we did in fast and faster R-CNN. <span onclick="goVideo(this)" class="timestamp">68:52</span></p>
<p>But now rather than just making a classification and a bounding box for regression decision for each of those boxes we in addition want to predict a segmentation mask for each of those bounding box, for each of those region proposals. <span onclick="goVideo(this)" class="timestamp">69:14</span></p>
<p>So now it kind of looks like a mini, like a semantic segmentation problem inside each of the region proposals that we're getting from our region proposal network. <span onclick="goVideo(this)" class="timestamp">69:27</span></p>
<p>So now after we do this ROI aligning to warp our features corresponding to the region of proposal into the right shape, then we have two different branches. <span onclick="goVideo(this)" class="timestamp">69:36</span></p>
<p>One branch will come up that looks exact, and this first branch at the top looks just like faster R-CNN and it will predict classification scores telling us what is the category corresponding to that region of proposal or alternatively whether or not it's background. <span onclick="goVideo(this)" class="timestamp">69:45</span></p>
<p>And we'll also predict some bounding box coordinates that regressed off the region proposal coordinates. <span onclick="goVideo(this)" class="timestamp">69:59</span></p>
<p>And now in addition we'll have this branch at the bottom which looks basically like a semantic segmentation mini network which will classify for each pixel in that input region proposal whether or not it's an object so this mask R-CNN problem, this mask R-CNN architecture just kind of unifies all of these different problems that we've been talking about today into one nice jointly end-to-end trainable model. <span onclick="goVideo(this)" class="timestamp">70:04</span></p>
<p>And it's really cool and it actually works really really well so when you look at the examples in the paper they're kind of amazing. <span onclick="goVideo(this)" class="timestamp">70:29</span></p>
<p>They look kind of indistinguishable from ground truth. <span onclick="goVideo(this)" class="timestamp">70:36</span></p>
<p>So in this example on the left you can see that there are these two people standing in front of motorcycles, it's drawn the boxes around these people, it's also gone in and labeled all the pixels of those people and it's really small but actually in the background on that image on the left there's also a whole crowd of people standing very small in the background. <span onclick="goVideo(this)" class="timestamp">70:39</span></p>
<p>It's also drawn boxes around each of those and grabbed the pixels of each of those images. <span onclick="goVideo(this)" class="timestamp">70:54</span></p>
<p>And you can see that this is just, it ends up working really really well and it's a relatively simple addition on top of the existing faster R-CNN framework. <span onclick="goVideo(this)" class="timestamp">70:58</span></p>
<p>So I told you that mask R-CNN unifies everything we talked about today and it also does pose estimation by the way. <span onclick="goVideo(this)" class="timestamp">71:08</span></p>
<p>So we talked about, you can do pose estimation by predicting these joint coordinates for each of the joints of the person so you can do mask R-CNN to do joint object detection, pose estimation, and instance segmentation. <span onclick="goVideo(this)" class="timestamp">71:15</span></p>
<p>And the only addition we need to make is that for each of these region proposals we add an additional little branch that predicts these coordinates of the joints for the instance of the current region proposal. <span onclick="goVideo(this)" class="timestamp">71:29</span></p>
<p>So now this is just another loss, like another layer that we add, another head coming out of the network and an additional term in our multi-task loss. <span onclick="goVideo(this)" class="timestamp">71:42</span></p>
<p>But once we add this one little branch then you can do all of these different problems jointly and you get results looking something like this. <span onclick="goVideo(this)" class="timestamp">71:51</span></p>
<p>Where now this network, like a single feed forward network is deciding how many people are in the image, detecting where those people are, figuring out the pixels corresponding to each of those people and also drawing a skeleton estimating the pose of those people and this works really well even in crowded scenes like this classroom where there's a ton of people sitting and they all overlap each other and it just seems to work incredibly well. <span onclick="goVideo(this)" class="timestamp">71:59</span></p>
<p>And because it's built on the faster R-CNN framework it also runs relatively close to real time so this is running something like five frames per second on a GPU because this is all sort of done in the single forward pass of the network. <span onclick="goVideo(this)" class="timestamp">72:22</span></p>
<p>So this is again, a super new paper but I think that this will probably get a lot of attention in the coming months. <span onclick="goVideo(this)" class="timestamp">72:36</span></p>
<p>So just to recap, we've talked. <span onclick="goVideo(this)" class="timestamp">72:42</span></p>
<p>Sorry question? <span onclick="goVideo(this)" class="timestamp">72:45</span></p>
<p>The question is how much training data do you need? <span onclick="goVideo(this)" class="timestamp">72:53</span></p>
<p>So all of these instant segmentation results were trained on the Microsoft Coco data set so Microsoft Coco is roughly 200,000 training images. <span onclick="goVideo(this)" class="timestamp">72:55</span></p>
<p>It has 80 categories that it cares about so in each of those 200,000 training images it has all the instances of those 80 categories labeled. <span onclick="goVideo(this)" class="timestamp">73:05</span></p>
<p>So there's something like 200,000 images for training and there's something like I think an average of fivee or six instances per image. <span onclick="goVideo(this)" class="timestamp">73:14</span></p>
<p>So it actually is quite a lot of data. <span onclick="goVideo(this)" class="timestamp">73:21</span></p>
<p>And for Microsoft Coco for all the people in Microsoft Coco they also have all the joints annotated as well so this actually does have quite a lot of supervision at training time you're right, and actually is trained with quite a lot of data. <span onclick="goVideo(this)" class="timestamp">73:23</span></p>
<p>So I think one really interesting topic to study moving forward is that we kind of know that if you have a lot of data to solve some problem, at this point we're relatively confident that you can stitch up some convolutional network that can probably do a reasonable job at that problem but figuring out ways to get performance like this with less training data is a super interesting and active area of research and I think that's something people will be spending a lot of their efforts working on in the next few years. <span onclick="goVideo(this)" class="timestamp">73:36</span></p>
<p>So just to recap, today we had kind of a whirlwind tour of a whole bunch of different computer vision topics and we saw how a lot of the machinery that we built up from image classification can be applied relatively easily to tackle these different computer vision topics. <span onclick="goVideo(this)" class="timestamp">74:03</span></p>
<p>And next time we'll talk about, we'll have a really fun lecture on visualizing CNN features. <span onclick="goVideo(this)" class="timestamp">74:15</span></p>
<p>Well also talk about DeepDream and neural style transfer. <span onclick="goVideo(this)" class="timestamp">74:20</span></p>
		</div>
	</body>
</html>
