Lecture 7 - Training Neural Networks II

Okay, it's after 12, so I think we should get started. 00:08
Today we're going to kind of pick up where we left off last time. 00:15
Last time we talked about a lot of sort of tips and tricks involved in the nitty gritty details of training neural networks. 00:17
Today we'll pick up where we left off, and talk about a lot more of these sort of nitty gritty details about training these things. 00:23
As usual, a couple administrative notes before we get into the material. 00:30
As you all know, assignment one is already due. 00:35
Hopefully you all turned it in. 00:38
Did it go okay? 00:40
Was it not okay?
Rough sentiment? 00:42
Mostly okay. 00:45
Okay, that's good. 00:46
Awesome. 00:48
[laughs] We're in the process of grading those, so stay turned.
We're hoping to get grades back for those before A two is due. 00:53
Another reminder, that your project proposals are due tomorrow. 00:57
Actually, no, today at . 01:02
Make sure you send those in. 01:05
Details are on the website and on Piazza. 01:06
Also a reminder, assignment two is already out. 01:09
That'll be due a week from Thursday. 01:12
Historically, assignment two has been the longest one in the class, so if you haven't started already on assignment two, I'd recommend you take a look at that pretty soon. 01:15
Another reminder is that for assignment two, I think of a lot of you will be using Google Cloud. 01:27
Big reminder, make sure to stop your instances when you're not using them because whenever your instance is on, you get charged, and we only have so many coupons to distribute to you guys. 01:32
Anytime your instance is on, even if you're not SSH to it, even if you're not running things immediately in your Jupyter Notebook, any time that instance is on, you're going to be charged. 01:43
Just make sure that you explicitly stop your instances when you're not using them. 01:52
In this example, I've got a little screenshot of my dashboard on Google Cloud. 01:57
I need to go in there and explicitly go to the dropdown and click stop. 02:01
Just make sure that you do this when you're done working each day. 02:05
Another thing to remember is it's kind of up to you guys to keep track of your spending on Google Cloud. 02:09
In particular, instances that use GPUs are a lot more expensive than those with CPUs. 02:15
Rough order of magnitude, those GPU instances are around 90 cents to a dollar an hour. 02:21
Those are actually quite pricey. 02:26
The CPU instances are much cheaper. 02:28
The general strategy is that you probably want to make two instances, one with a GPU and one without, and then only use that GPU instance when you really need the GPU. 02:31
For example, on assignment two, most of the assignment, you should only need the CPU, so you should only use your CPU instance for that. 02:40
But then the final question, about TensorFlow or PyTorch, that will need a GPU. 02:47
This'll give you a little bit of practice with switching between multiple instances and only using that GPU when it's really necessary. 02:53
Again, just kind of watch your spending. 02:59
Try not to go too crazy on these things. 03:01
Any questions on the administrative stuff before we move on? 03:04
Question. 03:11
- [Student] How much RAM should we use? 03:12
- Question is how much RAM should we use? 03:14
I think eight or 16 gigs is probably good for everything that you need in this class. 03:16
As you scale up the number of CPUs and the number of RAM, you also end up spending more money. 03:22
If you stick with two or four CPUs and eight or 16 gigs of RAM, that should be plenty for all the homework-related stuff that you need to do. 03:27
As a quick recap, last time we talked about activation functions. 03:37
We talked about this whole zoo of different activation functions and some of their different properties. 03:40
We saw that the sigmoid, which used to be quite popular when training neural networks maybe 10 years ago or so, has this problem with vanishing gradients near the two ends of the activation function. 03:45
tanh has this similar sort of problem. 03:57
Kind of the general recommendation is that you probably want to stick with ReLU for most cases as sort of a default choice 'cause it tends to work well for a lot of different architectures. 04:00
We also talked about weight initialization. 04:09
Remember that up on the top, we have this idea that when you initialize your weights at the start of training, if those weights are initialized to be too small, then if you look at, then the activations will vanish as you go through the network because as you multiply by these small numbers over and over again, they'll all sort of decay to zero. 04:12
Then everything will be zero, learning won't happen, you'll be sad. 04:30
On the other hand, if you initialize your weights too big, then as you go through the network and multiply by your weight matrix over and over again, eventually they'll explode. 04:33
You'll be unhappy, there'll be no learning, it will be very bad. 04:41
But if you get that initialization just right, for example, using the Xavier initialization or the MSRA initialization, then you kind of keep a nice distribution of activations as you go through the network. 04:45
Remember that this kind of gets more and more important and more and more critical as your networks get deeper and deeper because as your network gets deeper, you're multiplying by those weight matrices over and over again with these more multiplicative terms. 04:59
We also talked last time about data preprocessing. 05:12
We talked about how it's pretty typical in conv nets to zero center and normalize your data so it has zero mean and unit variance. 05:14
I wanted to provide a little bit of extra intuition about why you might actually want to do this. 05:24
Imagine a simple setup where we have a binary classification problem where we want to draw a line to separate these red points from these blue points. 05:30
On the left, you have this idea where if those data points are kind of not normalized and not centered and far away from the origin, then we can still use a line to separate them, but now if that line wiggles just a little bit, then our classification is going to get totally destroyed. 05:40
That kind of means that in the example on the left, the loss function is now extremely sensitive to small perturbations in that linear classifier in our weight matrix. 05:55
We can still represent the same functions, but that might make learning quite difficult because, again, their loss is very sensitive to our parameter vector, whereas in the situation on the right, if you take that data cloud and you move it into the origin and you make it unit variance, then now, again, we can still classify that data quite well, but now as we wiggle that line a little bit, then our loss function is less sensitive to small perturbations in the parameter values. 06:07
That maybe makes optimization a little bit easier, as we'll see a little bit going forward. 06:36
By the way, this situation is not only in the linear classification case. 06:41
Inside a neural network, remember we kind of have these interleavings of these linear matrix multiplies, or convolutions, followed by non-linear activation functions. 06:47
If the input to some layer in your neural network is not centered or not zero mean, not unit variance, then again, small perturbations in the weight matrix of that layer of the network could cause large perturbations in the output of that layer, which, again, might make learning difficult. 06:59
This is kind of a little bit of extra intuition about why normalization might be important. 07:16
Because we have this intuition that normalization is so important, we talked about batch normalization, which is where we just add this additional layer inside our networks to just force all of the intermediate activations to be zero mean and unit variance. 07:22
I've sort of resummarized the batch normalization equations here with the shapes a little bit more explicitly. 07:36
Hopefully this can help you out when you're implementing this thing on assignment two. 07:41
But again, in batch normalization, we have this idea that in the forward pass, we use the statistics of the mini batch to compute a mean and a standard deviation, and then use those estimates to normalize our data on the forward pass. 07:45
Then we also reintroduce the scale and shift parameters to increase the expressivity of the layer. 07:59
You might want to refer back to this when working on assignment two. 08:06
We also talked last time a little bit about babysitting the learning process, how you should probably be looking at your loss curves during training. 08:10
Here's an example of some networks I was actually training over the weekend. 08:18
This is usually my setup when I'm working on these things. 08:24
On the left, I have some plot showing the training loss over time. 08:27
You can see it's kind of going down, which means my network is reducing the loss. 08:30
It's doing well. 08:34
On the right, there's this plot where the X axis is, again, time, or the iteration number, and the Y axis is my performance measure both on my training set and on my validation set. 08:36
You can see that as we go over time, then my training set performance goes up and up and up and up and up as my loss function goes down, but at some point, my validation set performance kind of plateaus. 08:48
This kind of suggests that maybe I'm overfitting in this situation. 08:59
Maybe I should have been trying to add additional regularization. 09:02
We also talked a bit last time about hyperparameter search. 09:06
All these networks have sort of a large zoo of hyperparameters. 09:09
It's pretty important to set them correctly. 09:13
We talked a little bit about grid search versus random search, and how random search is maybe a little bit nicer in theory because in the situation where your performance might be more sensitive, with respect to one hyperparameter than other, and random search lets you cover that space a little bit better. 09:15
We also talked about the idea of coarse to fine search, where when you're doing this hyperparameter optimization, probably you want to start with very wide ranges for your hyperparameters, only train for a couple iterations, and then based on those results, you kind of narrow in on the range of hyperparameters that are good. 09:31
Now, again, redo your search in a smaller range for more iterations. 09:48
You can kind of iterate this process to kind of hone in on the right region for hyperparameters. 09:52
But again, it's really important to, at the start, have a very coarse range to start with, where you want very, very wide ranges for all your hyperparameters. 09:57
Ideally, those ranges should be so wide that your network is kind of blowing up at either end of the range so that you know that you've searched a wide enough range for those things. 10:04
Question? 10:17
- [Student] How many [speaks too low to hear] optimize at once? 10:20
[speaks too low to hear] - The question is how many hyperparameters do we typically search at a time? 10:23
Here is two, but there's a lot more than two in these typical things. 10:35
It kind of depends on the exact model and the exact architecture, but because the number of possibilities is exponential in the number of hyperparameters, you can't really test too many at a time. 10:38
It also kind of depends on how many machines you have available. 10:48
It kind of varies from person to person and from experiment to experiment. 10:52
But generally, I try not to do this over more than maybe two or three or four at a time at most because, again, this exponential search just gets out of control. 10:56
Typically, learning rate is the really important one that you need to nail first. 11:05
Then other things, like regularization, like learning rate decay, model size, these other types of things tend to be a little bit less sensitive than learning rate. 11:10
Sometimes you might do kind of a block coordinate descent, where you go and find the good learning rate, then you go back and try to look at different model sizes. 11:20
This can help you cut down on the exponential search a little bit, but it's a little bit problem dependent on exactly which ones you should be searching over in which order. 11:27
More questions? 11:36
- [Student] [speaks too low to hear] Another parameter, but then changing that other parameter, two or three other parameters, makes it so that your learning rate or the ideal learning rate is still [speaks too low to hear]. 11:38
- Question is how often does it happen where when you change one hyperparameter, then the other, the optimal values of the other hyperparameters change? 11:57
That does happen sometimes, although for learning rates, that's typically less of a problem. 12:05
For learning rates, typically you want to get in a good range, and then set it maybe even a little bit lower than optimal, and let it go for a long time. 12:11
Then if you do that, combined with some of the fancier optimization strategies that we'll talk about today, then a lot of models tend to be a little bit less sensitive to learning rate once you get them in a good range. 12:18
Sorry, did you have a question in front, as well? 12:31
- [Student] [speaks too low to hear] - The question is what's wrong with having a small learning rate and increasing the number of epochs? 12:33
The answer is that it might take a very long time. 12:41
[laughs] - [Student] [speaks too low to hear] - Intuitively, if you set the learning rate very low and let it go for a very long time, then this should, in theory, always work.
But in practice, those factors of 10 or 100 actually matter a lot when you're training these things. 12:55
Maybe if you got the right learning rate, you could train it in six hours, 12 hours or a day, but then if you just were super safe and dropped it by a factor of 10 or by a factor of 100, now that one-day training becomes 100 days of training. 13:00
That's three months. 13:12
That's not going to be good. 13:14
When you're taking these intro computer science classes, they always kind of sweep the constants under the rug, but when you're actually thinking about training things, those constants end up mattering a lot. 13:16
Another question? 13:25
- [Student] If you have a low learning rate, [speaks too low to hear]. 13:28
- Question is for a low learning rate, are you more likely to be stuck in local optima? 13:33
I think that makes some intuitive sense, but in practice, that seems not to be much of a problem. 13:38
I think we'll talk a bit more about that later today. 13:43
Today I wanted to talk about a couple other really interesting and important topics when we're training neural networks. 13:47
In particular, I wanted to talk, we've kind of alluded to this fact of fancier, more powerful optimization algorithms a couple times. 13:53
I wanted to spend some time today and really dig into those and talk about what are the actual optimization algorithms that most people are using these days. 14:00
We also touched on regularization in earlier lectures. 14:07
This concept of making your network do additional things to reduce the gap between train and test error. 14:10
I wanted to talk about some more strategies that people are using in practice of regularization, with respect to neural networks. 14:16
Finally, I also wanted to talk a bit about transfer learning, where you can sometimes get away with using less data than you think by transferring from one problem to another. 14:22
If you recall from a few lectures ago, the kind of core strategy in training neural networks is an optimization problem where we write down some loss function, which defines, for each value of the network weights, the loss function tells us how good or bad is that value of the weights doing on our problem. 14:33
Then we imagine that this loss function gives us some nice landscape over the weights, where on the right, I've shown this maybe small, two-dimensional problem, where the X and Y axes are two values of the weights. 14:51
Then the color of the plot kind of represents the value of the loss. 15:04
In this kind of cartoon picture of a two-dimensional problem, we're only optimizing over these two values, W one, W two. 15:08
The goal is to find the most red region in this case, which corresponds to the setting of the weights with the lowest loss. 15:15
Remember, we've been working so far with this extremely simple optimization algorithm, stochastic gradient descent, where it's super simple, it's three lines. 15:23
While true, we first evaluate the loss in the gradient on some mini batch of data. 15:32
Then we step, updating our parameter vector in the negative direction of the gradient because this gives, again, the direction of greatest decrease of the loss function. 15:39
Then we repeat this over and over again, and hopefully we converge to the red region and we get great errors and we're very happy. 15:49
But unfortunately, this relatively simple optimization algorithm has quite a lot of problems that actually could come up in practice. 15:56
One problem with stochastic gradient descent, imagine what happens if our objective function looks something like this, where, again, we're plotting two values, W one and W two. 16:05
As we change one of those values, the loss function changes very slowly. 16:19
As we change the horizontal value, then our loss changes slowly. 16:23
As we go up and down in this landscape, now our loss is very sensitive to changes in the vertical direction. 16:28
By the way, this is referred to as the loss having a bad condition number at this point, which is the ratio between the largest and smallest singular values of the Hessian matrix at that point. 16:35
But the intuitive idea is that the loss landscape kind of looks like a taco shell. 16:46
It's sort of very sensitive in one direction, not sensitive in the other direction. 16:50
The question is what might SGD, stochastic gradient descent, do on a function that looks like this? 16:54
If you run stochastic gradient descent on this type of function, you might get this characteristic zigzagging behavior, where because for this type of objective function, the direction of the gradient does not align with the direction towards the minima. 17:05
When you compute the gradient and take a step, you might step sort of over this line and sort of zigzag back and forth. 17:22
In effect, you get very slow progress along the horizontal dimension, which is the less sensitive dimension, and you get this zigzagging, nasty, nasty zigzagging behavior across the fast-changing dimension. 17:29
This is undesirable behavior. 17:42
By the way, this problem actually becomes much more common in high dimensions. 17:45
In this kind of cartoon picture, we're only showing a two-dimensional optimization landscape, but in practice, our neural networks might have millions, tens of millions, hundreds of millions of parameters. 17:51
That's hundreds of millions of directions along which this thing can move. 18:01
Now among those hundreds of millions of different directions to move, if the ratio between the largest one and the smallest one is bad, then SGD will not perform so nicely. 18:04
You can imagine that if we have 100 million parameters, probably the maximum ratio between those two will be quite large. 18:14
I think this is actually quite a big problem in practice for many high-dimensional problems. 18:21
Another problem with SGD has to do with this idea of local minima or saddle points. 18:28
Here I've sort of swapped the graph a little bit. 18:34
Now the X axis is showing the value of one parameter, and then the Y axis is showing the value of the loss. 18:37
In this top example, we have kind of this curvy objective function, where there's a valley in the middle. 18:44
What happens to SGD in this situation? 18:52
- [Student] [speaks too low to hear] - In this situation, SGD will get stuck because at this local minima, the gradient is zero because it's locally flat. 18:55
Now remember with SGD, we compute the gradient and step in the direction of opposite gradient, so if at our current point, the opposite gradient is zero, then we're not going to make any progress, and we'll get stuck at this point. 19:04
There's another problem with this idea of saddle points. 19:16
Rather than being a local minima, you can imagine a point where in one direction we go up, and in the other direction we go down. 19:19
Then at our current point, the gradient is zero. 19:26
Again, in this situation, the function will get stuck at the saddle point because the gradient is zero. 19:29
Although one thing I'd like to point out is that in one dimension, in a one-dimensional problem like this, local minima seem like a big problem and saddle points seem like kind of not something to worry about, but in fact, it's the opposite once you move to very high-dimensional problems because, again, if you think about you're in this 100 million dimensional space, what does a saddle point mean? 19:36
That means that at my current point, some directions the loss goes up, and some directions the loss goes down. 19:57
If you have 100 million dimensions, that's probably going to happen more frequently than, that's probably going to happen almost everywhere, basically. 20:03
Whereas a local minima says that of all those 100 million directions that I can move, every one of them causes the loss to go up. 20:10
In fact, that seems pretty rare when you're thinking about, again, these very high-dimensional problems. 20:17
Really, the idea that has come to light in the last few years is that when you're training these very large neural networks, the problem is more about saddle points and less about local minima. 20:23
By the way, this also is a problem not just exactly at the saddle point, but also near the saddle point. 20:33
If you look at the example on the bottom, you see that in the regions around the saddle point, the gradient isn't zero, but the slope is very small. 20:40
That means that if we're, again, just stepping in the direction of the gradient, and that gradient is very small, we're going to make very, very slow progress whenever our current parameter value is near a saddle point in the objective landscape. 20:48
This is actually a big problem. 21:02
Another problem with SGD comes from the S. 21:06
Remember that SGD is stochastic gradient descent. 21:10
Recall that our loss function is typically defined by computing the loss over many, many different examples. 21:14
In this case, if N is your whole training set, then that could be something like a million. 21:21
Each time computing the loss would be very, very expensive. 21:26
In practice, remember that we often estimate the loss and estimate the gradient using a small mini batch of examples. 21:29
What this means is that we're not actually getting the true information about the gradient at every time step. 21:37
Instead, we're just getting some noisy estimate of the gradient at our current point. 21:42
Here on the right, I've kind of faked this plot a little bit. 21:47
I've just added random uniform noise to the gradient at every point, and then run SGD with these noisy, messed up gradients. 21:51
This is maybe not exactly what happens with the SGD process, but it still give you the sense that if there's noise in your gradient estimates, then vanilla SGD kind of meanders around the space and might actually take a long time to get towards the minima. 22:00
Now that we've talked about a lot of these problems. 22:16
Sorry, was there a question? 22:19
- [Student] [speaks too low to hear] - The question is do all of these just go away if we use normal gradient descent? 22:21
Let's see. 22:35
I think that the taco shell problem of high condition numbers is still a problem with full batch gradient descent. 22:37
The noise. 22:44
As we'll see, we might sometimes introduce additional noise into the network, not only due to sampling mini batches, but also due to explicit stochasticity in the network, so we'll see that later. 22:45
That can still be a problem. 22:55
Saddle points, that's still a problem for full batch gradient descent because there can still be saddle points in the full objective landscape. 22:58
Basically, even if we go to full batch gradient descent, it doesn't really solve these problems. 23:05
We kind of need to think about a slightly fancier optimization algorithm that can try to address these concerns. 23:10
Thankfully, there's a really, really simple strategy that works pretty well at addressing many of these problems. 23:17
That's this idea of adding a momentum term to our stochastic gradient descent. 23:22
Here on the left, we have our classic old friend, SGD, where we just always step in the direction of the gradient. 23:27
But now on the right, we have this minor, minor variance called SGD plus momentum, which is now two equations and five lines of code, so it's twice as complicated. 23:33
But it's very simple. 23:43
The idea is that we maintain a velocity over time, and we add our gradient estimates to the velocity. 23:44
Then we step in the direction of the velocity, rather than stepping in the direction of the gradient. 23:51
This is very, very simple. 23:58
We also have this hyperparameter rho now which corresponds to friction. 24:01
Now at every time step, we take our current velocity, we decay the current velocity by the friction constant, rho, which is often something high, like .9 is a common choice. 24:06
We take our current velocity, we decay it by friction and we add in our gradient. 24:17
Now we step in the direction of our velocity vector, rather than the direction of our raw gradient vector. 24:21
This super, super simple strategy actually helps for all of these problems that we just talked about. 24:28
If you think about what happens at local minima or saddle points, then if we're imagining velocity in this system, then you kind of have this physical interpretation of this ball kind of rolling down the hill, picking up speed as it comes down. 24:35
Now once we have velocity, then even when we pass that point of local minima, the point will still have velocity, even if it doesn't have gradient. 24:48
Then we can hopefully get over this local minima and continue downward. 24:57
There's this similar intuition near saddle points, where even though the gradient around the saddle point is very small, we have this velocity vector that we've built up as we roll downhill. 25:01
That can hopefully carry us through the saddle point and let us continue rolling all the way down. 25:11
If you think about what happens in poor conditioning, now if we were to have these kind of zigzagging approximations to the gradient, then those zigzags will hopefully cancel each other out pretty fast once we're using momentum. 25:16
This will effectively reduce the amount by which we step in the sensitive direction, whereas in the horizontal direction, our velocity will just keep building up, and will actually accelerate our descent across that less sensitive dimension. 25:31
Adding momentum here can actually help us with this high condition number problem, as well. 25:46
Finally, on the right, we've repeated the same visualization of gradient descent with noise. 25:51
Here, the black is this vanilla SGD, which is sort of zigzagging all over the place, where the blue line is showing now SGD with momentum. 25:57
You can see that because we're adding it, we're building up this velocity over time, the noise kind of gets averaged out in our gradient estimates. 26:05
Now SGD ends up taking a much smoother path towards the minima, compared with the SGD, which is kind of meandering due to noise. 26:13
Question? 26:20
- [Student] [speaks too low to hear] - The question is how does SGD momentum help with the poorly conditioned coordinate? 26:22
The idea is that if you go back and look at this velocity estimate and look at the velocity computation, we're adding in the gradient at every time step. 26:40
It kind of depends on your setting of rho, that hyperparameter, but you can imagine that if the gradient is relatively small, and if rho is well behaved in this situation, then our velocity could actually monotonically increase up to a point where the velocity could now be larger than the actual gradient. 26:49
Then we might actually make faster progress along the poorly conditioned dimension. 27:05
Kind of one picture that you can have in mind when we're doing SGD plus momentum is that the red here is our current point. 27:13
At our current point, we have some red vector, which is the direction of the gradient, or rather our estimate of the gradient at the current point. 27:20
Green is now the direction of our velocity vector. 27:27
Now when we do the momentum update, we're actually stepping according to a weighted average of these two. 27:30
This helps overcome some noise in our gradient estimate. 27:36
There's a slight variation of momentum that you sometimes see, called Nesterov accelerated gradient, also sometimes called Nesterov momentum. 27:40
That switches up this order of things a little bit. 27:48
In sort of normal SGD momentum, we imagine that we estimate the gradient at our current point, and then take a mix of our velocity and our gradient. 27:52
With Nesterov accelerated gradient, you do something a little bit different. 28:00
Here, you start at the red point. 28:04
You step in the direction of where the velocity would take you. 28:07
You evaluate the gradient at that point. 28:11
Then you go back to your original point and kind of mix together those two. 28:14
This is kind of a funny interpretation, but you can imagine that you're kind of mixing together information a little bit more. 28:19
If your velocity direction was actually a little bit wrong, it lets you incorporate gradient information from a little bit larger parts of the objective landscape. 28:26
This also has some really nice theoretical properties when it comes to convex optimization, but those guarantees go a little bit out the window once it comes to non-convex problems like neural networks. 28:35
Writing it down in equations, Nesterov momentum looks something like this, where now to update our velocity, we take a step, according to our previous velocity, and evaluate that gradient there. 28:46
Now when we take our next step, we actually step in the direction of our velocity that's incorporating information from these multiple points. 28:57
Question? 29:06
- [Student] [speaks too low to hear] - Oh, sorry. 29:08
The question is what's a good initialization for the velocity? 29:12
This is almost always zero. 29:15
It's not even a hyperparameter. 29:17
Just set it to zero and don't worry. 29:18
Another question? 29:20
- [Student] [speaks too low to hear] - Intuitively, the velocity is kind of a weighted sum of your gradients that you've seen over time. 29:21
- [Student] [speaks too low to hear] - With more recent gradients being weighted heavier. 29:38
At every time step, we take our old velocity, we decay by friction and we add in our current gradient. 29:44
You can kind of think of this as a smooth moving average of your recent gradients with kind of a exponentially decaying weight on your gradients going back in time. 29:50
This Nesterov formulation is a little bit annoying 'cause if you look at this, normally when you have your loss function, you want to evaluate your loss and your gradient at the same point. 30:03
Nesterov breaks this a little bit. 30:12
It's a little bit annoying to work with. 30:14
Thankfully, there's a cute change of variables you can do. 30:17
If you do the change of variables and reshuffle a little bit, then you can write Nesterov momentum in a slightly different way that now, again, lets you evaluate the loss and the gradient at the same point always. 30:19
Once you make this change of variables, you get kind of a nice interpretation of Nesterov, which is that here in the first step, this looks exactly like updating the velocity in the vanilla SGD momentum case, where we have our current velocity, we evaluate gradient at the current point and mix these two together in a decaying way. 30:29
Now in the second update, now when we're actually updating our parameter vector, if you look at the second equation, we have our current point plus our current velocity plus a weighted difference between our current velocity and our previous velocity. 30:48
Here, Nesterov momentum is kind of incorporating some kind of error-correcting term between your current velocity and your previous velocity. 31:01
If we look at SGD, SGD momentum and Nesterov momentum on this kind of simple problem, compared with SGD, we notice that SGD kind of takes this, SGD is in the black, kind of taking this slow progress toward the minima. 31:13
The blue and the green show momentum and Nesterov. 31:26
These have this behavior of kind of overshooting the minimum 'cause they're building up velocity going past the minimum, and then kind of correcting themselves and coming back towards the minima. 31:30
Question? 31:40
- [Student] [speaks too low to hear] - The question is this picture looks good, but what happens if your minima call lies in this very narrow basin? 31:42
Will the velocity just cause you to skip right over that minima? 31:58
That's actually a really interesting point, and the subject of some recent theoretical work, but the idea is that maybe those really sharp minima are actually bad minima. 32:02
We don't want to even land in those 'cause the idea is that maybe if you have a very sharp minima, that actually could be a minima that overfits more. 32:09
If you imagine that we doubled our training set, the whole optimization landscape would change, and maybe that very sensitive minima would actually disappear if we were to collect more training data. 32:18
We kind of have this intuition that we maybe want to land in very flat minima because those very flat minima are probably more robust as we change the training data. 32:27
Those flat minima might actually generalize better to testing data. 32:36
This is again, sort of very recent theoretical work, but that's actually a really good point that you bring it up. 32:40
In some sense, it's actually a feature and not a bug that SGD momentum actually skips over those very sharp minima. 32:46
That's actually a good thing, believe it or not. 32:56
Another thing you can see is if you look at the difference between momentum and Nesterov here, you can see that because of the correction factor in Nesterov, maybe it's not overshooting quite as drastically, compared to vanilla momentum. 33:01
There's another kind of common optimization strategy is this algorithm called AdaGrad, which John Duchi, who's now a professor here, worked on during his Ph.D. 33:15
The idea with AdaGrad is that as you, during the course of the optimization, you're going to keep a running estimate or a running sum of all the squared gradients that you see during training. 33:25
Now rather than having a velocity term, instead we have this grad squared term. 33:40
During training, we're going to just keep adding the squared gradients to this grad squared term. 33:44
Now when we update our parameter vector, we'll divide by this grad squared term when we're making our update step. 33:49
The question is what does this kind of scaling do in this situation where we have a very high condition number? 33:59
- [Student] [speaks too low to hear] - The idea is that if we have two coordinates, one that always has a very high gradient and one that always has a very small gradient, then as we add the sum of the squares of the small gradient, we're going to be dividing by a small number, so we'll accelerate movement along the slow dimension, along the one dimension. 34:08
Then along the other dimension, where the gradients tend to be very large, then we'll be dividing by a large number, so we'll kind of slow down our progress along the wiggling dimension. 34:35
But there's kind of a problem here. 34:46
That's the question of what happens with AdaGrad over the course of training, as t gets larger and larger and larger? 34:48
- [Student] [speaks too low to hear] - With AdaGrad, the steps actually get smaller and smaller and smaller because we just continue updating this estimate of the squared gradients over time, so this estimate just grows and grows and grows monotonically over the course of training. 34:56
Now this causes our step size to get smaller and smaller and smaller over time. 35:10
Again, in the convex case, there's some really nice theory showing that this is actually really good 'cause in the convex case, as you approach a minimum, you kind of want to slow down so you actually converge. 35:15
That's actually kind of a feature in the convex case. 35:28
But in the non-convex case, that's a little bit problematic because as you come towards a saddle point, you might get stuck with AdaGrad, and then you kind of no longer make any progress. 35:31
There's a slight variation of AdaGrad, called RMSProp, that actually addresses this concern a little bit. 35:42
Now with RMSProp, we still keep this estimate of the squared gradients, but instead of just letting that squared estimate continually accumulate over training, instead, we let that squared estimate actually decay. 35:49
This ends up looking kind of like a momentum update, except we're having kind of momentum over the squared gradients, rather than momentum over the actual gradients. 36:01
Now with RMSProp, after we compute our gradient, we take our current estimate of the grad squared, we multiply it by this decay rate, which is commonly something like .9 or .99. 36:09
Then we add in this one minus the decay rate of our current squared gradient. 36:20
Now over time, you can imagine that. 36:27
Then again, when we make our step, the step looks exactly the same as AdaGrad, where we divide by the squared gradient in the step to again have this nice property of accelerating movement along the one dimension, and slowing down movement along the other dimension. 36:30
But now with RMSProp, because these estimates are leaky, then it kind of addresses the problem of maybe always slowing down where you might not want to. 36:44
Here again, we're kind of showing our favorite toy problem with SGD in black, SGD momentum in blue and RMSProp in red. 36:56
You can see that RMSProp and SGD momentum are both doing much better than SGD, but their qualitative behavior is a little bit different. 37:04
With SGD momentum, it kind of overshoots the minimum and comes back, whereas with RMSProp, it's kind of adjusting its trajectory such that we're making approximately equal progress among all the dimensions. 37:12
By the way, you can't actually tell, but this plot is also showing AdaGrad in green with the same learning rate, but it just gets stuck due to this problem of continually decaying learning rates. 37:26
In practice, AdaGrad is maybe not so common for many of these things. 37:39
That's a little bit of an unfair comparison of AdaGrad. 37:44
Probably you need to increase the learning rate with AdaGrad, and then it would end up looking kind of like RMSProp in this case. 37:46
But in general, we tend not to use AdaGrad so much when training neural networks. 37:53
Question? 37:57
- [Student] [speaks too low to hear] - The answer is yes, this problem is convex, but in this case, it's a little bit of an unfair comparison because the learning rates are not so comparable among the methods. 37:58
I've been a little bit unfair to AdaGrad in this visualization by showing the same learning rate between the different algorithms, when probably you should have separately turned the learning rates per algorithm. 38:12
We saw in momentum, we had this idea of velocity, where we're building up velocity by adding in the gradients, and then stepping in the direction of the velocity. 38:28
We saw with AdaGrad and RMSProp that we had this other idea, of building up an estimate of the squared gradients, and then dividing by the squared gradients. 38:35
Then these both seem like good ideas on their own. 38:44
Why don't we just stick 'em together and use them both? 38:47
Maybe that would be even better. 38:49
That brings us to this algorithm called Adam, or rather brings us very close to Adam. 38:51
We'll see in a couple slides that there's a slight correction we need to make here. 38:57
Here with Adam, we maintain an estimate of the first moment and the second moment. 39:01
Now in the red, we make this estimate of the first moment as a weighed sum of our gradients. 39:07
We have this moving estimate of the second moment, like AdaGrad and like RMSProp, which is a moving estimate of our squared gradients. 39:15
Now when we make our update step, we step using both the first moment, which is kind of our velocity, and also divide by the second moment, or rather the square root of the second moment, which is this squared gradient term. 39:23
This idea of Adam ends up looking a little bit like RMSProp plus momentum, or ends up looking like momentum plus second squared gradients. 39:38
It kind of incorporates the nice properties of both. 39:46
But there's a little bit of a problem here. 39:50
That's the question of what happens at the very first time step? 39:52
At the very first time step, you can see that at the beginning, we've initialized our second moment with zero. 40:00
Now after one update of the second moment, typically this beta two, second moment decay rate, is something like .9 or .99, something very close to one. 40:06
After one update, our second moment is still very, very close to zero. 40:18
Now when we're making our update step here and we divide by our second moment, now we're dividing by a very small number. 40:23
We're making a very, very large step at the beginning. 40:30
This very, very large step at the beginning is not really due to the geometry of the problem. 40:32
It's kind of an artifact of the fact that we initialized our second moment estimate was zero. 40:38
Question? 40:43
- [Student] [speaks too low to hear] - That's true. 40:44
The comment is that if your first moment is also very small, then you're multiplying by small and you're dividing by square root of small squared, so what's going to happen? 40:55
They might cancel each other out, you might be okay. 41:03
That's true. 41:06
Sometimes these cancel each other out and you're okay, but sometimes this ends up in taking very large steps right at the beginning. 41:07
That can be quite bad. 41:14
Maybe you initialize a little bit poorly. 41:16
You take a very large step. 41:18
Now your initialization is completely messed up, and then you're in a very bad part of the objective landscape and you just can't converge from there. 41:20
Question? 41:26
- [Student] [speaks too low to hear] - The idea is what is this 10 to the minus seven term in the last equation? 41:27
That's actually appeared in AdaGrad, RMSProp and Adam. 41:35
The idea is that we're dividing by something. 41:38
We want to make sure we're not dividing by zero, so we always add a small positive constant to the denominator, just to make sure we're not dividing by zero. 41:40
That's technically a hyperparameter, but it tends not to matter too much, so just setting 10 to minus seven, 10 to minus eight, something like that, tends to work well. 41:49
With Adam, remember we just talked about this idea of at the first couple steps, it gets very large, and we might take very large steps and mess ourselves up. 41:58
Adam also adds this bias correction term to avoid this problem of taking very large steps at the beginning. 42:05
You can see that after we update our first and second moments, we create an unbiased estimate of those first and second moments by incorporating the current time step, t. 42:12
Now we actually make our step using these unbiased estimates, rather than the original first and second moment estimates. 42:23
This gives us our full form of Adam. 42:30
By the way, Adam is a really, [laughs] really good optimization algorithm, and it works really well for a lot of different problems, so that's kind of my default optimization algorithm for just about any new problem that I'm tackling. 42:33
In particular, if you set beta one equals .9, beta two equals .999, learning rate one e minus three or five e minus four, that's a great staring point for just about all the architectures I've ever worked with. 42:46
Try that. 42:59
That's a really good place to start, in general. 43:00
[laughs] If we actually plot these things out and look at SGD, SGD momentum, RMSProp and Adam on the same problem, you can see that Adam, in the purple here, kind of combines elements of both SGD momentum and RMSProp. 43:04
Adam kind of overshoots the minimum a little bit like SGD momentum, but it doesn't overshoot quite as much as momentum. 43:18
Adam also has this similar behavior of RMSProp of kind of trying to curve to make equal progress along all dimensions. 43:25
Maybe in this small two-dimensional example, Adam converged about similarly to other ones, but you can see qualitatively that it's kind of combining the behaviors of both momentum and RMSProp. 43:33
Any questions about optimization algorithms? 43:45
- [Student] [speaks too low to hear] They still take a very long time to train. 43:50
[speaks too low to hear] - The question is what does Adam not fix? 43:54
Would these neural networks are still large, they still take a long time to train. 43:59
There can still be a problem. 44:05
In this picture where we have this landscape of things looking like ovals, if you imagine that we're kind of making estimates along each dimension independently to allow us to speed up or slow down along different coordinate axes, but one problem is that if that taco shell is kind of tilted and is not axis aligned, then we're still only making estimates along the individual axes independently. 44:07
That corresponds to taking your rotated taco shell and squishing it horizontally and vertically, but you can't actually unrotate it. 44:31
In cases where you have this kind of rotated picture of poor conditioning, then Adam or any of these other algorithms really can't address that, that concern. 44:38
Another thing that we've seen in all these optimization algorithms is learning rate as a hyperparameter. 44:51
We've seen this picture before a couple times, that as you use different learning rates, sometimes if it's too high, it might explode in the yellow. 44:58
If it's a very low learning rate, in the blue, it might take a very long time to converge. 45:05
It's kind of tricky to pick the right learning rate. 45:10
This is a little bit of a trick question because we don't actually have to stick with one learning rate throughout the course of training. 45:14
Sometimes you'll see people decay the learning rates over time, where we can kind of combine the effects of these different curves on the left, and get the nice properties of each. 45:19
Sometimes you'll start with a higher learning rate near the start of training, and then decay the learning rate and make it smaller and smaller throughout the course of training. 45:30
A couple strategies for these would be a step decay, where at 100,000th iteration, you just decay by some factor and you keep going. 45:39
You might see an exponential decay, where you continually decay during training. 45:47
You might see different variations of continually decaying the learning rate during training. 45:53
If you look at papers, especially the resonate paper, you often see plots that look kind of like this, where the loss is kind of going down, then dropping, then flattening again, then dropping again. 45:58
What's going on in these plots is that they're using a step decay learning rate, where at these parts where it plateaus and then suddenly drops again, those are the iterations where they dropped the learning rate by some factor. 46:08
This idea of dropping the learning rate, you might imagine that it got near some good region, but now the gradients got smaller, it's kind of bouncing around too much. 46:18
Then if we drop the learning rate, it lets it slow down and continue to make progress down the landscape. 46:28
This tends to help in practice sometimes. 46:33
Although one thing to point out is that learning rate decay is a little bit more common with SGD momentum, and a little bit less common with something like Adam. 46:36
Another thing I'd like to point out is that learning rate decay is kind of a second-order hyperparameter. 46:45
You typically should not optimize over this thing from the start. 46:50
Usually when you're kind of getting networks to work at the beginning, you want to pick a good learning rate with no learning rate decay from the start. 46:53
Trying to cross-validate jointly over learning rate decay and initial learning rate and other things, you'll just get confused. 47:01
What you do for setting learning rate decay is try with no decay, see what happens. 47:06
Then kind of eyeball the loss curve and see where you think you might need decay. 47:11
Another thing I wanted to mention briefly is this idea of all these algorithms that we've talked about are first-order optimization algorithms. 47:17
In this picture, in this one-dimensional picture, we have this kind of curvy objective function at our current point in red. 47:25
What we're basically doing is computing the gradient at that point. 47:33
We're using the gradient information to compute some linear approximation to our function, which is kind of a first-order Taylor approximation to our function. 47:36
Now we pretend that the first-order approximation is our actual function, and we make a step to try to minimize the approximation. 47:44
But this approximation doesn't hold for very large regions, so we can't step too far in that direction. 47:52
But really, the idea here is that we're only incorporating information about the first derivative of the function. 47:57
You can actually go a little bit fancier. 48:03
There's this idea of second-order approximation, where we take into account both first derivative and second derivative information. 48:04
Now we make a second-order Taylor approximation to our function and kind of locally approximate our function with a quadratic. 48:11
Now with a quadratic, you can step right to the minimum, and you're really happy. 48:18
That's this idea of second-order optimization. 48:22
When you generalize this to multiple dimensions, you get something called the Newton step, where you compute this Hessian matrix, which is a matrix of second derivatives, and you end up inverting this Hessian matrix in order to step directly to the minimum of this quadratic approximation to your function. 48:26
Does anyone spot something that's quite different about this update rule, compared to the other ones that we've seen? 48:44
- [Student] [speaks too low to hear] - This doesn't have a learning rate. 48:49
That's kind of cool. 48:53
We're making this quadratic approximation and we're stepping right to the minimum of the quadratic. 48:56
At least in this vanilla version of Newton's method, you don't actually need a learning rate. 49:01
You just always step to the minimum at every time step. 49:05
However, in practice, you might end up, have a learning rate anyway because, again, that quadratic approximation might not be perfect, so you might only want to step in the direction towards the minimum, rather than actually stepping to the minimum, but at least in this vanilla version, it doesn't have a learning rate. 49:08
But unfortunately, this is maybe a little bit impractical for deep learning because this Hessian matrix is N by N, where N is the number of parameters in your network. 49:24
If N is 100 million, then 100 million squared is way too big. 49:35
You definitely can't store that in memory, and you definitely can't invert it. 49:38
In practice, people sometimes use these quasi-Newton methods that, rather than working with the full Hessian and inverting the full Hessian, they work with approximations. 49:42
Low-rank approximations are common. 49:50
You'll sometimes see these for some problems. 49:53
L-BFGS is one particular second-order optimizer that has this approximate second, keeps this approximation of the Hessian that you'll sometimes see, but in practice, it doesn't work too well for many deep learning problems because these approximations, these second-order approximations, don't really handle the stochastic case very much, very nicely. 49:57
They also tend not to work so well with non-convex problems. 50:16
I don't want to get into that right now too much. 50:21
In practice, what you should really do is probably Adam is a really good choice for many different neural network things, but if you're in a situation where you can afford to do full batch updates, and you know that your problem doesn't have really any stochasticity, then L-BFGS is kind of a good choice. 50:23
L-BFGS doesn't really get used for training neural networks too much, but as we'll see in a couple of lectures, it does sometimes get used for things like style transfer, where you actually have less stochasticity and fewer parameters, but you still want to solve an optimization problem. 50:39
All of these strategies we've talked about so far are about reducing training error. 50:56
All these optimization algorithms are really about driving down your training error and minimizing your objective function, but we don't really care about training error that much. 51:02
Instead, we really care about our performance on unseen data. 51:10
We really care about reducing this gap between train and test error. 51:13
The question is once we're already good at optimizing our objective function, what can we do to try to reduce this gap and make our model perform better on unseen data? 51:17
One really quick and dirty, easy thing to try is this idea of model ensembles that sometimes works across many different areas in machine learning. 51:28
The idea is pretty simple. 51:37
Rather than having just one model, we'll train 10 different models independently from different initial random restarts. 51:38
Now at test time, we'll run our data through all of the 10 models and average the predictions of those 10 models. 51:45
Adding these multiple models together tends to reduce overfitting a little bit and tend to improve performance a little bit, typically by a couple percent. 51:54
This is generally not a drastic improvement, but it is a consistent improvement. 52:02
You'll see that in competitions, like ImageNet and other things like that, using model ensembles is very common to get maximal performance. 52:05
You can actually get a little bit creative with this. 52:14
Sometimes rather than training separate models independently, you can just keep multiple snapshots of your model during the course of training, and then use these as your ensembles. 52:17
Then you still, at test time, need to average the predictions of these multiple snapshots, but you can collect the snapshots during the course of training. 52:26
There's actually a very nice paper being presented at ICLR this week that kind of has a fancy version of this idea, where we use a crazy learning rate schedule, where our learning rate goes very slow, then very fast, then very slow, then very fast. 52:34
The idea is that with this crazy learning rate schedule, then over the course of training, the model might be able to converge to different regions in the objective landscape that all are reasonably good. 52:48
If you do an ensemble over these different snapshots, then you can improve your performance quite nicely, even though you're only training the model once. 52:59
Questions? 53:06
- [Student] [speaks too low to hear] - The question is, it's bad when there's a large gap between error 'cause that means you're overfitting, but if there's no gap, then is that also maybe bad? 53:07
Do we actually want some small, optimal gap between the two? 53:33
We don't really care about the gap. 53:37
What we really care about is maximizing the performance on the validation set. 53:39
What tends to happen is that if you don't see a gap, then you could have improved your absolute performance, in many cases, by overfitting a little bit more. 53:44
There's this weird correlation between the absolute performance on the validation set and the size of that gap. 53:55
We only care about absolute performance. 54:00
Question in the back? 54:03
- [Student] Are hyperparameters the same for the ensemble? 54:04
- Are the hyperparameters the same for the ensembles? 54:07
That's a good question. 54:10
Sometimes they're not. 54:11
You might want to try different sizes of the model, different learning rates, different regularization strategies and ensemble across these different things. 54:12
That actually does happen sometimes. 54:20
Another little trick you can do sometimes is that during training, you might actually keep an exponentially decaying average of your parameter vector itself to kind of have a smooth ensemble of your own network during training. 54:23
Then use this smoothly decaying average of your parameter vector, rather than the actual checkpoints themselves. 54:36
This is called Polyak averaging, and it sometimes helps a little bit. 54:42
It's just another one of these small tricks you can sometimes add, but it's not maybe too common in practice. 54:45
Another question you might have is that how can we actually improve the performance of single models? 54:51
When we have ensembles, we still need to run, like, 10 models at test time. 54:57
That's not so great. 55:01
We really want some strategies to improve the performance of our single models. 55:02
That's really this idea of regularization, where we add something to our model to prevent it from fitting the training data too well in the attempts to make it perform better on unseen data. 55:06
We've seen a couple ideas, a couple methods for regularization already, where we add some explicit extra term to the loss. 55:16
Where we have this one term telling the model to fit the data, and another term that's a regularization term. 55:24
You saw this in homework one, where we used L2 regularization. 55:30
As we talked about in lecture a couple lectures ago, this L2 regularization doesn't really make maybe a lot of sense in the context of neural networks. 55:35
Sometimes we use other things for neural networks. 55:44
One regularization strategy that's super, super common for neural networks is this idea of dropout. 55:48
Dropout is super simple. 55:53
Every time we do a forward pass through the network, at every layer, we're going to randomly set some neurons to zero. 55:55
Every time we do a forward pass, we'll set a different random subset of the neurons to zero. 56:02
This kind of proceeds one layer at a time. 56:07
We run through one layer, we compute the value of the layer, we randomly set some of them to zero, and then we continue up through the network. 56:09
Now if you look at this fully connected network on the left versus a dropout version of the same network on the right, you can see that after we do dropout, it kind of looks like a smaller version of the same network, where we're only using some subset of the neurons. 56:15
This subset that we use varies at each iteration, at each forward pass. 56:30
Question? 56:36
- [Student] [speaks too low to hear] - The question is what are we setting to zero? 56:37
It's the activations. 56:45
Each layer is computing previous activation times the weight matrix gives you our next activation. 56:46
Then you just take that activation, set some of them to zero, and then your next layer will be partially zeroed activations times another matrix give you your next activations. 56:52
Question? 57:02
- [Student] [speaks too low to hear] - Question is which layers do you do this on? 57:03
It's more common in fully connected layers, but you sometimes see this in convolutional layers, as well. 57:09
When you're working in convolutional layers, sometimes instead of dropping each activation randomly, instead you sometimes might drop entire feature maps randomly. 57:14
In convolutions, you have this channel dimension, and you might drop out entire channels, rather than random elements. 57:24
Dropout is kind of super simple in practice. 57:32
It only requires adding two lines, one line per dropout call. 57:34
Here we have a three-layer neural network, and we've added dropout. 57:38
You can see that all we needed to do was add this extra line where we randomly set some things to zero. 57:42
This is super easy to implement. 57:47
But the question is why is this even a good idea? 57:49
We're seriously messing with the network at training time by setting a bunch of its values to zero. 57:52
How can this possibly make sense? 57:58
One sort of slightly hand wavy idea that people have is that dropout helps prevent co-adaptation of features. 58:01
Maybe if you imagine that we're trying to classify cats, maybe in some universe, the network might learn one neuron for having an ear, one neuron for having a tail, one neuron for the input being furry. 58:10
Then it kind of combines these things together to decide whether or not it's a cat. 58:21
But now if we have dropout, then in making the final decision about catness, the network cannot depend too much on any of these one features. 58:25
Instead, it kind of needs to distribute its idea of catness across many different features. 58:33
This might help prevent overfitting somehow. 58:38
Another interpretation of dropout that's come out a little bit more recently is that it's kind of like doing model ensembling within a single model. 58:42
If you look at the picture on the left, after you apply dropout to the network, we're kind of computing this subnetwork using some subset of the neurons. 58:52
Now every different potential dropout mask leads to a different potential subnetwork. 58:59
Now dropout is kind of learning a whole ensemble of networks all at the same time that all share parameters. 59:03
By the way, because of the number of potential dropout masks grows exponentially in the number of neurons, you're never going to sample all of these things. 59:09
This is really a gigantic, gigantic ensemble of networks that are all being trained simultaneously. 59:18
Then the question is what happens at test time? 59:26
Once we move to dropout, we've kind of fundamentally changed the operation of our neural network. 59:29
Previously, we've had our neural network, f, be a function of the weights, w, and the inputs, x, and then produce the output, y. 59:34
But now, our network is also taking this additional input, z, which is some random dropout mask. 59:43
That z is random. 59:48
Having randomness at test time is maybe bad. 59:50
Imagine that you're working at Facebook, and you want to classify the images that people are uploading. 59:53
Then today, your image gets classified as a cat, and tomorrow it doesn't. 59:57
That would be really weird and really bad. 60:01
You'd probably want to eliminate this stochasticity at test time once the network is already trained. 60:03
Then we kind of want to average out this randomness. 60:09
If you write this out, you can imagine actually marginalizing out this randomness with some integral, but in practice, this integral is totally intractable. 60:12
We don't know how to evaluate this thing. 60:20
You're in bad shape. 60:23
One thing you might imagine doing is approximating this integral via sampling, where you draw multiple samples of z and then average them out at test time, but this still would introduce some randomness, which is little bit bad. 60:24
Thankfully, in the case of dropout, we can actually approximate this integral in kind of a cheap way locally. 60:36
If we consider a single neuron, the output is a, the inputs are x and y, with two weights, w one, w two. 60:41
Then at test time, our value a is just w one x plus w two y. 60:47
Now imagine that we trained to this network. 60:54
During training, we used dropout with probability 1/2 of dropping our neurons. 60:56
Now the expected value of a during training, we can kind of compute analytically for this small case. 61:01
There's four possible dropout masks, and we're going to average out the values across these four masks. 61:08
We can see that the expected value of a during training is 1/2 w one x plus w two y. 61:12
There's this disconnect between this average value of w one x plus w two y at test time, and at training time, the average value is only 1/2 as much. 61:19
One cheap thing we can do is that at test time, we don't have any stochasticity. 61:29
Instead, we just multiply this output by the dropout probability. 61:35
Now these expected values are the same. 61:38
This is kind of like a local cheap approximation to this complex integral. 61:41
This is what people really commonly do in practice with dropout. 61:45
At dropout, we have this predict function, and we just multiply our outputs of the layer by the dropout probability. 61:50
The summary of dropout is that it's really simple on the forward pass. 61:56
You're just adding two lines to your implementation to randomly zero out some nodes. 61:59
Then at the test time prediction function, you just added one little multiplication by your probability. 62:04
Dropout is super simple. 62:10
It tends to work well sometimes for regularizing neural networks. 62:11
By the way, one common trick you see sometimes is this idea of inverted dropout. 62:17
Maybe at test time, you care more about efficiency, so you want to eliminate that extra multiplication by p at test time. 62:23
Then what you can do is, at test time, you use the entire weight matrix, but now at training time, instead you divide by p because training is probably happening on a GPU. 62:29
You don't really care if you do one extra multiply at training time, but then at test time, you kind of want this thing to be as efficient as possible. 62:38
Question? 62:45
- [Student] [speaks too low to hear] Now the gradient [speaks too low to hear]. 62:46
- The question is what happens to the gradient during training with dropout? 62:58
You're right. 63:02
We only end up propagating the gradients through the nodes that were not dropped. 63:03
This has the consequence that when you're training with dropout, typically training takes longer because at each step, you're only updating some subparts of the network. 63:07
When you're using dropout, it typically takes longer to train, but you might have a better generalization after it's converged. 63:15
Dropout, we kind of saw is like this one concrete instantiation. 63:24
There's a little bit more general strategy for regularization where during training we add some kind of randomness to the network to prevent it from fitting the training data too well. 63:28
To kind of mess it up and prevent it from fitting the training data perfectly. 63:37
Now at test time, we want to average out all that randomness to hopefully improve our generalization. 63:41
Dropout is probably the most common example of this type of strategy, but actually batch normalization kind of fits this idea, as well. 63:46
Remember in batch normalization, during training, one data point might appear in different mini batches with different other data points. 63:54
There's a bit of stochasticity with respect to a single data point with how exactly that point gets normalized during training. 64:01
But now at test time, we kind of average out this stochasticity by using some global estimates to normalize, rather than the per mini batch estimates. 64:07
Actually batch normalization tends to have kind of a similar regularizing effect as dropout because they both introduce some kind of stochasticity or noise at training time, but then average it out at test time. 64:15
Actually, when you train networks with batch normalization, sometimes you don't use dropout at all, and just the batch normalization adds enough of a regularizing effect to your network. 64:25
Dropout is somewhat nice because you can actually tune the regularization strength by varying that parameter p, and there's no such control in batch normalization. 64:36
Another kind of strategy that fits in this paradigm is this idea of data augmentation. 64:44
During training, in a vanilla version for training, we have our data, we have our label. 64:49
We use it to update our CNN at each time step. 64:53
But instead, what we can do is randomly transform the image in some way during training such that the label is preserved. 64:57
Now we train on these random transformations of the image rather than the original images. 65:04
Sometimes you might see random horizontal flips 'cause if you take a cat and flip it horizontally, it's still a cat. 65:09
You'll randomly sample crops of different sizes from the image because the random crop of the cat is still a cat. 65:18
Then during testing, you kind of average out this stochasticity by evaluating with some fixed set of crops, often the four corners and the middle and their flips. 65:25
What's very common is that when you read, for example, papers on ImageNet, they'll report a single crop performance of their model, which is just like the whole image, and a 10 crop performance of their model, which are these five standard crops plus their flips. 65:34
Also with data augmentation, you'll sometimes use color jittering, where you might randomly vary the contrast or brightness of your image during training. 65:48
You can get a little bit more complex with color jittering, as well, where you try to make color jitters that are maybe in the PCA directions of your data space or whatever, where you do some color jittering in some data-dependent way, but that's a little bit less common. 65:56
In general, data augmentation is this really general thing that you can apply to just about any problem. 66:12
Whatever problem you're trying to solve, you kind of think about what are the ways that I can transform my data without changing the label? 66:18
Now during training, you just apply these random transformations to your input data. 66:25
This sort of has a regularizing effect on the network because you're, again, adding some kind of stochasticity during training, and then marginalizing it out at test time. 66:29
Now we've seen three examples of this pattern, dropout, batch normalization, data augmentation, but there's many other examples, as well. 66:40
Once you have this pattern in your mind, you'll kind of recognize this thing as you read other papers sometimes. 66:47
There's another kind of related idea to dropout called DropConnect. 66:53
With DropConnect, it's the same idea, but rather than zeroing out the activations at every forward pass, instead we randomly zero out some of the values of the weight matrix instead. 66:57
Again, it kind of has this similar flavor. 67:06
Another kind of cool idea that I like, this one's not so commonly used, but I just think it's a really cool idea, is this idea of fractional max pooling. 67:10
Normally when you do two-by-two max pooling, you have these fixed two-by-two regions over which you pool over in the forward pass, but now with fractional max pooling, every time we have our pooling layer, we're going to randomize exactly the pool that the regions over which we pool. 67:19
Here in the example on the right, I've shown three different sets of random pooling regions that you might see during training. 67:36
Now during test time, you kind of average the stochasticity out by trying many different, by either sticking to some fixed set of pooling regions. 67:43
or drawing many samples and averaging over them. 67:52
That's kind of a cool idea, even though it's not so commonly used. 67:55
Another really kind of surprising paper in this paradigm that actually came out in the last year, so this is new since the last time we taught the class, is this idea of stochastic depth. 67:59
Here we have a network on the left. 68:10
The idea is that we have a very deep network. 68:13
We're going to randomly drop layers from the network during training. 68:15
During training, we're going to eliminate some layers and only use some subset of the layers during training. 68:19
Now during test time, we'll use the whole network. 68:24
This is kind of crazy. 68:27
It's kind of amazing that this works, but this tends to have kind of a similar regularizing effect as dropout and these other strategies. 68:28
But again, this is super, super cutting-edge research. 68:35
This is not super commonly used in practice, but it is a cool idea. 68:38
Any last minute questions about regularization? 68:45
No? 68:50
Use it.
It's a good idea.
Yeah? 68:52
- [Student] [speaks too low to hear] - The question is do you usually use more than one regularization method? 68:53
You should generally be using batch normalization as kind of a good thing to have in most networks nowadays because it helps you converge, especially for very deep things. 69:04
In many cases, batch normalization alone tends to be enough, but then sometimes if batch normalization alone is not enough, then you can consider adding dropout or other thing once you see your network overfitting. 69:13
You generally don't do a blind cross-validation over these things. 69:25
Instead, you add them in in a targeted way once you see your network is overfitting. 69:29
One quick thing, it's this idea of transfer learning. 69:36
We've kind of seen with regularization, we can help reduce the gap between train and test error by adding these different regularization strategies. 69:39
One problem with overfitting is sometimes you overfit 'cause you don't have enough data. 69:49
You want to use a big, powerful model, but that big, powerful model just is going to overfit too much on your small dataset. 69:53
Regularization is one way to combat that, but another way is through using transfer learning. 70:00
Transfer learning kind of busts this myth that you don't need a huge amount of data in order to train a CNN. 70:06
The idea is really simple. 70:13
You'll maybe first take some CNN. 70:15
Here is kind of a VGG style architecture. 70:18
You'll take your CNN, you'll train it in a very large dataset, like ImageNet, where you actually have enough data to train the whole network. 70:21
Now the idea is that you want to apply the features from this dataset to some small dataset that you care about. 70:28
Maybe instead of classifying the 1,000 ImageNet categories, now you want to classify 10 dog breeds or something like that. 70:35
You only have a small dataset. 70:41
Here, our small dataset only has C classes. 70:43
Then what you'll typically do is for this last fully connected layer that is going from the last layer features to the final class scores, this now, you need to reinitialize that matrix randomly. 70:46
For ImageNet, it was a 4,096-by-1,000 dimensional matrix. 71:00
Now for your new classes, it might be 4,096-by-C or by 10 or whatever. 71:03
You reinitialize this last matrix randomly, freeze the weights of all the previous layers and now just basically train a linear classifier, and only train the parameters of this last layer and let it converge on your data. 71:09
This tends to work pretty well if you only have a very small dataset to work with. 71:24
Now if you have a little bit more data, another thing you can try is actually fine tuning the whole network. 71:29
After that top layer converges and after you learn that last layer for your data, then you can consider actually trying to update the whole network, as well. 71:35
If you have more data, then you might consider updating larger parts of the network. 71:45
A general strategy here is that when you're updating the network, you want to drop the learning rate from its initial learning rate because probably the original parameters in this network that converged on ImageNet probably worked pretty well generally, and you just want to change them a very small amount to tune performance for your dataset. 71:49
Then when you're working with transfer learning, you kind of imagine this two-by-two grid of scenarios where on the one side, you have maybe very small amounts of data for your dataset, or very large amount of data for your dataset. 72:09
Then maybe your data is very similar to images. 72:21
Like, ImageNet has a lot of pictures of animals and plants and stuff like that. 72:24
If you want to just classify other types of animals and plants and other types of images like that, then you're in pretty good shape. 72:29
Then generally what you do is if your data is very similar to something like ImageNet, if you have a very small amount of data, you can just basically train a linear classifier on top of features, extracted using an ImageNet model. 72:35
If you have a little bit more data to work with, then you might imagine fine tuning your data. 72:49
However, you sometimes get in trouble if your data looks very different from ImageNet. 72:55
Maybe if you're working with maybe medical images that are X-rays or CAT scans or something that looks very different from images in ImageNet, in that case, you maybe need to get a little bit more creative. 72:59
Sometimes it still works well here, but those last layer features might not be so informative. 73:09
You might consider reinitializing larger parts of the network and getting a little bit more creative and trying more experiments here. 73:14
This is somewhat mitigated if you have a large amount of data in your very different dataset 'cause then you can actually fine tune larger parts of the network. 73:21
Another point I'd like to make is this idea of transfer learning is super pervasive. 73:29
It's actually the norm, rather than the exception. 73:33
As you read computer vision papers, you'll often see system diagrams like this for different tasks. 73:36
On the left, we're working with object detection. 73:41
On the right, we're working with image captioning. 73:42
Both of these models have a CNN that's kind of processing the image. 73:45
In almost all applications of computer vision these days, most people are not training these things from scratch. 73:48
Almost always, that CNN will be pretrained on ImageNet, and then potentially fine tuned for the task at hand. 73:54
Also, in the captioning sense, sometimes you can actually pretrain some word vectors relating to the language, as well. 74:00
You maybe pretrain the CNN on ImageNet, pretrain some word vectors on a large text corpus, and then fine tune the whole thing for your dataset. 74:07
Although in the case of captioning, I think this pretraining with word vectors tends to be a little bit less common and a little bit less critical. 74:14
The takeaway for your projects, and more generally as you work on different models, is that whenever you have some large dataset, whenever you have some problem that you want to tackle, but you don't have a large dataset, then what you should generally do is download some pretrained model that's relatively close to the task you care about, and then either reinitialize parts of that model or fine tune that model for your data. 74:22
That tends to work pretty well, even if you have only a modest amount of training data to work with. 74:45
Because this is such a common strategy, all of the different deep learning software packages out there provide a model zoo where you can just download pretrained versions of various models. 74:51
In summary today, we talked about optimization, which is about how to improve the training loss. 75:01
We talked about regularization, which is improving your performance on the test data. 75:06
Model ensembling kind of fit into there. 75:11
We also talked about transfer learning, which is how you can actually do better with less data. 75:13
These are all super useful strategies. 75:17
You should use them in your projects and beyond. 75:19
Next time, we'll talk more concretely about some of the different deep learning software packages out there. 75:22
