Lecture 13 - Generative Models

Okay we have a lot to cover today so let's get started. 00:11
Today we'll be talking about Generative Models. 00:14
And before we start, a few administrative details. 00:17
So midterm grades will be released on Gradescope this week A reminder that A3 is due next Friday May 26th. 00:20
The HyperQuest deadline for extra credit you can do this still until Sunday May 21st. 00:27
And our poster session is June 6th from 12 to 3 P.M.. 00:33
Okay so an overview of what we're going to talk about today we're going to switch gears a little bit and take a look at unsupervised learning today. 00:40
And in particular we're going to talk about generative models which is a type of unsupervised learning. 00:47
And we'll look at three types of generative models. 00:54
So pixelRNNs and pixelCNNs variational autoencoders and Generative Adversarial networks. 00:57
So so far in this class we've talked a lot about supervised learning and different kinds of supervised learning problems. 01:05
So in the supervised learning set up we have our data X and then we have some labels Y. 01:11
And our goal is to learn a function that's mapping from our data X to our labels Y. 01:16
And these labels can take many different types of forms. 01:21
So for example, we've looked at classification where our input is an image and we want to output Y, a class label for the category. 01:26
We've talked about object detection where now our input is still an image but here we want to output the bounding boxes of instances of up to multiple dogs or cats. 01:34
We've talked about semantic segmentation where here we have a label for every pixel the category that every pixel belongs to. 01:46
And we've also talked about image captioning where here our label is now a sentence and so it's now in the form of natural language. 01:53
So unsupervised learning in this set up, it's a type of learning where here we have unlabeled training data and our goal now is to learn some underlying hidden structure of the data. 02:03
Right, so an example of this can be something like clustering which you guys might have seen before where here the goal is to find groups within the data that are similar through some type of metric. 02:15
For example, K means clustering. 02:25
Another example of an unsupervised learning task is a dimensionality reduction. 02:27
So in this problem want to find axes along which our training data has the most variation, and so these axes are part of the underlying structure of the data. 02:33
And then we can use this to reduce of dimensionality of the data such that the data has significant variation among each of the remaining dimensions. 02:43
Right, so this example here we start off with data in three dimensions and we're going to find two axes of variation in this case and reduce our data projected down to 2D. 02:51
Another example of unsupervised learning is learning feature representations for data. 03:04
We've seen how to do this in supervised ways before where we used the supervised loss, for example classification. 03:11
Where we have the classification label. 03:17
We have something like a Softmax loss And we can train a neural network where we can interpret activations for example our FC7 layers as some kind of future representation for the data. 03:19
And in an unsupervised setting, for example here autoencoders which we'll talk more about later In this case our loss is now trying to reconstruct the input data to basically, you have a good reconstruction of our input data and use this to learn features. 03:29
So we're learning a feature representation without using any additional external labels. 03:46
And finally another example of unsupervised learning is density estimation where in this case we want to estimate the underlying distribution of our data. 03:53
So for example in this top case over here, we have points in 1-d and we can try and fit a Gaussian into this density and in this bottom example over here it's 2D data and here again we're trying to estimate the density and we can model this density. 04:02
We want to fit a model such that the density is higher where there's more points concentrated. 04:18
And so to summarize the differences in unsupervised learning which we've looked a lot so far, we want to use label data to learn a function mapping from X to Y and an unsupervised learning we use no labels and instead we try to learn some underlying hidden structure of the data, whether this is grouping, acts as a variation or underlying density estimation. 04:26
And unsupervised learning is a huge and really exciting area of research and and some of the reasons are that training data is really cheap, it doesn't use labels so we're able to learn from a lot of data at one time and basically utilize a lot more data than if we required annotating or finding labels for data. 04:49
And unsupervised learning is still relatively unsolved research area by comparison. 05:09
There's a lot of open problems in this, but it also, it holds the potential of if you're able to successfully learn and represent a lot of the underlying structure in the data then this also takes you a long way towards the Holy Grail of trying to understand the structure of the visual world. 05:15
So that's a little bit of kind of a high-level big picture view of unsupervised learning. 05:35
And today will focus more specifically on generative models which is a class of models for unsupervised learning where given training data our goal is to try and generate new samples from the same distribution. 05:40
Right, so we have training data over here generated from some distribution P data and we want to learn a model, P model to generate samples from the same distribution and so we want to learn P model to be similar to P data. 05:52
And generative models address density estimations. 06:09
So this problem that we saw earlier of trying to estimate the underlying distribution of your training data which is a core problem in unsupervised learning. 06:12
And we'll see that there's several flavors of this. 06:22
We can use generative models to do explicit density estimation where we're going to explicitly define and solve for our P model or we can also do implicit density estimation where in this case we'll learn a model that can produce samples from P model without explicitly defining it. 06:25
So, why do we care about generative models? 06:47
Why is this a really interesting core problem in unsupervised learning? 06:50
Well there's a lot of things that we can do with generative models. 06:54
If we're able to create realistic samples from the data distributions that we want we can do really cool things with this, right? 06:57
We can generate just beautiful samples to start with so on the left you can see a completely new samples of just generated by these generative models. 07:04
Also in the center here generated samples of images we can also do tasks like super resolution, colorization so hallucinating or filling in these edges with generated ideas of colors and what the purse should look like. 07:14
We can also use generative models of time series data for simulation and planning and so this will be useful in for reinforcement learning applications which we'll talk a bit more about reinforcement learning in a later lecture. 07:32
And training generative models can also enable inference of latent representations. 07:45
Learning latent features that can be useful as general features for downstream tasks. 07:50
So if we look at types of generative models these can be organized into the taxonomy here where we have these two major branches that we talked about, explicit density models and implicit density models. 07:59
And then we can also get down into many of these other sub categories. 08:13
And well we can refer to this figure is adapted from a tutorial on GANs from Ian Goodfellow and so if you're interested in some of these different taxonomy and categorizations of generative models this is a good resource that you can take a look at. 08:19
But today we're going to discuss three of the most popular types of generative models that are in use and in research today. 08:36
And so we'll talk first briefly about pixelRNNs and CNNs And then we'll talk about variational autoencoders. 08:45
These are both types of explicit density models. 08:52
One that's using a tractable density and another that's using an approximate density And then we'll talk about generative adversarial networks, GANs which are a type of implicit density estimation. 08:55
So let's first talk about pixelRNNs and CNNs. 09:12
So these are a type of fully visible belief networks which are modeling a density explicitly so in this case what they do is we have this image data X that we have and we want to model the probability or likelihood of this image P of X. 09:16
Right and so in this case, for these kinds of models, we use the chain rule to decompose this likelihood into a product of one dimensional distribution. 09:32
So we have here the probability of each pixel X I conditioned on all previous pixels X1 through XI - 1. 09:40
and your likelihood all right, your joint likelihood of all the pixels in your image is going to be the product of all of these pixels together, all of these likelihoods together. 09:47
And then once we define this likelihood, in order to train this model we can just maximize the likelihood of our training data under this defined density. 09:58
So if we look at this this distribution over pixel values right, we have this P of XI given all the previous pixel values, well this is a really complex distribution. 10:10
So how can we model this? 10:20
Well we've seen before that if we want to have complex transformations we can do these using neural networks. 10:22
Neural networks are a good way to express complex transformations. 10:29
And so what we'll do is we'll use a neural network to express this complex function that we have of the distribution. 10:32
And one thing you'll see here is that, okay even if we're going to use a neural network for this another thing we have to take care of is how do we order the pixels. 10:43
Right, I said here that we have a distribution for P of XI given all previous pixels but what does all previous the pixels mean? 10:51
So we'll take a look at that. 10:58
So PixelRNN was a model proposed in 2016 that basically defines a way for setting up and optimizing this problem and so how this model works is that we're going to generate pixels starting in a corner of the image. 11:03
So we can look at this grid as basically the pixels of your image and so what we're going to do is start from the pixel in the upper left-hand corner and then we're going to sequentially generate pixels based on these connections from the arrows that you can see here. 11:21
And each of the dependencies on the previous pixels in this ordering is going to be modeled using an RNN or more specifically an LSTM which we've seen before in lecture. 11:37
Right so using this we can basically continue to move forward just moving down a long is diagonal and generating all of these pixel values dependent on the pixels that they're connected to. 11:48
And so this works really well but the drawback here is that this sequential generation, right, so it's actually quite slow to do this. 12:01
You can imagine you know if you're going to generate a new image instead of all of these feed forward networks that we see, we've seen with CNNs. 12:08
Here we're going to have to iteratively go through and generate all these images, all these pixels. 12:15
So a little bit later, after a pixelRNN, another model called pixelCNN was introduced. 12:24
And this has very similar setup as pixelCNN and we're still going to do this image generation starting from the corner of the of the image and expanding outwards but the difference now is that now instead of using 255 :43,074 --> :45,480 an RNN to model all these dependencies we're going to use the CNN instead. 12:30
And we're now going to use a CNN over a a context region that you can see here around in the particular pixel that we're going to generate now. 12:47
Right so we take the pixels around it, this gray area within the region that's already been generated and then we can pass this through a CNN and use that to generate our next pixel value. 12:56
And so what this is going to give is this is going to give This is a CNN, a neural network at each pixel location right and so the output of this is going to be a soft max loss over the pixel values here. 13:11
In this case we have a 0 to 255 and then we can train this by maximizing the likelihood of the training images. 13:22
Right so we say that basically we want to take a training image we're going to do this generation process and at each pixel location we have the ground truth training data image value that we have here and this is a quick basically the label or the the the classification label that we want our pixel to be which of these 255 values and we can train this using a Softmax loss. 13:31
Right and so basically the effect of doing this is that we're going to maximize the likelihood of our training data pixels being generated. 13:56
Okay any questions about this? 14:05
Yes. 14:06
[student's words obscured due to lack of microphone] Yeah, so the question is, I thought we were talking about unsupervised learning, why do we have basically a classification label here? 14:08
The reason is that this loss, this output that we have is the value of the input training data. 14:18
So we have no external labels, right? 14:24
We didn't go and have to manually collect any labels for this, we're just taking our input data and saying that this is what we used for the last function. 14:26
[student's words obscured due to lack of microphone] The question is, is this like bag of words? 14:41
I would say it's not really bag of words, it's more saying that we want where we're outputting a distribution over pixel values at each location of our image right, and what we want to do is we want to maximize the likelihood of our input, our training data being produced, being generated. 14:50
Right so, in that sense, this is why it's using our input data to create our loss. 15:10
So using pixelCNN training is faster than pixelRNN because here now right at every pixel location we want to maximize the value of our, we want to maximize the likelihood of our training data showing up and so we have all of these values already right, just from our training data and so we can do this much faster but a generation time for a test time we want to generate a completely new image right, just starting from the corner and we're not, we're not trying to do any type of learning so in that generation time we still have to generate each of these pixel locations before we can generate the next location. 15:21
And so generation time here it still slow even though training time is faster. 15:59
Question. 16:03
[student's words obscured due to lack of microphone] So the question is, is this training a sensitive distribution to what you pick for the first pixel? 16:04
Yeah, so it is dependent on what you have as the initial pixel distribution and then everything is conditioned based on that. 16:14
So again, how do you pick this distribution? 16:23
So at training time you have these distributions from your training data and then at generation time you can just initialize this with either uniform or from your training data, however you want. 16:26
And then once you have that everything else is conditioned based on that. 16:38
Question. 16:42
[student's words obscured due to lack of microphone] Yeah so the question is is there a way that we define this in this chain rule fashion instead of predicting all the pixels at one time? 16:43
And so we'll see, we'll see models later that do do this, but what the chain rule allows us to do is it allows us to find this very tractable density that we can then basically optimize and do, directly optimizes likelihood Okay so these are some examples of generations from this model and so here on the left you can see generations where the training data is CIFAR-10, CIFAR-10 dataset. 17:14
And so you can see that in general they are starting to capture statistics of natural images. 17:43
You can see general types of blobs and kind of things that look like parts of natural images coming out. 17:48
On the right here it's ImageNet, we can again see samples from here and these are starting to look like natural images but they're still not, there's still room for improvement. 17:56
You can still see that there are differences obviously with regional training images and some of the semantics are not clear in here. 18:09
So, to summarize this, pixelRNNs and CNNs allow you to explicitly compute likelihood P of X. 18:19
It's an explicit density that we can optimize. 18:27
And being able to do this also has another benefit of giving a good evaluation metric. 18:29
You know you can kind of measure how good your samples are by this likelihood of the data that you can compute. 18:34
And it's able to produce pretty good samples but it's still an active area of research and the main disadvantage of these methods is that the generation is sequential and so it can be pretty slow. 18:40
And these kinds of methods have also been used for generating audio for example. 18:53
And you can look online for some pretty interesting examples of this, but again the drawback is that it takes a long time to generate these samples. 18:59
And so there's a lot of work, has been work since then on still on improving pixelCNN performance And so all kinds of different you know architecture changes add the loss function formulating this differently on different types of training tricks And so if you're interested in learning more about this you can look at some of these papers on PixelCNN and then other pixelCNN plus plus better improved version that came out this year. 19:08
Okay so now we're going to talk about another type of generative models call variational autoencoders. 19:37
And so far we saw that pixelCNNs defined a tractable density function, right, using this this definition and based on that we can optimize directly optimize the likelihood of the training data. 19:44
So with variational autoencoders now we're going to define an intractable density function. 19:59
We're now going to model this with an additional latent variable Z and we'll talk in more detail about how this looks. 20:04
And so our data likelihood P of X is now basically has to be this integral right, taking the expectation over all possible values of Z. 20:10
And so this now is going to be a problem. 20:21
We'll see that we cannot optimize this directly. 20:24
And so instead what we have to do is we have to derive and optimize a lower bound on the likelihood instead. 20:26
Yeah, question. 20:33
So the question is is what is Z? 20:35
Z is a latent variable and I'll go through this in much more detail. 20:37
So let's talk about some background first. 20:44
Variational autoencoders are related to a type of unsupervised learning model called autoencoders. 20:48
And so we'll talk little bit more first about autoencoders and what they are and then I'll explain how variational autoencoders are related and build off of this and allow you to generate data. 20:54
So with autoencoders we don't use this to generate data, but it's an unsupervised approach for learning a lower dimensional feature representation from unlabeled training data. 21:05
All right so in this case we have our input data X and then we're going to want to learn some features that we call Z. 21:15
And then we'll have an encoder that's going to be a mapping, a function mapping from this input data to our feature Z. 21:22
And this encoder can take many different forms right, they would generally use neural networks so originally these models have been around, autoencoders have been around for a long time. 21:30
So in the 2000s we used linear layers of non-linearities, then later on we had fully connected deeper networks and then after that we moved on to using CNNs for these encoders. 21:41
So we take our input data X and then we map this to some feature Z. 21:55
And Z we usually have as, we usually specify this to be smaller than X and we perform basically dimensionality reduction because of that. 22:01
So the question who has an idea of why do we want to do dimensionality reduction here? 22:11
Why do we want Z to be smaller than X? 22:17
Yeah. 22:22
[student's words obscured due to lack of microphone] So the answer I heard is Z should represent the most important features in X and that's correct. 22:23
So we want Z to be able to learn features that can capture meaningful factors of variation in the data. 22:32
Right this makes them good features. 22:38
So how can we learn this feature representation? 22:42
Well the way autoencoders do this is that we train the model such that the features can be used to reconstruct our original data. 22:46
So what we want is we want to have input data that we use an encoder to map it to some lower dimensional features Z. 22:55
This is the output of the encoder network, and we want to be able to take these features that were produced based on this input data and then use a decoder a second network and be able to output now something of the same size dimensionality as X and have it be similar to X right so we want to be able to reconstruct the original data. 23:05
And again for the decoder we are basically using same types of networks as encoders so it's usually a little bit symmetric and now we can use CNN networks for most of these. 23:26
Okay so the process is going to be we're going to take our input data right we pass it through our encoder first which is going to be something for example like a four layer convolutional network and then we're going to pass it, get these features and then we're going to pass it through a decoder which is a four layer for example upconvolutional network and then get a reconstructed data out at the end of this. 23:41
Right in the reason why we have a convolutional network for the encoder and an upconvolutional network for the decoder is because at the encoder we're basically taking it from this high dimensional input to these lower dimensional features and now we want to go the other way go from our low dimensional features back out to our high dimensional reconstructed input. 24:04
And so in order to get this effect that we said we wanted before of being able to reconstruct our input data we'll use something like an L2 loss function. 24:28
Right that basically just says let me make my pixels of my input data to be the same as my, my pixels in my reconstructed data to be the same as the pixels of my input data. 24:39
An important thing to notice here, this relates back to a question that we had earlier, is that even though we have this loss function here, there's no, there's no external labels that are being used in training this. 24:51
All we have is our training data that we're going to use both to pass through the network as well as to compute our loss function. 25:02
So once we have this after training this model what we can do is we can throw away this decoder. 25:13
All this was used was too to be able to produce our reconstruction input and be able to compute our loss function. 25:19
And we can use the encoder that we have which produces our feature mapping and we can use this to initialize a supervised model. 25:26
Right and so for example we can now go from this input to our features and then have an additional classifier network on top of this that now we can use to output a class label for example for classification problem we can have external labels from here and use our standard loss functions like Softmax. 25:34
And so the value of this is that we basically were able to use a lot of unlabeled training data to try and learn good general feature representations. 25:55
Right, and now we can use this to initialize a supervised learning problem where sometimes we don't have so much data we only have small data. 26:04
And we've seen in previous homeworks and classes that with small data it's hard to learn a model, right? 26:12
You can have over fitting and all kinds of problems and so this allows you to initialize your model first with better features. 26:19
Okay so we saw that autoencoders are able to reconstruct data and are able to, as a result, learn features to initialize, that we can use to initialize a supervised model. 26:31
And we saw that these features that we learned have this intuition of being able to capture factors of variation in the training data. 26:42
All right so based on this intuition of okay these, we can have this latent this vector Z which has factors of variation in our training data. 26:50
Now a natural question is well can we use a similar type of setup to generate new images? 26:58
And so now we will talk about variational autoencoders which is a probabillstic spin on autoencoders that will let us sample from the model in order to generate new data. 27:06
Okay any questions on autoencoders first? 27:15
Okay, so variational autoencoders. 27:20
All right so here we assume that our training data that we have X I from one to N is generated from some underlying, unobserved latent representation Z. 27:22
Right, so it's this intuition that Z is some vector right which element of Z is capturing how little or how much of some factor of variation that we have in our training data. 27:34
Right so the intuition is, you know, maybe these could be something like different kinds of attributes. 27:48
Let's say we're trying to generate faces, it could be how much of a smile is on the face, it could be position of the eyebrows hair orientation of the head. 27:52
These are all possible types of latent factors that could be learned. 28:02
Right, and so our generation process is that we're going to sample from a prior over Z. 28:08
Right so for each of these attributes for example, you know, how much smile that there is, we can have a prior over what sort of distribution we think that there should be for this so, a gaussian is something that's a natural prior that we can use for each of these factors of Z and then we're going to generate our data X by sampling from a conditional, conditional distribution P of X given Z. 28:13
So we sample Z first, we sample a value for each of these latent factors and then we'll use that and sample our image X from here. 28:40
And so the true parameters of this generation process are theta, theta star right? 28:51
So we have the parameters of our prior and our conditional distributions and what we want to do is in order to have a generative model be able to generate new data we want to estimate these parameters of our true parameters Okay so let's first talk about how should we represent this model. 28:57
All right, so if we're going to have a model for this generator process, well we've already said before that we can choose our prior P of Z to be something simple. 29:20
Something like a Gaussian, right? 29:27
And this is the reasonable thing to choose for for latent attributes. 29:28
Now for our conditional distribution P of X given Z this is much more complex right, because we need to use this to generate an image and so for P of X given Z, well as we saw before, when we have some type of complex function that we want to represent we can represent this with a neural network. 29:35
And so that's a natural choice for let's try and model P of X given Z with a neural network. 29:53
And we're going to call this the decoder network. 30:00
Right, so we're going to think about taking some latent representation and trying to decode this into the image that it's specifying. 30:02
So now how can we train this model? 30:10
Right, we want to be able to train this model so that we can learn an estimate of these parameters. 30:13
So if we remember our strategy from training generative models, back from are fully visible belief networks, our pixelRNNs and CNNs, a straightforward natural strategy is to try and learn these model parameters in order to maximize the likelihood of the training data. 30:19
Right, so we saw earlier that in this case, with our latent variable Z, we're going to have to write out P of X taking expectation over all possible values of Z which is continuous and so we get this expression here. 30:35
Right so now we have it with this latent Z and now if we're going to, if you want to try and maximize its likelihood, well what's the problem? 30:46
Can we just take this take gradients and maximize this likelihood? 30:55
[student's words obscured due to lack of microphone] Right, so this integral is not going to be tractable, that's correct. 31:01
So let's take a look at this in a little bit more detail. 31:10
Right, so we have our data likelihood term here. 31:12
And the first time is P of Z. 31:15
And here we already said earlier, we can just choose this to be a simple Gaussian prior, so this is fine. 31:18
P of X given Z, well we said we were going to specify a decoder neural network. 31:24
So given any Z, we can get P of X given Z from here. 31:29
It's the output of our neural network. 31:32
But then what's the problem here? 31:35
Okay this was supposed to be a different unhappy face but somehow I don't know what happened, in the process of translation, it turned into a crying black ghost but what this is symbolizing is that basically if we want to compute P of X given Z for every Z this is now intractable right, we cannot compute this integral. 31:38
So data likelihood is intractable and it turns out that if we look at other terms in this model if we look at our posterior density, So P of our posterior of Z given X, then this is going to be P of X given Z times P of Z over P of X by Bayes' rule and this is also going to be intractable, right. 32:04
We have P of X given Z is okay, P of Z is okay, but we have this P of X our likelihood which has the integral and it's intractable. 32:25
So we can't directly optimizes this. 32:36
but we'll see that a solution, a solution that will enable us to learn this model is if in addition to using a decoder network defining this neural network to model P of X given Z. 32:37
If we now define an additional encoder network Q of Z given X we're going to call this an encoder because we want to turn our input X into, get the likelihood of Z given X, we're going to encode this into Z. 32:50
And defined this network to approximate the P of Z given X. 33:06
Right this was posterior density term now is also intractable. 33:12
If we use this additional network to approximate this then we'll see that this will actually allow us to derive a lower bound on the data likelihood that is tractable and which we can optimize. 33:15
Okay so first just to be a little bit more concrete about these encoder and decoder networks that I specified, in variational autoencoders we want the model probabilistic generation of data. 33:29
So in autoencoders we already talked about this concept of having an encoder going from input X to some feature Z and a decoder network going from Z back out to some image X. 33:40
And so here we go to again have an encoder network and a decoder network but we're going to make these probabilistic. 33:53
So now our encoder network Q of Z given X with parameters phi are going to output a mean and a diagonal covariance and from here, this will be the direct outputs of our encoder network and the same thing for our decoder network which is going to start from Z and now it's going to output the mean and the diagonal covariance of some X, same dimension as the input given Z And then this decoder network has different parameters theta. 33:58
And now in order to actually get our Z and our, This should be Z given X and X given Z. 34:31
We'll sample from these distributions. 34:40
So now our encoder and our decoder network are producing distributions over Z and X respectively and will sample from this distribution in order to get a value from here. 34:42
So you can see how this is taking us on the direction towards being able to sample and generate new data. 34:52
And just one thing to note is that these encoder and decoder networks, you'll also hear different terms for them. 34:59
The encoder network can also be kind of recognition or inference network because we're trying to form inference of this latent representation of Z given X and then for the decoder network, this is what we'll use to perform generation. 35:05
Right so you also hear generation network being used. 35:18
Okay so now equipped with our encoder and decoder networks, let's try and work out the data likelihood again. 35:24
and we'll use the log of the data likelihood here. 35:31
So we'll see that if we want the log of P of X right we can write this out as like a P of X but take the expectation with respect to Z. 35:35
So Z samples from our distribution of Q of Z given X that we've now defined using the encoder network. 35:44
And we can do this because P of X doesn't depend on Z. 35:52
Right 'cause Z is not part of that. 35:55
And so we'll see that taking the expectation with respect to Z is going to come in handy later on. 35:58
Okay so now from this original expression we can now expand it out to be log of P of X given Z, P of Z over P of Z given X using Bayes' rule. 36:06
And so this is just directly writing this out. 36:17
And then taking this we can also now multiply it by a constant. 36:20
Right, so Q of Z given X over Q of Z given X. 36:24
This is one we can do this. 36:28
It doesn't change it but it's going to be helpful later on. 36:30
So given that what we'll do is we'll write it out into these three separate terms. 36:33
And you can work out this math later on by yourself but it's essentially just using logarithm rules taking all of these terms that we had in the line above and just separating it out into these three different terms that will have nice meanings. 36:39
Right so if we look at this, the first term that we get separated out is log of P given X and then expectation of log of P given X and then we're going to have two KL terms, right. 36:56
This is basically KL divergence term to say how close these two distributions are. 37:07
So how close is a distribution Q of Z given X to P of Z. 37:14
So it's just the, it's exactly this expectation term above. 37:19
And it's just a distance metric for distributions. 37:24
And so we'll see that, right, we saw that these are nice KL terms that we can write out. 37:30
And now if we look at these three terms that we have here, the first term is P of X given Z, which is provided by our decoder network. 37:36
And we're able to compute an estimate of these term through sampling and we'll see that we can do a sampling that's differentiable through something called the re-parametrization trick which is a detail that you can look at this paper if you're interested. 37:45
But basically we can now compute this term. 37:59
And then these KL terms, the second KL term is a KL between two Gaussians, so our Q of Z given X, remember our encoder produced this distribution which had a mean and a covariance, it was a nice Gaussian. 38:02
And then also our prior P of Z which is also a Gaussian. 38:16
And so this has a nice, when you have a KL of two Gaussians you have a nice closed form solution that you can have. 38:19
And then this third KL term now, this is a KL of Q given X with a P of Z given X. 38:25
But we know that P of Z given X was this intractable posterior that we saw earlier, right? 38:32
That we didn't want to compute that's why we had this approximation using Q. 38:36
And so this term is still is a problem. 38:41
But one thing we do know about this term is that KL divergence, it's a distance between two distributions is always greater than or equal to zero by definition. 38:44
And so what we can do with this is that, well what we have here, the two terms that we can work nicely with, this is a, this is a tractable lower bound which we can actually take gradient of and optimize. 38:57
P of X given Z is differentiable and the KL terms are also, the close form solution is also differentiable. 39:10
And this is a lower bound because we know that the KL term on the right, the ugly one is greater than or equal it zero. 39:16
So we have a lower bound. 39:24
And so what we'll do to train a variational autoencoder is that we take this lower bound and we instead optimize and maximize this lower bound instead. 39:27
So we're optimizing a lower bound on the likelihood of our data. 39:37
So that means that our data is always going to have a likelihood that's at least as high as this lower bound that we're maximizing. 39:42
And so we want to find the parameters theta, estimate parameters theta and phi that allows us to maximize this. 39:49
And then one last sort of intuition about this lower bound that we have is that this first term is expectation over all samples of Z sampled from passing our X through the encoder network sampling Z taking expectation over all of these samples of likelihood of X given Z and so this is a reconstruction, right? 40:03
This is basically saying, if I want this to be big I want this likelihood P of X given Z to be high, so it's kind of like trying to do a good job reconstructing the data. 40:26
So similar to what we had from our autoencoder before. 40:37
But the second term here is saying make this KL small. 40:40
Make our approximate posterior distribution close to our prior distribution. 40:46
And this basically is saying that well we want our latent variable Z to be following this, have this distribution type, distribution shape that we would like it to have. 40:51
Okay so any questions about this? 41:08
I think this is a lot of math that if you guys are interested you should go back and kind of work through all of the derivations yourself. 41:12
Yeah. 41:19
[student's words obscured due to lack of microphone] So the question is why do we specify the prior and the latent variables as Gaussian? 41:20
And the reason is that well we're defining some sort of generative process right, of sampling Z first and then sampling X first. 41:29
And defining it as a Gaussian is a reasonable type of prior that we can say makes sense for these types of latent attributes to be distributed according to some sort of Gaussian, and then this lets us now then optimize our model. 41:35
Okay, so we talked about how we can deride this lower bound and now let's put this all together and walk through the process of the training of the AE. 41:55
Right so here's the bound that we want to optimize, to maximize. 42:06
And now for a forward pass. 42:10
We're going to proceed in the following manner. 42:12
We have our input data X, so we'll a mini batch of input data. 42:14
And then we'll pass it through our encoder network so we'll get Q of Z given X. 42:20
And from this Q of Z given X, this'll be the terms that we use to compute the KL term. 42:28
And then from here we'll sample Z from this distribution of Z given X so we have a sample of the latent factors that we can infer from X. 42:35
And then from here we're going to pass a Z through another, our second decoder network. 42:50
And from the decoder network we'll get this output for the mean and variance on our distribution for X given Z and then finally we can sample now our X given Z from this distribution and here this will produce some sample output. 42:54
And when we're training we're going to take this distribution and say well our loss term is going to be log of our training image pixel values given Z. 43:12
So our loss functions going to say let's maximize the likelihood of this original input being reconstructed. 43:23
And so now for every mini batch of input we're going to compute this forward pass. 43:32
Get all these terms that we need and then this is all differentiable so then we just backprop though all of this and then get our gradient, we update our model and we use this to continuously update our parameters, our generator and decoder network parameters theta and phi in order to maximize the likelihood of the trained data. 43:35
Okay so once we've trained our VAE, so now to generate data, what we can do is we can use just the decoder network. 43:58
All right, so from here we can sample Z now, instead of sampling Z from this posterior that we had during training, while during generation we sample from our true generative process. 44:05
So we sample from our prior that we specify. 44:15
And then we're going to then sample our data X from here. 44:18
And we'll see that this can produce, in this case, train on MNIST, these are samples of digits generated from a VAE trained on MNIST. 44:25
And you can see that, you know, we talked about this idea of Z representing these latent factors where we can bury Z right according to our sample from different parts of our prior and then get different kind of interpretable meanings from here. 44:36
So here we can see that this is the data manifold for two dimensional Z. 44:52
So if we have a two dimensional Z and we take Z and let's say some range from you know, from different percentiles of the distribution, and we vary Z1 and we vary Z2, then you can see how the image generated from every combination of Z1 and Z2 that we have here, you can see it's transitioning smoothly across all of these different variations. 44:57
And you know our prior on Z was, it was diagonal, so we chose this in order to encourage this to be independent latent variables that can then encode interpretable factors of variation. 45:24
So because of this now we'll have different dimensions of Z, encoding different interpretable factors of variation. 45:37
So, in this example train now on Faces, we'll see as we vary Z1, going up and down, you'll see the amount of smile changing. 45:44
So from a frown at the top to like a big smile at the bottom and then as we go vary Z2, from left to right, you can see the head pose changing. 45:54
From one direction all the way to the other. 46:04
And so one additional thing I want to point out is that as a result of doing this, these Z variables are also good feature representations. 46:09
Because they encode how much of these different these different interpretable semantics that we have. 46:19
And so we can use our Q of Z given X, the encoder that we've learned and give it an input images X, we can map this to Z and use the Z as features that we can use for downstream tasks like supervision, or like classification or other tasks. 46:26
Okay so just another couple of examples of data generated from VAEs. 46:47
So on the left here we have data generated on CIFAR-10, trained on CIFAR-10, and then on the right we have data trained and generated on Faces. 46:51
And we'll see so we can see that in general VAEs are able to generate recognizable data. 47:02
One of the main drawbacks of VAEs is that they tend to still have a bit of a blurry aspect to them. 47:08
You can see this in the faces and so this is still an active area of research. 47:15
Okay so to summarize VAEs, they're a probabilistic spin on traditional autoencoders. 47:22
So instead of deterministically taking your input X and going to Z, feature Z and then back to reconstructing X, now we have this idea of distributions and sampling involved which allows us to generate data. 47:28
And in order to train this, VAEs are defining an intractable density. 47:43
So we can derive and optimize a lower bound, a variational lower bound, so variational means basically using approximations to handle these types of intractable expressions. 47:48
And so this is why this is called a variational autoencoder. 47:59
And so some of the advantages of this approach is that VAEs are, they're a principled approach to generative models and they also allow this inference query so being able to infer things like Q of Z given X. 48:03
That we said could be useful feature representations for other tasks. 48:17
So disadvantages of VAEs are that while we're maximizing the lower bound of the likelihood, which is okay like you know in general this is still pushing us in the right direction and there's more other theoretical analysis of this. 48:23
So you know, it's doing okay, but it's maybe not still as direct an optimization and evaluation as the pixel RNNs and CNNs that we saw earlier, but which had, and then, also the VAE samples are tending to be a little bit blurrier and of lower quality compared to state of the art samples that we can see from other generative models such as GANs that we'll talk about next. 48:37
And so VAEs now are still, they're still an active area of research. 49:04
People are working on more flexible approximations, so richer approximate posteriors, so instead of just a diagonal Gaussian some richer functions for this. 49:11
And then also, another area that people have been working on is incorporating more structure in these latent variables. 49:20
So now we had all of these independent latent variables but people are working on having modeling structure in here, groupings, other types of structure. 49:26
Okay, so yeah, question. 49:41
[student's words obscured due to lack of microphone] Yeah, so the question is we're deciding the dimensionality of the latent variable. 49:44
Yeah, that's something that you specify. 49:51
Okay, so we've talked so far about pixelCNNs and VAEs and now we'll take a look at a third and very popular type of generative model called GANs. 49:55
So the models that we've seen so far, pixelCNNs and RNNs define a tractable density function. 50:10
And they optimize the likelihood of the trained data. 50:15
And then VAEs in contrast to that now have this additional latent variable Z that they define in the generative process. 50:19
And so having the Z has a lot of nice properties that we talked about, but they are also cause us to have this intractable density function that we can't optimize directly and so we derive and optimize a lower bound on the likelihood instead. 50:27
And so now what if we just give up on explicitly modeling this density at all? 50:43
And we say well what we want is just the ability to sample and to have nice samples from our distribution. 50:48
So this is the approach that GANs take. 50:56
So in GANs we don't work with an explicit density function, but instead we're going to take a game-theoretic approach and we're going to learn to generate from our training distribution through a set up of a two player game, and we'll talk about this in more detail. 50:59
So, in the GAN set up we're saying, okay well what we want, what we care about is we want to be able to sample from a complex high dimensional training distribution. 51:15
So if we think about well we want to produce samples from this distribution, there's no direct way that we can do this. 51:24
We have this very complex distribution, we can't just take samples from here. 51:31
So the solution that we're going to take is that we can, however, sample from simpler distributions. 51:35
For example random noise, right? 51:42
Gaussians are, these we can sample from. 51:44
And so what we're going to do is we're going to learn a transformation from these simple distributions directly to the training distribution that we want. 51:46
So the question, what can we used to represent this complex distribution? 51:58
Neural network, I heard the answer. 52:06
So when we want to model some kind of complex function or transformation we use a neural network. 52:07
Okay so what we're going to do is we're going to take in the GAN set up, we're going to take some input which is a vector of some dimension that we specify of random noise and then we're going to pass this through a generator network, and then we're going to get as output directly a sample from the training distribution. 52:14
So every input of random noise we want to correspond to a sample from the training distribution. 52:33
And so the way we're going to train and learn this network is that we're going to look at this as a two player game. 52:41
So we have two players, a generator network as well as an additional discriminator network that I'll show next. 52:48
And our generator network is going to try to, as player one, it's going to try to fool the discriminator by generating real looking images. 52:54
And then our second player, our discriminator network is then going to try to distinguish between real and fake images. 53:04
So it wants to do as good a job as possible of trying to determine which of these images are counterfeit or fake images generated by this generator. 53:12
Okay so what this looks like is, we have our random noise going to our generator network, generator network is generating these images that we're going to call, they're fake from our generator. 53:25
And then we're going to also have real images that we take from our training set and then we want the discriminator to be able to distinguish between real and fake images. 53:36
Output real and fake for each images. 53:50
So the idea is if we're able to have a very good discriminator, we want to train a good discriminator, if it can do a good job of discriminating real versus fake, and then if our generator network is able to generate, if it's able to do well and generate fake images that can successfully fool this discriminator, then we have a good generative model. 53:52
We're generating images that look like images from the training set. 54:13
Okay, so we have these two players and so we're going to train this jointly in a minimax game formulation. 54:19
So this minimax objective function is what we have here. 54:25
We're going to take, it's going to be minimum over theta G our parameters of our generator network G, and maximum over parameter Zeta of our Discriminator network D, of this objective, right, these terms. 54:28
And so if we look at these terms, what this is saying is well this first thing, expectation over data of log of D given X. 54:47
This log of D of X is the discriminator output for real data X. 54:56
This is going to be likelihood of real data being real from the data distribution P data. 55:01
And then the second term here, expectation of Z drawn from P of Z, Z drawn from P of Z means samples from our generator network and this term D of G of Z that we have here is the output of our discriminator for generated fake data for our, what does the discriminator output of G of Z which is our fake data. 55:09
And so if we think about this is trying to do, our discriminator wants to maximize this objective, right, it's a max over theta D such that D of X is close to one. 55:36
It's close to real, it's high for the real data. 55:49
And then D of G of X, what it thinks of the fake data on the left here is small, we want this to be close to zero. 55:53
So if we're able to maximize this, this means discriminator is doing a good job of distinguishing between real and zero. 56:02
Basically classifying between real and fake data. 56:09
And then our generator, here we want the generator to minimize this objective such that D of G of Z is close to one. 56:13
So if this D of G of Z is close to one over here, then the one minus side is small and basically we want to, if we minimize this term then, then it's having discriminator think that our fake data's actually real. 56:22
So that means that our generator is producing real samples. 56:39
Okay so this is the important objective of GANs to try and understand so are there any questions about this? 56:44
[student's words obscured due to lack of microphone] I'm not sure I understand your question, can you, [student's words obscured due to lack of microphone] Yeah, so the question is is this basically trying to have the first network produce real looking images that our second network, the discriminator cannot distinguish between. 56:51
Okay, so the question is how do we actually label the data or do the training for these networks. 57:30
We'll see how to train the networks next. 57:36
But in terms of like what is the data label basically, this is unsupervised, so there's no data labeling. 57:39
But data generated from the generator network, the fake images have a label of basically zero or fake. 57:46
And we can take training images that are real images and this basically has a label of one or real. 57:52
So when we have, the loss function for our discriminator is using this. 58:00
It's trying to output a zero for the generator images and a one for the real images. 58:04
So there's no external labels. 58:09
[student's words obscured due to lack of microphone] So the question is the label for the generator network will be the output for the discriminator network. 58:12
The generator is not really doing, it's not really doing classifications necessarily. 58:22
What it's objective is is here, D of G of Z, it wants this to be high. 58:29
So given a fixed discriminator, it wants to learn the generator parameter such that this is high. 58:35
So we'll take the fixed discriminator output and use that to do the backprop. 58:42
Okay so in order to train this, what we're going to do is we're going to alternate between gradient ascent on our discriminator, so we're trying to learn theta beta to maximizing this objective. 58:51
And then gradient descent on the generator. 59:05
So taking gradient ascent on these parameters theta G such that we're minimizing this and this objective. 59:08
And here we are only taking this right part over here because that's the only part that's dependent on theta G parameters. 59:15
Okay so this is how we can train this GAN. 59:26
We can alternate between training our discriminator and our generator in this game, each trying to fool the other or generator trying to fool the discriminator. 59:30
But one thing that is important to note is that in practice this generator objective as we've just defined actually doesn't work that well. 59:40
And the reason for this is we have to look at the loss landscape. 59:50
So if we look at the loss landscape over here for D of G of X, if we apply here one minus D of G of X which is what we want to minimize for the generator, it has this shape here. 59:55
So we want to minimize this and it turns out the slope of this loss is actually going to be higher towards the right. 60:12
High when D of G of Z is closer to one. 60:21
So that means that when our generator is doing a good job of fooling the discriminator, we're going to have a high gradient, more higher gradient terms. 60:26
And on the other hand when we have bad samples, our generator has not learned a good job yet, it's not good at generating yet, then this is when the discriminator can easily tell it's now closer to this zero region on the X axis. 60:36
Then here the gradient's relatively flat. 60:53
And so what this actually means is that our our gradient signal is dominated by region where the sample is already pretty good. 60:55
Whereas we actually want it to learn a lot when the samples are bad, right? 61:05
These are training samples that we want to learn from. 61:08
And so in order to, so this basically makes it hard to learn and so in order to improve learning, what we're going to do is define a different, slightly different objective function for the gradient. 61:12
Where now we're going to do gradient ascent instead. 61:26
And so instead of minimizing the likelihood of our discriminator being correct, which is what we had earlier, now we'll kind of flip it and say let's maximize the likelihood of our discriminator being wrong. 61:30
And so this will produce this objective here of maximizing, maximizing log of D of G of X. 61:40
And so, now basically we want to, there should be a negative sign here. 61:50
But basically we want to now maximize this flip objective instead and what this now does is if we plot this function on the right here, then we have a high gradient signal in this region on the left where we have bad samples, and now the flatter region is to the right where we would have good samples. 61:59
So now we're going to learn more from regions of bad samples. 62:23
And so this has the same objective of fooling the discriminator but it actually works much better in practice and for a lot of work on GANs that are using these kind of vanilla GAN formulation is actually using this objective. 62:26
Okay so just an aside on that is that jointly training these two networks is challenging and can be unstable. 62:44
So as we saw here, like we're alternating between training a discriminator and training a generator. 62:53
This type of alternation is, basically it's hard to learn two networks at once and there's also this issue of depending on what our loss landscape looks at, it can affect our training dynamics. 62:59
So an active area of research still is how can we choose objectives with better loss landscapes that can help training and make it more stable? 63:13
Okay so now let's put this all together and look at the full GAN training algorithm. 63:26
So what we're going to do is for each iteration of training we're going to first train the generation, train the discriminator network a bit and then train the generator network. 63:31
So for k steps of training the discriminator network we'll sample a mini batch of noise samples from our noise prior Z and then also sample a mini batch of real samples from our training data X. 63:41
So what we'll do is we'll pass the noise through our generator, we'll get our fake images out. 63:57
So we have a mini batch of fake images and mini batch of real images. 64:04
And then we'll pick a gradient step on the discriminator using this mini batch, our fake and our real images and then update our discriminator parameters. 64:08
And use this and do this a certain number of iterations to train the discriminator for a bit basically. 64:17
And then after that we'll go to our second step which is training the generator. 64:24
And so here we'll sample just a mini batch of noise samples. 64:28
We'll pass this through our generator and then now we want to do backpop on this to basically optimize our generator objective that we saw earlier. 64:32
So we want to have our generator fool our discriminator as much as possible. 64:45
And so we're going to alternate between these two steps of taking gradient steps for our discriminator and for the generator. 64:50
And I said for k steps up here, for training the discriminator and so this is kind of a topic of debate. 64:59
Some people think just having one iteration of discriminator one type of discriminator, one type of generator is best. 65:08
Some people think it's better to train the discriminator for a little bit longer before switching to the generator. 65:15
There's no real clear rule and it's something that people have found different things to work better depending on the problem. 65:20
And one thing I want to point out is that there's been a lot of recent work that alleviates this problem and makes it so you don't have to spend so much effort trying to balance how the training of these two networks. 65:30
It'll have more stable training and give better results. 65:45
And so Wasserstein GAN is an example of a paper that was an important work towards doing this. 65:47
Okay so looking at the whole picture we've now trained, we have our network setup, we've trained both our generator network and our discriminator network and now after training for generation, we can just take our generator network and use this to generate new images. 66:00
So we just take noise Z and pass this through and generate fake images from here. 66:16
Okay and so now let's look at some generated samples from these GANs. 66:23
So here's an example of trained on MNIST and then on the right on Faces. 66:28
And for each of these you can also see, just for visualization the closest, on the right, the nearest neighbor from the training set to the column right next to it. 66:33
And so you can see that we're able to generate very realistic samples and it never directly memorizes the training set. 66:43
And here are some examples from the original GAN paper on CIFAR images. 66:51
And these are still fairly, not such good quality yet, these were, the original work is from 2014, so these are some older, simpler networks. 66:56
And these were using simple, fully connected networks. 67:07
And so since that time there's been a lot of work on improving GANs. 67:12
One example of a work that really took a big step towards improving the quality of samples is this work from Alex Radford in ICLR 2016 on adding convolutional architectures to GANs. 67:18
In this paper there was a whole set of guidelines on architectures for helping GANs to produce better samples. 67:33
So you can look at this for more details. 67:42
This is an example of a convolutional architecture that they're using which is going from our input Z noise vector Z and transforming this all the way to the output sample. 67:46
So now from this large convolutional architecture we'll see that the samples from this model are really starting to look very good. 68:00
So this is trained on a dataset of bedrooms and we can see all kinds of very realistic fancy looking bedrooms with windows and night stands and other furniture around there so these are some really pretty samples. 68:08
And we can also try and interpret a little bit of what these GANs are doing. 68:26
So in this example here what we can do is we can take two points of Z, two different random noise vectors and let's just interpolate between these points. 68:32
And each row across here is an interpolation from one random noise Z to another random noise vector Z and you can see that as it's changing, it's smoothly interpolating the image as well all the way over. 68:42
And so something else that we can do is we can see that, well, let's try to analyze further what these vectors Z mean, and so we can try and do vector math on here. 68:59
So what this experiment does is it says okay, let's take some images of smiling, samples of smiling women images and then let's take some samples of neutral women and then also some samples of neutral men. 69:10
And so let's try and do take the average of the Z vectors that produced each of these samples and if we, Say we take this, mean vector for the smiling women, subtract the mean vector for the neutral women and add the mean vector for the neutral man, what do we get? 69:28
And we get samples of smiling man. 69:46
So we can take the Z vector produced there, generate samples and get samples of smiling men. 69:49
And we can have another example of this. 69:57
Of glasses man minus no glasses man and plus glasses women. 69:59
And get women with glasses. 70:05
So here you can see that basically the Z has this type of interpretability that you can use this to generate some pretty cool examples. 70:08
Okay so this year, 2017 has really been the year of the GAN. 70:20
There's been tons and tons of work on GANs and it's really sort of exploded and gotten some really cool results. 70:24
So on the left here you can see people working on better training and generation. 70:33
So we talked about improving the loss functions, more stable training and this was able to get really nice generations here of different types of architectures on the bottom here really crisp high resolution faces. 70:38
With GANs you can also do, there's also been models on source to try to domain transfer and conditional GANs. 70:54
And so here, this is an example of source to try to get domain transfer where, for example in the upper part here we are trying to go from source domain of horses to an output domain of zebras. 71:01
So we can take an image of horses and train a GAN such that the output is going to be the same thing but now zebras in the same image setting as the horses and go the other way around. 71:14
We can transform apples into oranges. 71:29
And also the other way around. 71:33
We can also use this to do photo enhancement. 71:35
So producing these, really taking a standard photo and trying to make really nice, as if you had, pretending that you have a really nice expensive camera. 71:38
That you can get the nice blur effects. 71:48
On the bottom here we have scene changing, so transforming an image of Yosemite from the image in winter time to the image in summer time. 71:52
And there's really tons of applications. 72:03
So on the right here there's more. 72:05
There's also going from a text description and having a GAN that's now conditioned on this text description and producing an image. 72:08
So there's something here about a small bird with a pink breast and crown and now we're going to generate images of this. 72:18
And there's also examples down here of filling in edges. 72:26
So given conditions on some sketch that we have, can we fill in a color version of what this would look like. 72:31
Can we take a Google, a map grid and put something that looks like Google Earth on, and turn it into something that looks like Google Earth. 72:40
Go in and hallucinate all of these buildings and trees and so on. 72:52
And so there's lots of really cool examples of this. 72:56
And there's also this website for pics to pics which did a lot of these kind of conditional GAN type examples. 72:59
I encourage you to go look at for more interesting applications that people have done with GANs. 73:08
And in terms of research papers there's also there's a huge number of papers about GANs this year now. 73:17
There's a website called the GAN Zoo that kind of is trying to compile a whole list of these. 73:26
And so here this has only taken me from A through C on the left here and through like L on the right. 73:31
So it won't even fit on the slide. 73:37
There's tons of papers as well that you can look at if you're interested. 73:40
And then one last pointer is also for tips and tricks for training GANs, here's a nice little website that has pointers if you're trying to train these GANs in practice. 73:44
Okay, so summary of GANs. 74:01
GANs don't work with an explicit density function. 74:04
Instead we're going to represent this implicitly through samples and they take a game-theoretic approach to training so we're going to learn to generate from our training distribution through a two player game setup. 74:06
And the pros of GANs are that they're really having gorgeous state of the art samples and you can do a lot with these. 74:18
The cons are that they are trickier and more unstable to train, we're not just directly optimizing a one objective function that we can just do backpop and train easily. 74:26
Instead we have these two networks that we're trying to balance training with so it can be a bit more unstable. 74:41
And we also can lose out on not being able to do some of the inference queries, P of X, P of Z given X that we had for example in our VAE. 74:47
And GANs are still an active area of research, this is a relatively new type of model that we're starting to see a lot of and you'll be seeing a lot more of. 74:57
And so people are still working now on better loss functions more stable training, so Wasserstein GAN for those of you who are interested is basically an improvement in this direction. 75:07
That now a lot of people are also using and basing models off of. 75:22
There's also other works like LSGAN, Least Square's GAN, Least Square's GAN and others. 75:26
So you can look into this more. 75:31
And a lot of times for these new models in terms of actually implementing this, they're not necessarily big changes. 75:32
They're different loss functions that you can change a little bit and get like a big improvement in training. 75:39
And so this is, some of these are worth looking into and you'll also get some practice on your homework assignment. 75:44
And there's also a lot of work on different types of conditional GANs and GANs for all kinds of different problem setups and applications. 75:51
Okay so a recap of today. 76:01
We talked about generative models. 76:03
We talked about three of the most common kinds of generative models that people are using and doing research on today. 76:05
So we talked first about pixelRNN and pixelCNN, which is an explicit density model. 76:12
It optimizes the exact likelihood and it produces good samples but it's pretty inefficient because of the sequential generation. 76:17
We looked at VAE which optimizes a variational or lower bound on the likelihood and this also produces useful a latent representation. 76:26
You can do inference queries. 76:35
But the example quality is still not the best. 76:36
So even though it has a lot of promise, it's still a very active area of research and has a lot of open problems. 76:40
And then GANs we talked about is a game-theoretic approach for training and it's what currently achieves the best state of the art examples. 76:47
But it can also be tricky and unstable to train and it loses out a bit on the inference queries. 76:57
And so what you'll also see is a lot of recent work on combinations of these kinds of models. 77:05
So for example adversarial autoencoders. 77:10
Something like a VAE trained with an additional adversarial loss on top which improves the sample quality. 77:12
There's also things like pixelVAE is now a combination of pixelCNN and VAE so there's a lot of combinations basically trying to take the best of all these worlds and put them together. 77:18
Okay so today we talked about generative models and next time we'll talk about reinforcement learning. 77:32
Thanks. 77:38
