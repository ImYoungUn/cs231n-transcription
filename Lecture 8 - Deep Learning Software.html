<html>
	<head>
		<link rel="stylesheet" href="css/style.css">
		<script>
			var url="https://www.youtube.com/watch?v=6SlgtELqOWc&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=8"
		</script>
		<script src="js/script.js"></script>
	</head>
	<body>
		<div class="lecture">
<h1>Lecture 8 - Deep Learning Software</h1>
<br/>
<p>Hello? <span onclick="goVideo(this)" class="timestamp">00:09</span></p>
<p>Okay, it's after 12, so I want to get started. <span onclick="goVideo(this)" class="timestamp">00:10</span></p>
<p>So today, lecture eight, we're going to talk about deep learning software. <span onclick="goVideo(this)" class="timestamp">00:13</span></p>
<p>This is a super exciting topic because it changes a lot every year. <span onclick="goVideo(this)" class="timestamp">00:17</span></p>
<p>But also means it's a lot of work to give this lecture 'cause it changes a lot every year. <span onclick="goVideo(this)" class="timestamp">00:21</span></p>
<p>But as usual, a couple administrative notes before we dive into the material. <span onclick="goVideo(this)" class="timestamp">00:25</span></p>
<p>So as a reminder the project proposals for your course projects were due on Tuesday. <span onclick="goVideo(this)" class="timestamp">00:30</span></p>
<p>So hopefully you all turned that in, and hopefully you all have a somewhat good idea of what kind of projects you want to work on for the class. <span onclick="goVideo(this)" class="timestamp">00:34</span></p>
<p>So we're in the process of assigning TA's to projects based on what the project area is and the expertise of the TA's. <span onclick="goVideo(this)" class="timestamp">00:42</span></p>
<p>So we'll have some more information about that in the next couple days I think. <span onclick="goVideo(this)" class="timestamp">00:50</span></p>
<p>We're also in the process of grading assignment one, so stay tuned and we'll get those grades back to you as soon as we can. <span onclick="goVideo(this)" class="timestamp">00:54</span></p>
<p>Another reminder is that assignment two has been out for a while. <span onclick="goVideo(this)" class="timestamp">01:00</span></p>
<p>That's going to be due next week, a week from today, Thursday. <span onclick="goVideo(this)" class="timestamp">01:04</span></p>
<p>And again, when working on assignment two, remember to stop your Google Cloud instances when you're not working to try to preserve your credits. <span onclick="goVideo(this)" class="timestamp">01:08</span></p>
<p>And another bit of confusion, I just wanted to re-emphasize is that for assignment two you really only need to use GPU instances for the last notebook. <span onclick="goVideo(this)" class="timestamp">01:16</span></p>
<p>For all of the several notebooks it's just in Python and Numpy so you don't need any GPUs for those questions. <span onclick="goVideo(this)" class="timestamp">01:24</span></p>
<p>So again, conserve your credits, only use GPUs when you need them. <span onclick="goVideo(this)" class="timestamp">01:32</span></p>
<p>And the final reminder is that the midterm is coming up. <span onclick="goVideo(this)" class="timestamp">01:36</span></p>
<p>It's kind of hard to believe we're there already, but the midterm will be in class on Tuesday, five nine. <span onclick="goVideo(this)" class="timestamp">01:39</span></p>
<p>So the midterm will be more theoretical. <span onclick="goVideo(this)" class="timestamp">01:45</span></p>
<p>It'll be sort of pen and paper working through different kinds of, slightly more theoretical questions to check your understanding of the material that we've covered so far. <span onclick="goVideo(this)" class="timestamp">01:47</span></p>
<p>And I think we'll probably post at least a short sort of sample of the types of questions to expect. <span onclick="goVideo(this)" class="timestamp">01:57</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">02:02</span></p>
<p>[student's words obscured due to lack of microphone] Oh yeah, question is whether it's open-book, so we're going to say closed note, closed book. <span onclick="goVideo(this)" class="timestamp">02:03</span></p>
<p>So just, Yeah, yeah, so that's what we've done in the past is just closed note, closed book, relatively just like want to check that you understand the intuition behind most of the stuff we've presented. <span onclick="goVideo(this)" class="timestamp">02:10</span></p>
<p>So, a quick recap as a reminder of what we were talking about last time. <span onclick="goVideo(this)" class="timestamp">02:23</span></p>
<p>Last time we talked about fancier optimization algorithms for deep learning models including SGD Momentum, Nesterov, RMSProp and Adam. <span onclick="goVideo(this)" class="timestamp">02:27</span></p>
<p>And we saw that these relatively small tweaks on top of vanilla SGD, are relatively easy to implement but can make your networks converge a bit faster. <span onclick="goVideo(this)" class="timestamp">02:34</span></p>
<p>We also talked about regularization, especially dropout. <span onclick="goVideo(this)" class="timestamp">02:45</span></p>
<p>So remember dropout, you're kind of randomly setting parts of the network to zero during the forward pass, and then you kind of marginalize out over that noise in the back at test time. <span onclick="goVideo(this)" class="timestamp">02:48</span></p>
<p>And we saw that this was kind of a general pattern across many different types of regularization in deep learning, where you might add some kind of noise during training, but then marginalize out that noise at test time so it's not stochastic at test time. <span onclick="goVideo(this)" class="timestamp">02:56</span></p>
<p>We also talked about transfer learning where you can maybe download big networks that were pre-trained on some dataset and then fine tune them for your own problem. <span onclick="goVideo(this)" class="timestamp">03:08</span></p>
<p>And this is one way that you can attack a lot of problems in deep learning, even if you don't have a huge dataset of your own. <span onclick="goVideo(this)" class="timestamp">03:15</span></p>
<p>So today we're going to shift gears a little bit and talk about some of the nuts and bolts about writing software and how the hardware works. <span onclick="goVideo(this)" class="timestamp">03:22</span></p>
<p>And a little bit, diving into a lot of details about what the software looks like that you actually use to train these things in practice. <span onclick="goVideo(this)" class="timestamp">03:29</span></p>
<p>So we'll talk a little bit about CPUs and GPUs and then we'll talk about several of the major deep learning frameworks that are out there in use these days. <span onclick="goVideo(this)" class="timestamp">03:36</span></p>
<p>So first, we've sort of mentioned this off hand a bunch of different times, that computers have CPUs, computers have GPUs. <span onclick="goVideo(this)" class="timestamp">03:45</span></p>
<p>Deep learning uses GPUs, but we weren't really too explicit up to this point about what exactly these things are and why one might be better than another for different tasks. <span onclick="goVideo(this)" class="timestamp">03:52</span></p>
<p>So, who's built a computer before? <span onclick="goVideo(this)" class="timestamp">04:02</span></p>
<p>Just kind of show of hands. <span onclick="goVideo(this)" class="timestamp">04:04</span></p>
<p>So, maybe about a third of you, half of you, somewhere around that ballpark. <span onclick="goVideo(this)" class="timestamp">04:06</span></p>
<p>So this is a shot of my computer at home that I built. <span onclick="goVideo(this)" class="timestamp">04:10</span></p>
<p>And you can see that there's a lot of stuff going on inside the computer, maybe, hopefully you know what most of these parts are. <span onclick="goVideo(this)" class="timestamp">04:15</span></p>
<p>And the CPU is the Central Processing Unit. <span onclick="goVideo(this)" class="timestamp">04:22</span></p>
<p>That's this little chip hidden under this cooling fan right here near the top of the case. <span onclick="goVideo(this)" class="timestamp">04:25</span></p>
<p>And the CPU is actually relatively small piece. <span onclick="goVideo(this)" class="timestamp">04:31</span></p>
<p>It's a relatively small thing inside the case. <span onclick="goVideo(this)" class="timestamp">04:34</span></p>
<p>It's not taking up a lot of space. <span onclick="goVideo(this)" class="timestamp">04:36</span></p>
<p>And the GPUs are these two big monster things that are taking up a gigantic amount of space in the case. <span onclick="goVideo(this)" class="timestamp">04:39</span></p>
<p>They have their own cooling, they're taking a lot of power. <span onclick="goVideo(this)" class="timestamp">04:46</span></p>
<p>They're quite large. <span onclick="goVideo(this)" class="timestamp">04:48</span></p>
<p>So, just in terms of how much power they're using, in terms of how big they are, the GPUs are kind of physically imposing and taking up a lot of space in the case. <span onclick="goVideo(this)" class="timestamp">04:50</span></p>
<p>So the question is what are these things and why are they so important for deep learning? <span onclick="goVideo(this)" class="timestamp">04:59</span></p>
<p>Well, the GPU is called a graphics card, or Graphics Processing Unit. <span onclick="goVideo(this)" class="timestamp">05:04</span></p>
<p>And these were really developed, originally for rendering computer graphics, and especially around games and that sort of thing. <span onclick="goVideo(this)" class="timestamp">05:08</span></p>
<p>So another show of hands, who plays video games at home sometimes, from time to time on their computer? <span onclick="goVideo(this)" class="timestamp">05:16</span></p>
<p>Yeah, so again, maybe about half, good fraction. <span onclick="goVideo(this)" class="timestamp">05:23</span></p>
<p>So for those of you who've played video games before and who've built your own computers, you probably have your own opinions on this debate. <span onclick="goVideo(this)" class="timestamp">05:25</span></p>
<p>[laughs] So this is one of those big debates in computer science. <span onclick="goVideo(this)" class="timestamp">05:32</span></p>
<p>You know, there's like Intel versus AMD, NVIDIA versus AMD for graphics cards. <span onclick="goVideo(this)" class="timestamp">05:37</span></p>
<p>It's up there with Vim versus Emacs for text editor. <span onclick="goVideo(this)" class="timestamp">05:42</span></p>
<p>And pretty much any gamer has their own opinions on which of these two sides they prefer for their own cards. <span onclick="goVideo(this)" class="timestamp">05:45</span></p>
<p>And in deep learning we kind of have mostly picked one side of this fight, and that's NVIDIA. <span onclick="goVideo(this)" class="timestamp">05:51</span></p>
<p>So if you guys have AMD cards, you might be in a little bit more trouble if you want to use those for deep learning. <span onclick="goVideo(this)" class="timestamp">05:59</span></p>
<p>And really, NVIDIA's been pushing a lot for deep learning in the last several years. <span onclick="goVideo(this)" class="timestamp">06:05</span></p>
<p>It's been kind of a large focus of some of their strategy. <span onclick="goVideo(this)" class="timestamp">06:08</span></p>
<p>And they put in a lot effort into engineering sort of good solutions to make their hardware better suited for deep learning. <span onclick="goVideo(this)" class="timestamp">06:11</span></p>
<p>So most people in deep learning when we talk about GPUs, we're pretty much exclusively talking about NVIDIA GPUs. <span onclick="goVideo(this)" class="timestamp">06:19</span></p>
<p>Maybe in the future this'll change a little bit, and there might be new players coming up, but at least for now NVIDIA is pretty dominant. <span onclick="goVideo(this)" class="timestamp">06:27</span></p>
<p>So to give you an idea of like what is the difference between a CPU and a GPU, I've kind of made a little spread sheet here. <span onclick="goVideo(this)" class="timestamp">06:35</span></p>
<p>On the top we have two of the kind of top end Intel consumer CPUs, and on the bottom we have two of NVIDIA's sort of current top end consumer GPUs. <span onclick="goVideo(this)" class="timestamp">06:41</span></p>
<p>And there's a couple general trends to notice here. <span onclick="goVideo(this)" class="timestamp">06:52</span></p>
<p>Both GPUs and CPUs are kind of a general purpose computing machine where they can execute programs and do sort of arbitrary instructions, but they're qualitatively pretty different. <span onclick="goVideo(this)" class="timestamp">06:55</span></p>
<p>So CPUs tend to have just a few cores, for consumer desktop CPUs these days, they might have something like four or six or maybe up to 10 cores. <span onclick="goVideo(this)" class="timestamp">07:05</span></p>
<p>With hyperthreading technology that means they can run, the hardware can physically run, like maybe eight or up to 20 threads concurrently. <span onclick="goVideo(this)" class="timestamp">07:16</span></p>
<p>So the CPU can maybe do 20 things in parallel at once. <span onclick="goVideo(this)" class="timestamp">07:24</span></p>
<p>So that's just not a gigantic number, but those threads for a CPU are pretty powerful. <span onclick="goVideo(this)" class="timestamp">07:29</span></p>
<p>They can actually do a lot of things, they're very fast. <span onclick="goVideo(this)" class="timestamp">07:34</span></p>
<p>Every CPU instruction can actually do quite a lot of stuff. <span onclick="goVideo(this)" class="timestamp">07:37</span></p>
<p>And they can all work pretty independently. <span onclick="goVideo(this)" class="timestamp">07:39</span></p>
<p>For GPUs it's a little bit different. <span onclick="goVideo(this)" class="timestamp">07:43</span></p>
<p>So for GPUs we see that these sort of common top end consumer GPUs have thousands of cores. <span onclick="goVideo(this)" class="timestamp">07:45</span></p>
<p>So the NVIDIA Titan XP which is the current top of the line consumer GPU has 3840 cores. <span onclick="goVideo(this)" class="timestamp">07:51</span></p>
<p>So that's a crazy number. <span onclick="goVideo(this)" class="timestamp">08:00</span></p>
<p>That's like way more than the 10 cores that you'll get for a similarly priced CPU. <span onclick="goVideo(this)" class="timestamp">08:02</span></p>
<p>The downside of a GPU is that each of those cores, one, it runs at a much slower clock speed. <span onclick="goVideo(this)" class="timestamp">08:06</span></p>
<p>And two they really can't do quite as much. <span onclick="goVideo(this)" class="timestamp">08:12</span></p>
<p>You can't really compare CPU cores and GPU cores apples to apples. <span onclick="goVideo(this)" class="timestamp">08:14</span></p>
<p>The GPU cores can't really operate very independently. <span onclick="goVideo(this)" class="timestamp">08:19</span></p>
<p>They all kind of need to work together and sort of paralyze one task across many cores rather than each core totally doing its own thing. <span onclick="goVideo(this)" class="timestamp">08:22</span></p>
<p>So you can't really compare these numbers directly. <span onclick="goVideo(this)" class="timestamp">08:29</span></p>
<p>But it should give you the sense that due to the large number of cores GPUs can sort of, are really good for parallel things where you need to do a lot of things all at the same time, but those things are all pretty much the same flavor. <span onclick="goVideo(this)" class="timestamp">08:32</span></p>
<p>Another thing to point out between CPUs and GPUs is this idea of memory. <span onclick="goVideo(this)" class="timestamp">08:44</span></p>
<p>Right, so CPUs have some cache on the CPU, but that's relatively small and the majority of the memory for your CPU is pulling from your system memory, the RAM, which will maybe be like eight, 12, 16, 32 gigabytes of RAM on a typical consumer desktop these days. <span onclick="goVideo(this)" class="timestamp">08:49</span></p>
<p>Whereas GPUs actually have their own RAM built into the chip. <span onclick="goVideo(this)" class="timestamp">09:06</span></p>
<p>There's a pretty large bottleneck communicating between the RAM in your system and the GPU, so the GPUs typically have their own relatively large block of memory within the card itself. <span onclick="goVideo(this)" class="timestamp">09:12</span></p>
<p>And for the Titan XP, which again is maybe the current top of the line consumer card, this thing has 12 gigabytes of memory local to the GPU. <span onclick="goVideo(this)" class="timestamp">09:23</span></p>
<p>GPUs also have their own caching system where there are sort of multiple hierarchies of caching between the 12 gigabytes of GPU memory and the actual GPU cores. <span onclick="goVideo(this)" class="timestamp">09:33</span></p>
<p>And that's somewhat similar to the caching hierarchy that you might see in a CPU. <span onclick="goVideo(this)" class="timestamp">09:41</span></p>
<p>So, CPUs are kind of good for general purpose processing. <span onclick="goVideo(this)" class="timestamp">09:47</span></p>
<p>They can do a lot of different things. <span onclick="goVideo(this)" class="timestamp">09:50</span></p>
<p>And GPUs are maybe more specialized for these highly paralyzable algorithms. <span onclick="goVideo(this)" class="timestamp">09:52</span></p>
<p>So the prototypical algorithm of something that works really really well and is like perfectly suited to a GPU is matrix multiplication. <span onclick="goVideo(this)" class="timestamp">09:57</span></p>
<p>So remember in matrix multiplication on the left we've got like a matrix composed of a bunch of rows. <span onclick="goVideo(this)" class="timestamp">10:04</span></p>
<p>We multiply that on the right by another matrix composed of a bunch of columns and then this produces another, a final matrix where each element in the output matrix is a dot product between one of the rows and one of the columns of the two input matrices. <span onclick="goVideo(this)" class="timestamp">10:09</span></p>
<p>And these dot products are all independent. <span onclick="goVideo(this)" class="timestamp">10:22</span></p>
<p>Like you could imagine, for this output matrix you could split it up completely and have each of those different elements of the output matrix all being computed in parallel and they all sort of are running the same computation which is taking a dot product of these two vectors. <span onclick="goVideo(this)" class="timestamp">10:25</span></p>
<p>But exactly where they're reading that data from is from different places in the two input matrices. <span onclick="goVideo(this)" class="timestamp">10:38</span></p>
<p>So you could imagine that for a GPU you can just like blast this out and have all of this elements of the output matrix all computed in parallel and that could make this thing computer super super fast on GPU. <span onclick="goVideo(this)" class="timestamp">10:44</span></p>
<p>So that's kind of the prototypical type of problem that like where a GPU is really well suited, where a CPU might have to go in and step through sequentially and compute each of these elements one by one. <span onclick="goVideo(this)" class="timestamp">10:55</span></p>
<p>That picture is a little bit of a caricature because CPUs these days have multiple cores, they can do vectorized instructions as well, but still, for these like massively parallel problems GPUs tend to have much better throughput. <span onclick="goVideo(this)" class="timestamp">11:06</span></p>
<p>Especially when these matrices get really really big. <span onclick="goVideo(this)" class="timestamp">11:19</span></p>
<p>And by the way, convolution is kind of the same kind of story. <span onclick="goVideo(this)" class="timestamp">11:21</span></p>
<p>Where you know in convolution we have this input tensor, we have this weight tensor and then every point in the output tensor after a convolution is again some inner product between some part of the weights and some part of the input. <span onclick="goVideo(this)" class="timestamp">11:25</span></p>
<p>And you can imagine that a GPU could really paralyze this computation, split it all up across the many cores and compute it very quickly. <span onclick="goVideo(this)" class="timestamp">11:36</span></p>
<p>So that's kind of the general flavor of the types of problems where GPUs give you a huge speed advantage over CPUs. <span onclick="goVideo(this)" class="timestamp">11:43</span></p>
<p>So you can actually write programs that run directly on GPUs. <span onclick="goVideo(this)" class="timestamp">11:51</span></p>
<p>So NVIDIA has this CUDA abstraction that lets you write code that kind of looks like C, but executes directly on the GPUs. <span onclick="goVideo(this)" class="timestamp">11:55</span></p>
<p>But CUDA code is really really tricky. <span onclick="goVideo(this)" class="timestamp">12:03</span></p>
<p>It's actually really tough to write CUDA code that's performant and actually squeezes all the juice out of these GPUs. <span onclick="goVideo(this)" class="timestamp">12:05</span></p>
<p>You have to be very careful managing the memory hierarchy and making sure you don't have cache misses and branch mispredictions and all that sort of stuff. <span onclick="goVideo(this)" class="timestamp">12:12</span></p>
<p>So it's actually really really hard to write performant CUDA code on your own. <span onclick="goVideo(this)" class="timestamp">12:19</span></p>
<p>So as a result NVIDIA has released a lot of libraries that implement common computational primitives that are very very highly optimized for GPUs. <span onclick="goVideo(this)" class="timestamp">12:22</span></p>
<p>So for example NVIDIA has a cuBLAS library that implements different kinds of matrix multiplications and different matrix operations that are super optimized, run really well on GPU, get very close to sort of theoretical peak hardware utilization. <span onclick="goVideo(this)" class="timestamp">12:32</span></p>
<p>Similarly they have a cuDNN library which implements things like convolution, forward and backward passes, batch normalization, recurrent networks, all these kinds of computational primitives that we need in deep learning. <span onclick="goVideo(this)" class="timestamp">12:46</span></p>
<p>NVIDIA has gone in there and released their own binaries that compute these primitives very efficiently on NVIDIA hardware. <span onclick="goVideo(this)" class="timestamp">12:57</span></p>
<p>So in practice, you tend not to end up writing your own CUDA code for deep learning. <span onclick="goVideo(this)" class="timestamp">13:03</span></p>
<p>You typically are just mostly calling into existing code that other people have written. <span onclick="goVideo(this)" class="timestamp">13:09</span></p>
<p>Much of which is the stuff which has been heavily optimized by NVIDIA already. <span onclick="goVideo(this)" class="timestamp">13:14</span></p>
<p>There's another sort of language called OpenCL which is a bit more general. <span onclick="goVideo(this)" class="timestamp">13:19</span></p>
<p>Runs on more than just NVIDIA GPUs, can run on AMD hardware, can run on CPUs, but OpenCL, nobody's really spent a really large amount of effort and energy trying to get optimized deep learning primitives for OpenCL, so it tends to be a lot less performant the super optimized versions in CUDA. <span onclick="goVideo(this)" class="timestamp">13:23</span></p>
<p>So maybe in the future we might see a bit of a more open standard and we might see this across many different more types of platforms, but at least for now, NVIDIA's kind of the main game in town for deep learning. <span onclick="goVideo(this)" class="timestamp">13:43</span></p>
<p>So you can check, there's a lot of different resources for learning about how you can do GPU programming yourself. <span onclick="goVideo(this)" class="timestamp">13:55</span></p>
<p>It's kind of fun. <span onclick="goVideo(this)" class="timestamp">14:00</span></p>
<p>It's sort of a different paradigm of writing code because it's this massively parallel architecture, but that's a bit beyond the scope of this course. <span onclick="goVideo(this)" class="timestamp">14:01</span></p>
<p>And again, you don't really need to write your own CUDA code much in practice for deep learning. <span onclick="goVideo(this)" class="timestamp">14:08</span></p>
<p>And in fact, I've never written my own CUDA code for any research project, so, but it is kind of useful to know like how it works and what are the basic ideas even if you're not writing it yourself. <span onclick="goVideo(this)" class="timestamp">14:12</span></p>
<p>So if you want to look at kind of CPU GPU performance in practice, I did some benchmarks last summer comparing a decent Intel CPU against a bunch of different GPUs that were sort of near top of the line at that time. <span onclick="goVideo(this)" class="timestamp">14:23</span></p>
<p>And these were my own benchmarks that you can find more details on GitHub, but my findings were that for things like VGG 16 and 19, ResNets, various ResNets, then you typically see something like a 65 to 75 times speed up when running the exact same computation on a top of the line GPU, in this case a Pascal Titan X, versus a top of the line, well, not quite top of the line CPU, which in this case was an Intel E5 processor. <span onclick="goVideo(this)" class="timestamp">14:38</span></p>
<p>Although, I'd like to make one sort of caveat here is that you always need to be super careful whenever you're reading any kind of benchmarks about deep learning, because it's super easy to be unfair between different things. <span onclick="goVideo(this)" class="timestamp">15:08</span></p>
<p>And you kind of need to know a lot of the details about what exactly is being benchmarked in order to know whether or not the comparison is fair. <span onclick="goVideo(this)" class="timestamp">15:20</span></p>
<p>So in this case I'll come right out and tell you that probably this comparison is a little bit unfair to CPU because I didn't spend a lot of effort trying to squeeze the maximal performance out of CPUs. <span onclick="goVideo(this)" class="timestamp">15:26</span></p>
<p>I probably could have tuned the blast libraries better for the CPU performance. <span onclick="goVideo(this)" class="timestamp">15:38</span></p>
<p>And I probably could have gotten these numbers a bit better. <span onclick="goVideo(this)" class="timestamp">15:42</span></p>
<p>This was sort of out of the box performance between just installing Torch, running it on a CPU, just installing Torch running it on a GPU. <span onclick="goVideo(this)" class="timestamp">15:44</span></p>
<p>So this is kind of out of the box performance, but it's not really like peak, possible, theoretical throughput on the CPU. <span onclick="goVideo(this)" class="timestamp">15:51</span></p>
<p>But that being said, I think there are still pretty substantial speed ups to be had here. <span onclick="goVideo(this)" class="timestamp">15:57</span></p>
<p>Another kind of interesting outcome from this benchmarking was comparing these optimized cuDNN libraries from NVIDIA for convolution and whatnot versus sort of more naive CUDA that had been hand written out in the open source community. <span onclick="goVideo(this)" class="timestamp">16:02</span></p>
<p>And you can see that if you compare the same networks on the same hardware with the same deep learning framework and the only difference is swapping out these cuDNN versus sort of hand written, less optimized CUDA you can see something like nearly a three X speed up across the board when you switch from the relatively simple CUDA to these like super optimized cuDNN implementations. <span onclick="goVideo(this)" class="timestamp">16:17</span></p>
<p>So in general, whenever you're writing code on GPU, you should probably almost always like just make sure you're using cuDNN because you're leaving probably a three X performance boost on the table if you're not calling into cuDNN for your stuff. <span onclick="goVideo(this)" class="timestamp">16:37</span></p>
<p>So another problem that comes up in practice, when you're training these things is that you know, your model is maybe sitting on the GPU, the weights of the model are in that 12 gigabytes of local storage on the GPU, but your big dataset is sitting over on the right on a hard drive or an SSD or something like that. <span onclick="goVideo(this)" class="timestamp">16:51</span></p>
<p>So if you're not careful you can actually bottleneck your training by just trying to read the data off the disk. <span onclick="goVideo(this)" class="timestamp">17:07</span></p>
<p>'Cause the GPU is super fast, it can compute forward and backward quite fast, but if you're reading sequentially off a spinning disk, you can actually bottleneck your training quite, and that can be really bad and slow you down. <span onclick="goVideo(this)" class="timestamp">17:14</span></p>
<p>So some solutions here are that like you know if your dataset's really small, sometimes you might just read the whole dataset into RAM. <span onclick="goVideo(this)" class="timestamp">17:25</span></p>
<p>Or even if your dataset isn't so small, but you have a giant server with a ton of RAM, you might do that anyway. <span onclick="goVideo(this)" class="timestamp">17:31</span></p>
<p>You can also make sure you're using an SSD instead of a hard drive, that can help a lot with read throughput. <span onclick="goVideo(this)" class="timestamp">17:36</span></p>
<p>Another common strategy is to use multiple threads on the CPU that are pre-fetching data off RAM or off disk, buffering it in memory, in RAM so that then you can continue feeding that buffer data down to the GPU with good performance. <span onclick="goVideo(this)" class="timestamp">17:42</span></p>
<p>This is a little bit painful to set up, but again like, these GPU's are so fast that if you're not really careful with trying to feed them data as quickly as possible, just reading the data can sometimes bottleneck the whole training process. <span onclick="goVideo(this)" class="timestamp">17:57</span></p>
<p>So that's something to be aware of. <span onclick="goVideo(this)" class="timestamp">18:08</span></p>
<p>So that's kind of the brief introduction to like sort of GPU CPU hardware in practice when it comes to deep learning. <span onclick="goVideo(this)" class="timestamp">18:11</span></p>
<p>And then I wanted to switch gears a little bit and talk about the software side of things. <span onclick="goVideo(this)" class="timestamp">18:17</span></p>
<p>The various deep learning frameworks that people are using in practice. <span onclick="goVideo(this)" class="timestamp">18:21</span></p>
<p>But I guess before I move on, is there any sort of questions about CPU GPU? <span onclick="goVideo(this)" class="timestamp">18:25</span></p>
<p>Yeah, question? <span onclick="goVideo(this)" class="timestamp">18:28</span></p>
<p>[student's words obscured due to lack of microphone] Yeah, so the question is what can you sort of, what can you do mechanically when you're coding to avoid these problems? <span onclick="goVideo(this)" class="timestamp">18:30</span></p>
<p>Probably the biggest thing you can do in software is set up sort of pre-fetching on the CPU. <span onclick="goVideo(this)" class="timestamp">18:45</span></p>
<p>Like you couldn't like, sort of a naive thing would be you have this sequential process where you first read data off disk, wait for the data, wait for the minibatch to be read, then feed the minibatch to the GPU, then go forward and backward on the GPU, then read another minibatch and sort of do this all in sequence. <span onclick="goVideo(this)" class="timestamp">18:50</span></p>
<p>And if you actually have multiple, like instead you might have CPU threads running in the background that are fetching data off the disk such that while the, you can sort of interleave all of these things. <span onclick="goVideo(this)" class="timestamp">19:06</span></p>
<p>Like the GPU is computing, the CPU background threads are feeding data off disk and your main thread is kind of waiting for these things to, just doing a bit of synchronization between these things so they're all happening in parallel. <span onclick="goVideo(this)" class="timestamp">19:17</span></p>
<p>And thankfully if you're using some of these deep learning frameworks that we're about to talk about, then some of this work has already been done for you 'cause it's a little bit painful. <span onclick="goVideo(this)" class="timestamp">19:28</span></p>
<p>So the landscape of deep learning frameworks is super fast moving. <span onclick="goVideo(this)" class="timestamp">19:38</span></p>
<p>So last year when I gave this lecture I talked mostly about Caffe, Torch, Theano and TensorFlow. <span onclick="goVideo(this)" class="timestamp">19:41</span></p>
<p>And when I last gave this talk, again more than a year ago, TensorFlow was relatively new. <span onclick="goVideo(this)" class="timestamp">19:47</span></p>
<p>It had not seen super widespread adoption yet at that time. <span onclick="goVideo(this)" class="timestamp">19:53</span></p>
<p>But now I think in the last year TensorFlow has gotten much more popular. <span onclick="goVideo(this)" class="timestamp">19:57</span></p>
<p>It's probably the main framework of choice for many people. <span onclick="goVideo(this)" class="timestamp">20:01</span></p>
<p>So that's a big change. <span onclick="goVideo(this)" class="timestamp">20:04</span></p>
<p>We've also seen a ton of new frameworks sort of popping up like mushrooms in the last year. <span onclick="goVideo(this)" class="timestamp">20:07</span></p>
<p>So in particular Caffe2 and PyTorch are new frameworks from Facebook that I think are pretty interesting. <span onclick="goVideo(this)" class="timestamp">20:12</span></p>
<p>There's also a ton of other frameworks. <span onclick="goVideo(this)" class="timestamp">20:18</span></p>
<p>Paddle, Baidu has Paddle, Microsoft has CNTK, Amazon is mostly using MXNet and there's a ton of other frameworks as well, but I'm less familiar with, and really don't have time to get into. <span onclick="goVideo(this)" class="timestamp">20:20</span></p>
<p>But one interesting thing to point out from this picture is that kind of the first generation of deep learning frameworks that really saw wide adoption were built in academia. <span onclick="goVideo(this)" class="timestamp">20:33</span></p>
<p>So Caffe was from Berkeley, Torch was developed originally NYU and also in collaboration with Facebook. <span onclick="goVideo(this)" class="timestamp">20:43</span></p>
<p>And Theana was mostly build at the University of Montreal. <span onclick="goVideo(this)" class="timestamp">20:49</span></p>
<p>But these kind of next generation deep learning frameworks all originated in industry. <span onclick="goVideo(this)" class="timestamp">20:52</span></p>
<p>So Caffe2 is from Facebook, PyTorch is from Facebook. <span onclick="goVideo(this)" class="timestamp">20:56</span></p>
<p>TensorFlow is from Google. <span onclick="goVideo(this)" class="timestamp">20:58</span></p>
<p>So it's kind of an interesting shift that we've seen in the landscape over the last couple of years is that these ideas have really moved a lot from academia into industry. <span onclick="goVideo(this)" class="timestamp">21:00</span></p>
<p>And now industry is kind of giving us these big powerful nice frameworks to work with. <span onclick="goVideo(this)" class="timestamp">21:08</span></p>
<p>So today I wanted to mostly talk about PyTorch and TensorFlow 'cause I personally think that those are probably the ones you should be focusing on for a lot of research type problems these days. <span onclick="goVideo(this)" class="timestamp">21:14</span></p>
<p>I'll also talk a bit about Caffe and Caffe2. <span onclick="goVideo(this)" class="timestamp">21:24</span></p>
<p>But probably a little bit less emphasis on those. <span onclick="goVideo(this)" class="timestamp">21:28</span></p>
<p>And before we move any farther, I thought I should make my own biases a little bit more explicit. <span onclick="goVideo(this)" class="timestamp">21:32</span></p>
<p>So I have mostly, I've worked with Torch mostly for the last several years. <span onclick="goVideo(this)" class="timestamp">21:36</span></p>
<p>And I've used it quite a lot, I like it a lot. <span onclick="goVideo(this)" class="timestamp">21:40</span></p>
<p>And then in the last year I've mostly switched to PyTorch as my main research framework. <span onclick="goVideo(this)" class="timestamp">21:43</span></p>
<p>So I have a little bit less experience with some of these others, especially TensorFlow, but I'll still try to do my best to give you a fair picture and a decent overview of these things. <span onclick="goVideo(this)" class="timestamp">21:48</span></p>
<p>So, remember that in the last several lectures we've hammered this idea of computational graphs in sort of over and over. <span onclick="goVideo(this)" class="timestamp">21:58</span></p>
<p>That whenever you're doing deep learning, you want to think about building some computational graph that computes whatever function that you want to compute. <span onclick="goVideo(this)" class="timestamp">22:06</span></p>
<p>So in the case of a linear classifier you'll combine your data X and your weights W with a matrix multiply. <span onclick="goVideo(this)" class="timestamp">22:13</span></p>
<p>You'll do some kind of hinge loss to maybe have, compute your loss. <span onclick="goVideo(this)" class="timestamp">22:18</span></p>
<p>You'll have some regularization term and you imagine stitching together all these different operations into some graph structure. <span onclick="goVideo(this)" class="timestamp">22:22</span></p>
<p>Remember that these graph structures can get pretty complex in the case of a big neural net, now there's many different layers, many different activations. <span onclick="goVideo(this)" class="timestamp">22:28</span></p>
<p>Many different weights spread all around in a pretty complex graph. <span onclick="goVideo(this)" class="timestamp">22:36</span></p>
<p>And as you move to things like neural turing machines then you can get these really crazy computational graphs that you can't even really draw because they're so big and messy. <span onclick="goVideo(this)" class="timestamp">22:39</span></p>
<p>So the point of deep learning frameworks is really, there's really kind of three main reasons why you might want to use one of these deep learning frameworks rather than just writing your own code. <span onclick="goVideo(this)" class="timestamp">22:48</span></p>
<p>So the first would be that these frameworks enable you to easily build and work with these big hairy computational graphs without kind of worrying about a lot of those bookkeeping details yourself. <span onclick="goVideo(this)" class="timestamp">22:58</span></p>
<p>Another major idea is that, whenever we're working in deep learning we always need to compute gradients. <span onclick="goVideo(this)" class="timestamp">23:08</span></p>
<p>We're always computing some loss, we're always computer gradient of our weight with respect to the loss. <span onclick="goVideo(this)" class="timestamp">23:14</span></p>
<p>And we'd like to make this automatically computing gradient, you don't want to have to write that code yourself. <span onclick="goVideo(this)" class="timestamp">23:18</span></p>
<p>You want that framework to handle all these back propagation details for you so you can just think about writing down the forward pass of your network and have the backward pass sort of come out for free without any additional work. <span onclick="goVideo(this)" class="timestamp">23:26</span></p>
<p>And finally you want all this stuff to run efficiently on GPUs so you don't have to worry too much about these low level hardware details about cuBLAS and cuDNN and CUDA and moving data between the CPU and GPU memory. <span onclick="goVideo(this)" class="timestamp">23:36</span></p>
<p>You kind of want all those messy details to be taken care of for you. <span onclick="goVideo(this)" class="timestamp">23:48</span></p>
<p>So those are kind of some of the major reasons why you might choose to use frameworks rather than writing your own stuff from scratch. <span onclick="goVideo(this)" class="timestamp">23:52</span></p>
<p>So as kind of a concrete example of a computational graph we can maybe write down this super simple thing. <span onclick="goVideo(this)" class="timestamp">23:59</span></p>
<p>Where we have three inputs, X, Y, and Z. <span onclick="goVideo(this)" class="timestamp">24:05</span></p>
<p>We're going to combine X and Y to produce A. <span onclick="goVideo(this)" class="timestamp">24:08</span></p>
<p>Then we're going to combine A and Z to produce B and then finally we're going to do some maybe summing out operation on B to give some scaler final result C. <span onclick="goVideo(this)" class="timestamp">24:09</span></p>
<p>So you've probably written enough Numpy code at this point to realize that it's super easy to write down, to implement this computational graph, or rather to implement this bit of computation in Numpy, right? <span onclick="goVideo(this)" class="timestamp">24:18</span></p>
<p>You can just kind of write down in Numpy that you want to generate some random data, you want to multiply two things, you want to add two things, you want to sum out a couple things. <span onclick="goVideo(this)" class="timestamp">24:31</span></p>
<p>And it's really easy to do this in Numpy. <span onclick="goVideo(this)" class="timestamp">24:38</span></p>
<p>But then the question is like suppose that we want to compute the gradient of C with respect to X, Y, and Z. <span onclick="goVideo(this)" class="timestamp">24:41</span></p>
<p>So, if you're working in Numpy, you kind of need to write out this backward pass yourself. <span onclick="goVideo(this)" class="timestamp">24:48</span></p>
<p>And you've gotten a lot of practice with this on the homeworks, but it can be kind of a pain and a little bit annoying and messy once you get to really big complicated things. <span onclick="goVideo(this)" class="timestamp">24:52</span></p>
<p>The other problem with Numpy is that it doesn't run on the GPU. <span onclick="goVideo(this)" class="timestamp">25:02</span></p>
<p>So Numpy is definitely CPU only. <span onclick="goVideo(this)" class="timestamp">25:05</span></p>
<p>And you're never going to be able to experience or take advantage of these GPU accelerated speedups if you're stuck working in Numpy. <span onclick="goVideo(this)" class="timestamp">25:08</span></p>
<p>And it's, again, it's a pain to have to compute your own gradients in all these situations. <span onclick="goVideo(this)" class="timestamp">25:14</span></p>
<p>So, kind of the goal of most deep learning frameworks these days is to let you write code in the forward pass that looks very similar to Numpy, but lets you run it on the GPU and lets you automatically compute gradients. <span onclick="goVideo(this)" class="timestamp">25:19</span></p>
<p>And that's kind of the big picture goal of most of these frameworks. <span onclick="goVideo(this)" class="timestamp">25:33</span></p>
<p>So if you imagine looking at, if we look at an example in TensorFlow of the exact same computational graph, we now see that in this forward pass, you write this code that ends up looking very very similar to the Numpy forward pass where you're kind of doing these multiplication and these addition operations. <span onclick="goVideo(this)" class="timestamp">25:36</span></p>
<p>But now TensorFlow has this magic line that just computes all the gradients for you. <span onclick="goVideo(this)" class="timestamp">25:52</span></p>
<p>So now you don't have go in and write your own backward pass and that's much more convenient. <span onclick="goVideo(this)" class="timestamp">25:57</span></p>
<p>The other nice thing about TensorFlow is you can really just, like with one line you can switch all this computation between CPU and GPU. <span onclick="goVideo(this)" class="timestamp">26:02</span></p>
<p>So here, if you just add this with statement before you're doing this forward pass, you just can explicitly tell the framework, hey I want to run this code on the CPU. <span onclick="goVideo(this)" class="timestamp">26:08</span></p>
<p>But now if we just change that with statement a little bit with just with a one character change in this case, changing that C to a G, now the code runs on GPU. <span onclick="goVideo(this)" class="timestamp">26:16</span></p>
<p>And now in this little code snippet, we've solved these two problems. <span onclick="goVideo(this)" class="timestamp">26:24</span></p>
<p>We're running our code on the GPU and we're having the framework compute all the gradients for us, so that's really nice. <span onclick="goVideo(this)" class="timestamp">26:29</span></p>
<p>And PyTorch kind looks almost exactly the same. <span onclick="goVideo(this)" class="timestamp">26:35</span></p>
<p>So again, in PyTorch you kind of write down, you define some variables, you have some forward pass and the forward pass again looks very similar to like, in this case identical to the Numpy code. <span onclick="goVideo(this)" class="timestamp">26:38</span></p>
<p>And then again, you can just use PyTorch to compute gradients, all your gradients with just one line. <span onclick="goVideo(this)" class="timestamp">26:49</span></p>
<p>And now in PyTorch again, it's really easy to switch to GPU, you just need to cast all your stuff to the CUDA data type before you rung your computation and now everything runs transparently on the GPU for you. <span onclick="goVideo(this)" class="timestamp">26:56</span></p>
<p>So if you kind of just look at these three examples, these three snippets of code side by side, the Numpy, the TensorFlow and the PyTorch you see that the TensorFlow and the PyTorch code in the forward pass looks almost exactly like Numpy which is great 'cause Numpy has a beautiful API, it's really easy to work with. <span onclick="goVideo(this)" class="timestamp">27:06</span></p>
<p>But we can compute gradients automatically and we can run the GPU automatically. <span onclick="goVideo(this)" class="timestamp">27:24</span></p>
<p>So after that kind of introduction, I wanted to dive in and talk in a little bit more detail about kind of what's going on inside this TensorFlow example. <span onclick="goVideo(this)" class="timestamp">27:30</span></p>
<p>So as a running example throughout the rest of the lecture, I'm going to use the training a two-layer fully connected ReLU network on random data as kind of a running example throughout the rest of the examples here. <span onclick="goVideo(this)" class="timestamp">27:37</span></p>
<p>And we're going to train this thing with an L2 Euclidean loss on random data. <span onclick="goVideo(this)" class="timestamp">27:50</span></p>
<p>So this is kind of a silly network, it's not really doing anything useful, but it does give you the, it's relatively small, self contained, the code fits on the slide without being too small, and it lets you demonstrate kind of a lot of the useful ideas inside these frameworks. <span onclick="goVideo(this)" class="timestamp">27:55</span></p>
<p>So here on the right, oh, and then another note, I'm kind of assuming that Numpy and TensorFlow have already been imported in all these code snippets. <span onclick="goVideo(this)" class="timestamp">28:08</span></p>
<p>So in TensorFlow you would typically divide your computation into two major stages. <span onclick="goVideo(this)" class="timestamp">28:15</span></p>
<p>First, we're going to write some code that defines our computational graph, and that's this red code up in the top half. <span onclick="goVideo(this)" class="timestamp">28:21</span></p>
<p>And then after you define your graph, you're going to run the graph over and over again and actually feed data into the graph to perform whatever computation you want it to perform. <span onclick="goVideo(this)" class="timestamp">28:28</span></p>
<p>So this is the really, this is kind of the big common pattern in TensorFlow. <span onclick="goVideo(this)" class="timestamp">28:36</span></p>
<p>You'll first have a bunch of code that builds the graph and then you'll go and run the graph and reuse it many many times. <span onclick="goVideo(this)" class="timestamp">28:40</span></p>
<p>So if you kind of dive into the code of building the graph in this case. <span onclick="goVideo(this)" class="timestamp">28:48</span></p>
<p>Up at the top you see that we're defining this X, Y, w1 and w2, and we're creating these tf.placeholder objects. <span onclick="goVideo(this)" class="timestamp">28:52</span></p>
<p>So these are going to be input nodes to the graph. <span onclick="goVideo(this)" class="timestamp">29:01</span></p>
<p>These are going to be sort of entry points to the graph where when we run the graph, we're going to feed in data and put them in through these input slots in our computational graph. <span onclick="goVideo(this)" class="timestamp">29:05</span></p>
<p>So this is not actually like allocating any memory right now. <span onclick="goVideo(this)" class="timestamp">29:15</span></p>
<p>We're just sort of setting up these input slots to the graph. <span onclick="goVideo(this)" class="timestamp">29:19</span></p>
<p>Then we're going to use those input slots which are now kind of like these symbolic variables and we're going to perform different TensorFlow operations on these symbolic variables in order to set up what computation we want to run on those variables. <span onclick="goVideo(this)" class="timestamp">29:23</span></p>
<p>So in this case we're doing a matrix multiplication between X and w1, we're doing some tf.maximum to do a ReLU nonlinearity and then we're doing another matrix multiplication to compute our output predictions. <span onclick="goVideo(this)" class="timestamp">29:37</span></p>
<p>And then we're again using a sort of basic Tensor operations to compute our Euclidean distance, our L2 loss between our prediction and the target Y. <span onclick="goVideo(this)" class="timestamp">29:49</span></p>
<p>Another thing to point out here is that these lines of code are not actually computing anything. <span onclick="goVideo(this)" class="timestamp">29:58</span></p>
<p>There's no data in the system right now. <span onclick="goVideo(this)" class="timestamp">30:03</span></p>
<p>We're just building up this computational graph data structure telling TensorFlow which operations we want to eventually run once we put in real data. <span onclick="goVideo(this)" class="timestamp">30:05</span></p>
<p>So this is just building the graph, this is not actually doing anything. <span onclick="goVideo(this)" class="timestamp">30:15</span></p>
<p>Then we have this magical line where after we've computed our loss with these symbolic operations, then we can just ask TensorFlow to compute the gradient of the loss with respect to w1 and w2 in this one magical, beautiful line. <span onclick="goVideo(this)" class="timestamp">30:18</span></p>
<p>And this avoids you writing all your own backprop code that you had to do in the assignments. <span onclick="goVideo(this)" class="timestamp">30:33</span></p>
<p>But again there's no actual computation happening here. <span onclick="goVideo(this)" class="timestamp">30:37</span></p>
<p>This is just sort of adding extra operations to the computational graph where now the computational graph has these additional operations which will end up computing these gradients for you. <span onclick="goVideo(this)" class="timestamp">30:40</span></p>
<p>So now at this point we've computed our computational graph, we have this big graph in this graph data structure in memory that knows what operations we want to perform to compute the loss in gradients. <span onclick="goVideo(this)" class="timestamp">30:51</span></p>
<p>And now we enter a TensorFlow session to actually run this graph and feed it with data. <span onclick="goVideo(this)" class="timestamp">31:01</span></p>
<p>So then, once we've entered the session, then we actually need to construct some concrete values that will be fed to the graph. <span onclick="goVideo(this)" class="timestamp">31:06</span></p>
<p>So TensorFlow just expects to receive data from Numpy arrays in most cases. <span onclick="goVideo(this)" class="timestamp">31:13</span></p>
<p>So here we're just creating concrete actual values for X, Y, w1 and w2 using Numpy and then storing these in some dictionary. <span onclick="goVideo(this)" class="timestamp">31:19</span></p>
<p>And now here is where we're actually running the graph. <span onclick="goVideo(this)" class="timestamp">31:30</span></p>
<p>So you can see that we're calling a session.run to actually execute some part of the graph. <span onclick="goVideo(this)" class="timestamp">31:32</span></p>
<p>The first argument loss, tells us which part of the graph do we actually want as output. <span onclick="goVideo(this)" class="timestamp">31:38</span></p>
<p>And that, so we actually want the graph, in this case we need to tell it that we actually want to compute loss and grad1 and grad w2 and we need to pass in with this feed dict parameter the actual concrete values that will be fed to the graph. <span onclick="goVideo(this)" class="timestamp">31:43</span></p>
<p>And then after, in this one line, it's going and running the graph and then computing those values for loss grad1 to grad w2 and then returning the actual concrete values for those in Numpy arrays again. <span onclick="goVideo(this)" class="timestamp">31:57</span></p>
<p>So now after you unpack this output in the second line, you get Numpy arrays, or you get Numpy arrays with the loss and the gradients. <span onclick="goVideo(this)" class="timestamp">32:12</span></p>
<p>So then you can go and do whatever you want with these values. <span onclick="goVideo(this)" class="timestamp">32:19</span></p>
<p>So then, this has only run sort of one forward and backward pass through our graph, and it only takes a couple extra lines if we actually want to train the network. <span onclick="goVideo(this)" class="timestamp">32:23</span></p>
<p>So here we're, now we're running the graph many times in a loop so we're doing a four loop and in each iteration of the loop, we're calling session.run asking it to compute the loss and the gradients. <span onclick="goVideo(this)" class="timestamp">32:33</span></p>
<p>And now we're doing a manual gradient discent step using those computed gradients to now update our current values of the weights. <span onclick="goVideo(this)" class="timestamp">32:45</span></p>
<p>So if you actually run this code and plot the losses, then you'll see that the loss goes down and the network is training and this is working pretty well. <span onclick="goVideo(this)" class="timestamp">32:52</span></p>
<p>So this is kind of like a super bare bones example of training a fully connected network in TensorFlow. <span onclick="goVideo(this)" class="timestamp">33:00</span></p>
<p>But there's a problem here. <span onclick="goVideo(this)" class="timestamp">33:06</span></p>
<p>So here, remember that on the forward pass, every time we execute this graph, we're actually feeding in the weights. <span onclick="goVideo(this)" class="timestamp">33:08</span></p>
<p>We have the weights as Numpy arrays and we're explicitly feeding them into the graph. <span onclick="goVideo(this)" class="timestamp">33:15</span></p>
<p>And now when the graph finishes executing it's going to give us these gradients. <span onclick="goVideo(this)" class="timestamp">33:18</span></p>
<p>And remember the gradients are the same size as the weights. <span onclick="goVideo(this)" class="timestamp">33:23</span></p>
<p>So this means that every time we're running the graph here, we're copying the weights from Numpy arrays into TensorFlow then getting the gradients and then copying the gradients from TensorFlow back out to Numpy arrays. <span onclick="goVideo(this)" class="timestamp">33:26</span></p>
<p>So if you're just running on CPU, this is maybe not a huge deal, but remember we talked about CPU GPU bottleneck and how it's very expensive actually to copy data between CPU memory and GPU memory. <span onclick="goVideo(this)" class="timestamp">33:36</span></p>
<p>So if your network is very large and your weights and gradients were very big, then doing something like this would be super expensive and super slow because we'd be copying all kinds of data back and forth between the CPU and the GPU at every time step. <span onclick="goVideo(this)" class="timestamp">33:47</span></p>
<p>So that's bad, we don't want to do that. <span onclick="goVideo(this)" class="timestamp">33:59</span></p>
<p>We need to fix that. <span onclick="goVideo(this)" class="timestamp">34:00</span></p>
<p>So, obviously TensorFlow has some solution to this. <span onclick="goVideo(this)" class="timestamp">34:01</span></p>
<p>And the idea is that now we want our weights, w1 and w2, rather than being placeholders where we're going to, where we expect to feed them in to the network on every forward pass, instead we define them as variables. <span onclick="goVideo(this)" class="timestamp">34:06</span></p>
<p>So a variable is something is a value that lives inside the computational graph and it's going to persist inside the computational graph across different times when you run the same graph. <span onclick="goVideo(this)" class="timestamp">34:17</span></p>
<p>So now instead of declaring these w1 and w2 as placeholders, instead we just construct them as variables. <span onclick="goVideo(this)" class="timestamp">34:27</span></p>
<p>But now since they live inside the graph, we also need to tell TensorFlow how they should be initialized, right? <span onclick="goVideo(this)" class="timestamp">34:33</span></p>
<p>Because in the previous case we were feeding in their values from outside the graph, so we initialized them in Numpy, but now because these things live inside the graph, TensorFlow is responsible for initializing them. <span onclick="goVideo(this)" class="timestamp">34:39</span></p>
<p>So we need to pass in a tf.randomnormal operation, which again is not actually initializing them when we run this line, this is just telling TensorFlow how we want them to be initialized. <span onclick="goVideo(this)" class="timestamp">34:50</span></p>
<p>So it's a little bit of confusing misdirection going on here. <span onclick="goVideo(this)" class="timestamp">35:00</span></p>
<p>And now, remember in the previous example we were actually updating the weights outside of the computational graph. <span onclick="goVideo(this)" class="timestamp">35:04</span></p>
<p>We, in the previous example, we were computing the gradients and then using them to update the weights as Numpy arrays and then feeding in the updated weights at the next time step. <span onclick="goVideo(this)" class="timestamp">35:11</span></p>
<p>But now because we want these weights to live inside the graph, this operation of updating the weights needs to also be an operation inside the computational graph. <span onclick="goVideo(this)" class="timestamp">35:20</span></p>
<p>So now we used this assign function which mutates these variables inside the computational graph and now the mutated value will persist across multiple runs of the same graph. <span onclick="goVideo(this)" class="timestamp">35:29</span></p>
<p>So now when we run this graph and when we train the network, now we need to run the graph once with a little bit of special incantation to tell TensorFlow to set up these variables that are going to live inside the graph. <span onclick="goVideo(this)" class="timestamp">35:41</span></p>
<p>And then once we've done that initialization, now we can run the graph over and over again. <span onclick="goVideo(this)" class="timestamp">35:53</span></p>
<p>And here, we're now only feeding in the data and labels X and Y and the weights are living inside the graph. <span onclick="goVideo(this)" class="timestamp">35:58</span></p>
<p>And here we've asked the network to, we've asked TensorFlow to compute the loss for us. <span onclick="goVideo(this)" class="timestamp">36:05</span></p>
<p>And then you might think that this would train the network, but there's actually a bug here. <span onclick="goVideo(this)" class="timestamp">36:09</span></p>
<p>So, if you actually run this code, and you plot the loss, it doesn't train. <span onclick="goVideo(this)" class="timestamp">36:15</span></p>
<p>So that's bad, it's confusing, like what's going on? <span onclick="goVideo(this)" class="timestamp">36:19</span></p>
<p>We wrote this assign code, we ran the thing, like we computed the loss and the gradients and our loss is flat, what's going on? <span onclick="goVideo(this)" class="timestamp">36:23</span></p>
<p>Any ideas? <span onclick="goVideo(this)" class="timestamp">36:29</span></p>
<p>[student's words obscured due to lack of microphone] Yeah so one hypothesis is that maybe we're accidentally re-initializing the w's every time we call the graph. <span onclick="goVideo(this)" class="timestamp">36:31</span></p>
<p>That's a good hypothesis, that's actually not the problem in this case. <span onclick="goVideo(this)" class="timestamp">36:41</span></p>
<p>[student's words obscured due to lack of microphone] Yeah, so the answer is that we actually need to explicitly tell TensorFlow that we want to run these new w1 and new w2 operations. <span onclick="goVideo(this)" class="timestamp">36:44</span></p>
<p>So we've built up this big computational graph data structure in memory and now when we call run, we only told TensorFlow that we wanted to compute loss. <span onclick="goVideo(this)" class="timestamp">36:56</span></p>
<p>And if you look at the dependencies among these different operations inside the graph, you see that in order to compute loss we don't actually need to perform this update operation. <span onclick="goVideo(this)" class="timestamp">37:04</span></p>
<p>So TensorFlow is smart and it only computes the parts of the graph that are necessary for computing the output that you asked it to compute. <span onclick="goVideo(this)" class="timestamp">37:13</span></p>
<p>So that's kind of a nice thing because it means it's only doing as much work as it needs to, but in situations like this it can be a little bit confusing and lead to behavior that you didn't expect. <span onclick="goVideo(this)" class="timestamp">37:21</span></p>
<p>So the solution in this case is that we actually need to explicitly tell TensorFlow to perform those update operations. <span onclick="goVideo(this)" class="timestamp">37:32</span></p>
<p>So one thing we could do, which is what was suggested is we could add new w1 and new w2 as outputs and just tell TensorFlow that we want to produce these values as outputs. <span onclick="goVideo(this)" class="timestamp">37:39</span></p>
<p>But that's a problem too because the values, those new w1, new w2 values are again these big tensors. <span onclick="goVideo(this)" class="timestamp">37:49</span></p>
<p>So now if we tell TensorFlow we want those as output, we're going to again get this copying behavior between CPU and GPU at ever iteration. <span onclick="goVideo(this)" class="timestamp">37:58</span></p>
<p>So that's bad, we don't want that. <span onclick="goVideo(this)" class="timestamp">38:05</span></p>
<p>So there's a little trick you can do instead. <span onclick="goVideo(this)" class="timestamp">38:07</span></p>
<p>Which is that we add kind of a dummy node to the graph. <span onclick="goVideo(this)" class="timestamp">38:09</span></p>
<p>With these fake data dependencies and we just say that this dummy node updates, has these data dependencies of new w1 and new w2. <span onclick="goVideo(this)" class="timestamp">38:11</span></p>
<p>And now when we actually run the graph, we tell it to compute both the loss and this dummy node. <span onclick="goVideo(this)" class="timestamp">38:20</span></p>
<p>And this dummy node doesn't actually return any value it just returns none, but because of this dependency that we've put into the node it ensures that when we run the updates value, we actually also run these update operations. <span onclick="goVideo(this)" class="timestamp">38:25</span></p>
<p>So, question? <span onclick="goVideo(this)" class="timestamp">38:38</span></p>
<p>[student's words obscured due to lack of microphone] Is there a reason why we didn't put X and Y into the graph? <span onclick="goVideo(this)" class="timestamp">38:40</span></p>
<p>And that it stayed as Numpy. <span onclick="goVideo(this)" class="timestamp">38:48</span></p>
<p>So in this example we're reusing X and Y on every, we're reusing the same X and Y on every iteration. <span onclick="goVideo(this)" class="timestamp">38:51</span></p>
<p>So you're right, we could have just also stuck those in the graph, but in a more realistic scenario, X and Y will be minibatches of data so those will actually change at every iteration and we will want to feed different values for those at every iteration. <span onclick="goVideo(this)" class="timestamp">38:57</span></p>
<p>So in this case, they could have stayed in the graph, but in most cases they will change, so we don't want them to live in the graph. <span onclick="goVideo(this)" class="timestamp">39:10</span></p>
<p>Oh, another question? <span onclick="goVideo(this)" class="timestamp">39:19</span></p>
<p>[student's words obscured due to lack of microphone] Yeah, so we've told it, we had put into TensorFlow that the outputs we want are loss and updates. <span onclick="goVideo(this)" class="timestamp">39:21</span></p>
<p>Updates is not actually a real value. <span onclick="goVideo(this)" class="timestamp">39:44</span></p>
<p>So when updates evaluates it just returns none. <span onclick="goVideo(this)" class="timestamp">39:48</span></p>
<p>But because of this dependency we've told it that updates depends on these assign operations. <span onclick="goVideo(this)" class="timestamp">39:51</span></p>
<p>But these assign operations live inside the computational graph and all live inside GPU memory. <span onclick="goVideo(this)" class="timestamp">39:57</span></p>
<p>So then we're doing these update operations entirely on the GPU and we're no longer copying the updated values back out of the graph. <span onclick="goVideo(this)" class="timestamp">40:02</span></p>
<p>[student's words obscured due to lack of microphone] So the question is does tf.group return none? <span onclick="goVideo(this)" class="timestamp">40:11</span></p>
<p>So this gets into the trickiness of TensorFlow. <span onclick="goVideo(this)" class="timestamp">40:18</span></p>
<p>So tf.group returns some crazy TensorFlow value. <span onclick="goVideo(this)" class="timestamp">40:21</span></p>
<p>It sort of returns some like internal TensorFlow node operation that we need to continue building the graph. <span onclick="goVideo(this)" class="timestamp">40:25</span></p>
<p>But when you execute the graph, and when you tell, inside the session.run, when we told it we want it to compute the concrete value from updates, then that returns none. <span onclick="goVideo(this)" class="timestamp">40:32</span></p>
<p>So whenever you're working with TensorFlow you have this funny indirection between building the graph and the actual output values during building the graph is some funny weird object, and then you actually get a concrete value when you run the graph. <span onclick="goVideo(this)" class="timestamp">40:43</span></p>
<p>So here after you run updates, then the output is none. <span onclick="goVideo(this)" class="timestamp">40:55</span></p>
<p>Does that clear it up a little bit? <span onclick="goVideo(this)" class="timestamp">40:58</span></p>
<p>[student's words obscured due to lack of microphone] So the question is why is loss a value and why is updates none? <span onclick="goVideo(this)" class="timestamp">40:59</span></p>
<p>That's just the way that updates works. <span onclick="goVideo(this)" class="timestamp">41:22</span></p>
<p>So loss is a value when we compute, when we tell TensorFlow we want to run a tensor, then we get the concrete value. <span onclick="goVideo(this)" class="timestamp">41:24</span></p>
<p>Updates is this kind of special other data type that does not return a value, it instead returns none. <span onclick="goVideo(this)" class="timestamp">41:30</span></p>
<p>So it's kind of some TensorFlow magic that's going on there. <span onclick="goVideo(this)" class="timestamp">41:35</span></p>
<p>Maybe we can talk offline if you're still confused. <span onclick="goVideo(this)" class="timestamp">41:38</span></p>
<p>[student's words obscured due to lack of microphone] Yeah, yeah, that behavior is coming from the group method. <span onclick="goVideo(this)" class="timestamp">41:40</span></p>
<p>So now, we kind of have this weird pattern where we wanted to do these different assign operations, we have to use this funny tf.group thing. <span onclick="goVideo(this)" class="timestamp">41:46</span></p>
<p>That's kind of a pain, so thankfully TensorFlow gives you some convenience operations that kind of do that kind of stuff for you. <span onclick="goVideo(this)" class="timestamp">41:52</span></p>
<p>And that's called an optimizer. <span onclick="goVideo(this)" class="timestamp">42:00</span></p>
<p>So here we're using a tf.train.GradientDescentOptimizer and we're telling it what learning rate we want to use. <span onclick="goVideo(this)" class="timestamp">42:01</span></p>
<p>And you can imagine that there's, there's RMSprop, there's all kinds of different optimization algorithms here. <span onclick="goVideo(this)" class="timestamp">42:08</span></p>
<p>And now we call optimizer.minimize of loss and now this is a pretty magical, this is a pretty magical thing, because now this call is aware that these variables w1 and w2 are marked as trainable by default, so then internally, inside this optimizer.minimize it's going in and adding nodes to the graph which will compute gradient of loss with respect to w1 and w2 and then it's also performing that update operation for you and it's doing the grouping operation for you and it's doing the assigns. <span onclick="goVideo(this)" class="timestamp">42:12</span></p>
<p>It's like doing a lot of magical stuff inside there. <span onclick="goVideo(this)" class="timestamp">42:42</span></p>
<p>But then it ends up giving you this magical updates value which, if you dig through the code they're actually using tf.group so it looks very similar internally to what we saw before. <span onclick="goVideo(this)" class="timestamp">42:44</span></p>
<p>And now when we run the graph inside our loop we do the same pattern of telling it to compute loss and updates. <span onclick="goVideo(this)" class="timestamp">42:53</span></p>
<p>And every time we tell the graph to compute updates, then it'll actually go and update the graph. <span onclick="goVideo(this)" class="timestamp">43:00</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">43:07</span></p>
<p>[student's words obscured due to lack of microphone] Yeah, so what is the tf.GlobalVariablesInitializer? <span onclick="goVideo(this)" class="timestamp">43:08</span></p>
<p>So that's initializing w1 and w2 because these are variables which live inside the graph. <span onclick="goVideo(this)" class="timestamp">43:14</span></p>
<p>So we need to, when we saw this, when we create the tf.variable we have this tf.randomnormal which is this initialization so the tf.GlobalVariablesInitializer is causing the tf.randomnormal to actually run and generate concrete values to initialize those variables. <span onclick="goVideo(this)" class="timestamp">43:20</span></p>
<p>[student's words obscured due to lack of microphone] Sorry, what was the question? <span onclick="goVideo(this)" class="timestamp">43:37</span></p>
<p>[student's words obscured due to lack of microphone] So it knows that a placeholder is going to be fed outside of the graph and a variable is something that lives inside the graph. <span onclick="goVideo(this)" class="timestamp">43:42</span></p>
<p>So I don't know all the details about how it decides, what exactly it decides to run with that call. <span onclick="goVideo(this)" class="timestamp">43:51</span></p>
<p>I think you'd need to dig through the code to figure that out, or maybe it's documented somewhere. <span onclick="goVideo(this)" class="timestamp">43:56</span></p>
<p>So but now we've kind of got this, again we've got this full example of training a network in TensorFlow and we're kind of adding bells and whistles to make it a little bit more convenient. <span onclick="goVideo(this)" class="timestamp">44:00</span></p>
<p>So we can also here, in the previous example we were computing the loss explicitly using our own tensor operations, TensorFlow you can always do that, you can use basic tensor operations to compute just about anything you want. <span onclick="goVideo(this)" class="timestamp">44:09</span></p>
<p>But TensorFlow also gives you a bunch of convenience functions that compute these common neural network things for you. <span onclick="goVideo(this)" class="timestamp">44:20</span></p>
<p>So in this case we can use tf.losses.mean_squared_error and it just does the L2 loss for us so we don't have to compute it ourself in terms of basic tensor operations. <span onclick="goVideo(this)" class="timestamp">44:26</span></p>
<p>So another kind of weirdness here is that it was kind of annoying that we had to explicitly define our inputs and define our weights and then like chain them together in the forward pass using a matrix multiply. <span onclick="goVideo(this)" class="timestamp">44:36</span></p>
<p>And in this example we've actually not put biases in the layer because that would be kind of an extra, then we'd have to initialize biases, we'd have to get them in the right shape, we'd have to broadcast the biases against the output of the matrix multiply and you can see that that would kind of be a lot of code. <span onclick="goVideo(this)" class="timestamp">44:46</span></p>
<p>It would be kind of annoying write. <span onclick="goVideo(this)" class="timestamp">45:01</span></p>
<p>And once you get to like convolutions and batch normalizations and other types of layers this kind of basic way of working, of having these variables, having these inputs and outputs and combining them all together with basic computational graph operations could be a little bit unwieldy and it could be really annoying to make sure you initialize the weights with the right shapes and all that sort of stuff. <span onclick="goVideo(this)" class="timestamp">45:03</span></p>
<p>So as a result, there's a bunch of sort of higher level libraries that wrap around TensorFlow and handle some of these details for you. <span onclick="goVideo(this)" class="timestamp">45:22</span></p>
<p>So one example that ships with TensorFlow, is this tf.layers inside. <span onclick="goVideo(this)" class="timestamp">45:30</span></p>
<p>So now in this code example you can see that our code is only explicitly declaring the X and the Y which are the placeholders for the data and the labels. <span onclick="goVideo(this)" class="timestamp">45:35</span></p>
<p>And now we say that H=tf.layers.dense, we give it the input X and we tell it units=H. <span onclick="goVideo(this)" class="timestamp">45:44</span></p>
<p>This is again kind of a magical line because inside this line, it's kind of setting up w1 and b1, the bias, it's setting up variables for those with the right shapes that are kind of inside the graph but a little bit hidden from us. <span onclick="goVideo(this)" class="timestamp">45:53</span></p>
<p>And it's using this xavier initializer object to set up an initialization strategy for those. <span onclick="goVideo(this)" class="timestamp">46:07</span></p>
<p>So before we were doing that explicitly ourselves with the tf.randomnormal business, but now here it's kind of handling some of those details for us and it's just spitting out an H, which is again the same sort of H that we saw in the previous layer, it's just doing some of those details for us. <span onclick="goVideo(this)" class="timestamp">46:12</span></p>
<p>And you can see here, we're also passing an activation=tf.nn.relu so it's even doing the activation, the relu activation function inside this layer for us. <span onclick="goVideo(this)" class="timestamp">46:28</span></p>
<p>So it's taking care of a lot of these architectural details for us. <span onclick="goVideo(this)" class="timestamp">46:36</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">46:41</span></p>
<p>[student's words obscured due to lack of microphone] Question is does the xavier initializer default to particular distribution? <span onclick="goVideo(this)" class="timestamp">46:42</span></p>
<p>I'm sure it has some default, I'm not sure what it is. <span onclick="goVideo(this)" class="timestamp">46:51</span></p>
<p>I think you'll have to look at the documentation. <span onclick="goVideo(this)" class="timestamp">46:53</span></p>
<p>But it seems to be a reasonable strategy, I guess. <span onclick="goVideo(this)" class="timestamp">46:55</span></p>
<p>And in fact if you run this code, it converges much faster than the previous one because the initialization is better. <span onclick="goVideo(this)" class="timestamp">46:58</span></p>
<p>And you can see that we're using two calls to tf.layers and this lets us build our model without doing all these explicit bookkeeping details ourself. <span onclick="goVideo(this)" class="timestamp">47:04</span></p>
<p>So this is maybe a little bit more convenient. <span onclick="goVideo(this)" class="timestamp">47:11</span></p>
<p>But tf.contrib.layer is really not the only game in town. <span onclick="goVideo(this)" class="timestamp">47:14</span></p>
<p>There's like a lot of different higher level libraries that people build on top of TensorFlow. <span onclick="goVideo(this)" class="timestamp">47:18</span></p>
<p>And it's kind of due to this basic impotence mis-match where the computational graph is relatively low level thing, but when we're working with neural networks we have this concept of layers and weights and some layers have weights associated with them, and we typically think at a slightly higher level of abstraction than this raw computational graph. <span onclick="goVideo(this)" class="timestamp">47:23</span></p>
<p>So that's what these various packages are trying to help you out and let you work at this higher layer of abstraction. <span onclick="goVideo(this)" class="timestamp">47:41</span></p>
<p>So another very popular package that you may have seen before is Keras. <span onclick="goVideo(this)" class="timestamp">47:48</span></p>
<p>Keras is a very beautiful, nice API that sits on top of TensorFlow and handles sort of building up these computational graph for you up in the back end. <span onclick="goVideo(this)" class="timestamp">47:52</span></p>
<p>By the way, Keras also supports Theano as a back end, so that's also kind of nice. <span onclick="goVideo(this)" class="timestamp">48:02</span></p>
<p>And in this example you can see we build the model as a sequence of layers. <span onclick="goVideo(this)" class="timestamp">48:07</span></p>
<p>We build some optimizer object and we call model.compile and this does a lot of magic in the back end to build the graph. <span onclick="goVideo(this)" class="timestamp">48:10</span></p>
<p>And now we can call model.fit and that does the whole training procedure for us magically. <span onclick="goVideo(this)" class="timestamp">48:17</span></p>
<p>So I don't know all the details of how this works, but I know Keras is very popular, so you might consider using it if you're talking about TensorFlow. <span onclick="goVideo(this)" class="timestamp">48:22</span></p>
<p>Question? <span onclick="goVideo(this)" class="timestamp">48:29</span></p>
<p>[student's words obscured due to lack of microphone] Yeah, so the question is like why there's no explicit CPU, GPU going on here. <span onclick="goVideo(this)" class="timestamp">48:31</span></p>
<p>So I've kind of left that out to keep the code clean. <span onclick="goVideo(this)" class="timestamp">48:45</span></p>
<p>But you saw at the beginning examples it was pretty easy to flop all these things between CPU and GPU and there was either some global flag or some different data type or some with statement and it's usually relatively simple and just about one line to swap in each case. <span onclick="goVideo(this)" class="timestamp">48:48</span></p>
<p>But exactly what that line looks like differs a bit depending on the situation. <span onclick="goVideo(this)" class="timestamp">49:01</span></p>
<p>So there's actually like this whole large set of higher level TensorFlow wrappers that you might see out there in the wild. <span onclick="goVideo(this)" class="timestamp">49:06</span></p>
<p>And it seems that like even people within Google can't really agree on which one is the right one to use. <span onclick="goVideo(this)" class="timestamp">49:14</span></p>
<p>So Keras and TFLearn are third party libraries that are out there on the internet by other people. <span onclick="goVideo(this)" class="timestamp">49:22</span></p>
<p>But there's these three different ones, tf.layers, TF-Slim and tf.contrib.learn that all ship with TensorFlow, that are all kind of doing a slightly different version of this higher level wrapper thing. <span onclick="goVideo(this)" class="timestamp">49:26</span></p>
<p>There's another framework also from Google, but not shipping with TensorFlow called Pretty Tensor that does the same sort of thing. <span onclick="goVideo(this)" class="timestamp">49:39</span></p>
<p>And I guess none of these were good enough for DeepMind, because they went ahead a couple weeks ago and wrote and released their very own high level TensorFlow wrapper called Sonnet. <span onclick="goVideo(this)" class="timestamp">49:46</span></p>
<p>So I wouldn't begrudge you if you were kind of confused by all these things. <span onclick="goVideo(this)" class="timestamp">49:54</span></p>
<p>There's a lot of different choices. <span onclick="goVideo(this)" class="timestamp">49:59</span></p>
<p>They don't always play nicely with each other. <span onclick="goVideo(this)" class="timestamp">50:00</span></p>
<p>But you have a lot of options, so that's good. <span onclick="goVideo(this)" class="timestamp">50:03</span></p>
<p>TensorFlow has pretrained models. <span onclick="goVideo(this)" class="timestamp">50:07</span></p>
<p>There's some examples in TF-Slim, and in Keras. <span onclick="goVideo(this)" class="timestamp">50:09</span></p>
<p>'Cause remember retrained models are super important when you're training your own things. <span onclick="goVideo(this)" class="timestamp">50:11</span></p>
<p>There's also this idea of Tensorboard where you can load up your, I don't want to get into details, but Tensorboard you can add sort of instrumentation to your code and then plot losses and things as you go through the training process. <span onclick="goVideo(this)" class="timestamp">50:15</span></p>
<p>TensorFlow also let's you run distributed where you can break up a computational graph run on different machines. <span onclick="goVideo(this)" class="timestamp">50:27</span></p>
<p>That's super cool but I think probably not anyone outside of Google is really using that to great success these days, but if you do want to run distributed stuff probably TensorFlow is the main game in town for that. <span onclick="goVideo(this)" class="timestamp">50:32</span></p>
<p>A side note is that a lot of the design of TensorFlow is kind of spiritually inspired by this earlier framework called Theano from Montreal. <span onclick="goVideo(this)" class="timestamp">50:44</span></p>
<p>I don't want to go through the details here, just if you go through these slides on your own, you can see that the code for Theano ends up looking very similar to TensorFlow. <span onclick="goVideo(this)" class="timestamp">50:51</span></p>
<p>Where we define some variables, we do some forward pass, we compute some gradients, and we compile some function, then we run the function over and over to train the network. <span onclick="goVideo(this)" class="timestamp">50:59</span></p>
<p>So it kind of looks a lot like TensorFlow. <span onclick="goVideo(this)" class="timestamp">51:08</span></p>
<p>So we still have a lot to get through, so I'm going to move on to PyTorch and maybe take questions at the end. <span onclick="goVideo(this)" class="timestamp">51:10</span></p>
<p>So, PyTorch from Facebook is kind of different from TensorFlow in that we have sort of three explicit different layers of abstraction inside PyTorch. <span onclick="goVideo(this)" class="timestamp">51:16</span></p>
<p>So PyTorch has this tensor object which is just like a Numpy array. <span onclick="goVideo(this)" class="timestamp">51:26</span></p>
<p>It's just an imperative array, it doesn't know anything about deep learning, but it can run with GPU. <span onclick="goVideo(this)" class="timestamp">51:30</span></p>
<p>We have this variable object which is a node in a computational graph which builds up computational graphs, lets you compute gradients, that sort of thing. <span onclick="goVideo(this)" class="timestamp">51:36</span></p>
<p>And we have a module object which is a neural network layer that you can compose together these modules to build big networks. <span onclick="goVideo(this)" class="timestamp">51:44</span></p>
<p>So if you kind of want to think about rough equivalents between PyTorch and TensorFlow you can think of the PyTorch tensor as fulfilling the same role as the Numpy array in TensorFlow. <span onclick="goVideo(this)" class="timestamp">51:50</span></p>
<p>The PyTorch variable is similar to the TensorFlow tensor or variable or placeholder, which are all sort of nodes in a computational graph. <span onclick="goVideo(this)" class="timestamp">52:01</span></p>
<p>And now the PyTorch module is kind of equivalent to these higher level things from tf.slim or tf.layers or sonnet or these other higher level frameworks. <span onclick="goVideo(this)" class="timestamp">52:08</span></p>
<p>So right away one thing to notice about PyTorch is that because it ships with this high level abstraction and like one really nice higher level abstraction called modules on its own, there's sort of less choice involved. <span onclick="goVideo(this)" class="timestamp">52:18</span></p>
<p>Just stick with nnmodules and you'll be good to go. <span onclick="goVideo(this)" class="timestamp">52:29</span></p>
<p>You don't need to worry about which higher level wrapper to use. <span onclick="goVideo(this)" class="timestamp">52:32</span></p>
<p>So PyTorch tensors, as I said, are just like Numpy arrays so here on the right we've done an entire two layer network using entirely PyTorch tensors. <span onclick="goVideo(this)" class="timestamp">52:37</span></p>
<p>One thing to note is that we're not importing Numpy here at all anymore. <span onclick="goVideo(this)" class="timestamp">52:47</span></p>
<p>We're just doing all these operations using PyTorch tensors. <span onclick="goVideo(this)" class="timestamp">52:51</span></p>
<p>And this code looks exactly like the two layer net code that you wrote in Numpy on the first homework. <span onclick="goVideo(this)" class="timestamp">52:53</span></p>
<p>So you set up some random data, you use some operations to compute the forward pass. <span onclick="goVideo(this)" class="timestamp">53:01</span></p>
<p>And then we're explicitly viewing the backward pass ourself. <span onclick="goVideo(this)" class="timestamp">53:07</span></p>
<p>Just sort of backhopping through the network, through the operations, just as you did on homework one. <span onclick="goVideo(this)" class="timestamp">53:10</span></p>
<p>And now we're doing a manual update of the weights using a learning rate and using our computed gradients. <span onclick="goVideo(this)" class="timestamp">53:15</span></p>
<p>But the major difference between the PyTorch tensor and Numpy arrays is that they run on GPU so all you have to do to make this code run on GPU is use a different data type. <span onclick="goVideo(this)" class="timestamp">53:22</span></p>
<p>Rather than using torch.FloatTensor, you do torch.cuda.FloatTensor, cast all of your tensors to this new datatype and everything runs magically on the GPU. <span onclick="goVideo(this)" class="timestamp">53:33</span></p>
<p>You should think of PyTorch tensors as just Numpy plus GPU. <span onclick="goVideo(this)" class="timestamp">53:43</span></p>
<p>That's exactly what it is, nothing specific to deep learning. <span onclick="goVideo(this)" class="timestamp">53:47</span></p>
<p>So the next layer of abstraction in PyTorch is the variable. <span onclick="goVideo(this)" class="timestamp">53:52</span></p>
<p>So this is, once we moved from tensors to variables now we're building computational graphs and we're able to take gradients automatically and everything like that. <span onclick="goVideo(this)" class="timestamp">53:55</span></p>
<p>So here, if X is a variable, then x.data is a tensor and x.grad is another variable containing the gradients of the loss with respect to that tensor. <span onclick="goVideo(this)" class="timestamp">54:03</span></p>
<p>So x.grad.data is an actual tensor containing those gradients. <span onclick="goVideo(this)" class="timestamp">54:14</span></p>
<p>And PyTorch tensors and variables have the exact same API. <span onclick="goVideo(this)" class="timestamp">54:18</span></p>
<p>So any code that worked on PyTorch tensors you can just make them variables instead and run the same code, except now you're building up a computational graph rather than just doing these imperative operations. <span onclick="goVideo(this)" class="timestamp">54:22</span></p>
<p>So here when we create these variables each call to the variable constructor wraps a PyTorch tensor and then also gives a flag whether or not we want to compute gradients with respect to this variable. <span onclick="goVideo(this)" class="timestamp">54:35</span></p>
<p>And now in the forward pass it looks exactly like it did before in the variable in the case with tensors because they have the same API. <span onclick="goVideo(this)" class="timestamp">54:47</span></p>
<p>So now we're computing our predictions, we're computing our loss in kind of this imperative kind of way. <span onclick="goVideo(this)" class="timestamp">54:54</span></p>
<p>And then we call loss.backwards and now all these gradients come out for us. <span onclick="goVideo(this)" class="timestamp">54:59</span></p>
<p>And then we can make a gradient update step on our weights using the gradients that are now present in the w1.grad.data. <span onclick="goVideo(this)" class="timestamp">55:05</span></p>
<p>So this ends up looking quite like the Numpy case, except all the gradients come for free. <span onclick="goVideo(this)" class="timestamp">55:11</span></p>
<p>One thing to note that's kind of different between PyTorch and TensorFlow is that in a TensorFlow case we were building up this explicit graph, then running the graph many times. <span onclick="goVideo(this)" class="timestamp">55:18</span></p>
<p>Here in PyTorch, instead we're building up a new graph every time we do a forward pass. <span onclick="goVideo(this)" class="timestamp">55:27</span></p>
<p>And this makes the code look a bit cleaner. <span onclick="goVideo(this)" class="timestamp">55:32</span></p>
<p>And it has some other implications that we'll get to in a bit. <span onclick="goVideo(this)" class="timestamp">55:33</span></p>
<p>So in PyTorch you can define your own new autograd functions by defining the forward and backward in terms of tensors. <span onclick="goVideo(this)" class="timestamp">55:37</span></p>
<p>This ends up looking kind of like the module layers code that you write for homework two. <span onclick="goVideo(this)" class="timestamp">55:42</span></p>
<p>Where you can implement forward and backward using tensor operations and then stick these things inside computational graph. <span onclick="goVideo(this)" class="timestamp">55:48</span></p>
<p>So here we're defining our own relu and then we can actually go in and use our own relu operation and now stick it inside our computational graph and define our own operations this way. <span onclick="goVideo(this)" class="timestamp">55:54</span></p>
<p>But most of the time you will probably not need to define your own autograd operations. <span onclick="goVideo(this)" class="timestamp">56:05</span></p>
<p>Most of the times the operations you need will mostly be already implemented for you. <span onclick="goVideo(this)" class="timestamp">56:09</span></p>
<p>So in TensorFlow we saw, if we can move to something like Keras or TF.Learn and this gives us a higher level API to work with, rather than this raw computational graphs. <span onclick="goVideo(this)" class="timestamp">56:14</span></p>
<p>The equivalent in PyTorch is the nn package. <span onclick="goVideo(this)" class="timestamp">56:23</span></p>
<p>Where it provides these high level wrappers for working with these things. <span onclick="goVideo(this)" class="timestamp">56:25</span></p>
<p>But unlike TensorFlow there's only one of them. <span onclick="goVideo(this)" class="timestamp">56:31</span></p>
<p>And it works pretty well, so just use that if you're using PyTorch. <span onclick="goVideo(this)" class="timestamp">56:33</span></p>
<p>So here, this ends up kind of looking like Keras where we define our model as some sequence of layers. <span onclick="goVideo(this)" class="timestamp">56:37</span></p>
<p>Our linear and relu operations. <span onclick="goVideo(this)" class="timestamp">56:42</span></p>
<p>And we use some loss function defined in the nn package that's our mean squared error loss. <span onclick="goVideo(this)" class="timestamp">56:44</span></p>
<p>And now inside each iteration of our loop we can run data forward through the model to get our predictions. <span onclick="goVideo(this)" class="timestamp">56:49</span></p>
<p>We can run the predictions forward through the loss function to get our scale or loss, then we can call loss.backward, get all our gradients for free and then loop over the parameters of the models and do our explicit gradient descent step to update the models. <span onclick="goVideo(this)" class="timestamp">56:55</span></p>
<p>And again we see that we're sort of building up this new computational graph every time we do a forward pass. <span onclick="goVideo(this)" class="timestamp">57:07</span></p>
<p>And just like we saw in TensorFlow, PyTorch provides these optimizer operations that kind of abstract away this updating logic and implement fancier update rules like Adam and whatnot. <span onclick="goVideo(this)" class="timestamp">57:12</span></p>
<p>So here we're constructing an optimizer object telling it that we want it to optimize over the parameters of the model. <span onclick="goVideo(this)" class="timestamp">57:23</span></p>
<p>Giving it some learning rate under the hyper parameters. <span onclick="goVideo(this)" class="timestamp">57:28</span></p>
<p>And now after we compute our gradients we can just call optimizer.step and it updates all the parameters of the model for us right here. <span onclick="goVideo(this)" class="timestamp">57:31</span></p>
<p>So another common thing you'll do in PyTorch a lot is define your own nn modules. <span onclick="goVideo(this)" class="timestamp">57:39</span></p>
<p>So typically you'll write your own class which defines you entire model as a single new nn module class. <span onclick="goVideo(this)" class="timestamp">57:44</span></p>
<p>And a module is just kind of a neural network layer that can contain either other other modules or trainable weights or other other kinds of state. <span onclick="goVideo(this)" class="timestamp">57:51</span></p>
<p>So in this case we can redo the two layer net example by defining our own nn module class. <span onclick="goVideo(this)" class="timestamp">58:01</span></p>
<p>So now here in the initializer of the class we're assigning this linear1 and linear2. <span onclick="goVideo(this)" class="timestamp">58:07</span></p>
<p>We're constructing these new module objects and then store them inside of our own class. <span onclick="goVideo(this)" class="timestamp">58:11</span></p>
<p>And now in the forward pass we can use both our own internal modules as well as arbitrary autograd operations on variables to compute the output of our network. <span onclick="goVideo(this)" class="timestamp">58:17</span></p>
<p>So here we receive the, inside this forward method here, the input acts as a variable, then we pass the variable to our self.linear1 for the first layer. <span onclick="goVideo(this)" class="timestamp">58:26</span></p>
<p>We use an autograd op clamp to complete the relu, we pass the output of that to the second linear and then that gives us our output. <span onclick="goVideo(this)" class="timestamp">58:35</span></p>
<p>And now the rest of this code for training this thing looks pretty much the same. <span onclick="goVideo(this)" class="timestamp">58:42</span></p>
<p>Where we build an optimizer and loop over and on ever iteration feed data to the model, compute the gradients with loss.backwards, call optimizer.step. <span onclick="goVideo(this)" class="timestamp">58:46</span></p>
<p>So this is like relatively characteristic of what you might see in a lot of PyTorch type training scenarios. <span onclick="goVideo(this)" class="timestamp">58:54</span></p>
<p>Where you define your own class, defining your own model that contains other modules and whatnot and then you have some explicit training loop like this that runs it and updates it. <span onclick="goVideo(this)" class="timestamp">59:01</span></p>
<p>One kind of nice quality of life thing that you have in PyTorch is a dataloader. <span onclick="goVideo(this)" class="timestamp">59:11</span></p>
<p>So a dataloader can handle building minibatches for you. <span onclick="goVideo(this)" class="timestamp">59:16</span></p>
<p>It can handle some of the multi-threading that we talked about for you, where it can actually use multiple threads in the background to build many batches for you and stream off disk. <span onclick="goVideo(this)" class="timestamp">59:18</span></p>
<p>So here a dataloader wraps a dataset and provides some of these abstractions for you. <span onclick="goVideo(this)" class="timestamp">59:27</span></p>
<p>And in practice when you want to run your own data, you typically will write your own dataset class which knows how to read your particular type of data off whatever source you want and then wrap it in a data loader and train with that. <span onclick="goVideo(this)" class="timestamp">59:33</span></p>
<p>So, here we can see that now we're iterating over the dataloader object and at every iteration this is yielding minibatches of data. <span onclick="goVideo(this)" class="timestamp">59:44</span></p>
<p>And it's internally handling the shuffling of the data and multithreaded dataloading and all this sort of stuff for you. <span onclick="goVideo(this)" class="timestamp">59:52</span></p>
<p>So this is kind of a completely PyTorch example and a lot of PyTorch training code ends up looking something like this. <span onclick="goVideo(this)" class="timestamp">59:58</span></p>
<p>PyTorch provides pretrained models. <span onclick="goVideo(this)" class="timestamp">60:05</span></p>
<p>And this is probably the slickest pretrained model experience I've ever seen. <span onclick="goVideo(this)" class="timestamp">60:07</span></p>
<p>You just say torchvision.models.alexnet pretained=true. <span onclick="goVideo(this)" class="timestamp">60:11</span></p>
<p>That'll go down in the background, download the pretrained weights for you if you don't already have them, and then it's right there, you're good to go. <span onclick="goVideo(this)" class="timestamp">60:14</span></p>
<p>So this is super easy to use. <span onclick="goVideo(this)" class="timestamp">60:21</span></p>
<p>PyTorch also has, there's also a package called Visdom that lets you visualize some of these loss statistics somewhat similar to Tensorboard. <span onclick="goVideo(this)" class="timestamp">60:24</span></p>
<p>So that's kind of nice, I haven't actually gotten a chance to play around with this myself so I can't really speak to how useful it is, but one of the major differences between Tensorboard and Visdom is that Tensorboard actually lets you visualize the structure of the computational graph. <span onclick="goVideo(this)" class="timestamp">60:33</span></p>
<p>Which is really cool, a really useful debugging strategy. <span onclick="goVideo(this)" class="timestamp">60:45</span></p>
<p>And Visdom does not have that functionality yet. <span onclick="goVideo(this)" class="timestamp">60:47</span></p>
<p>But I've never really used this myself so I can't really speak to its utility. <span onclick="goVideo(this)" class="timestamp">60:50</span></p>
<p>As a bit of an aside, PyTorch is kind of an evolution of, kind of a newer updated version of an older framework called Torch which I worked with a lot in the last couple of years. <span onclick="goVideo(this)" class="timestamp">60:56</span></p>
<p>And I don't want to go through the details here, but PyTorch is pretty much better in a lot of ways than the old Lua Torch, but they actually share a lot of the same back end C code for computing with tensors and GPU operations on tensors and whatnot. <span onclick="goVideo(this)" class="timestamp">61:05</span></p>
<p>So if you look through this Torch example, some of it ends up looking kind of similar to PyTorch, some of it's a bit different. <span onclick="goVideo(this)" class="timestamp">61:18</span></p>
<p>Maybe you can step through this offline. <span onclick="goVideo(this)" class="timestamp">61:23</span></p>
<p>But kind of the high level differences between Torch and PyTorch are that Torch is actually in Lua, not Python, unlike these other things. <span onclick="goVideo(this)" class="timestamp">61:25</span></p>
<p>So learning Lua is a bit of a turn off for some people. <span onclick="goVideo(this)" class="timestamp">61:33</span></p>
<p>Torch doesn't have autograd. <span onclick="goVideo(this)" class="timestamp">61:37</span></p>
<p>Torch is also older, so it's more stable, less susceptible to bugs, there's maybe more example code for Torch. <span onclick="goVideo(this)" class="timestamp">61:40</span></p>
<p>They're about the same speeds, that's not really a concern. <span onclick="goVideo(this)" class="timestamp">61:45</span></p>
<p>But in PyTorch it's in Python which is great, you've got autograd which makes it a lot simpler to write complex models. <span onclick="goVideo(this)" class="timestamp">61:47</span></p>
<p>In Lua Torch you end up writing a lot of your own back prop code sometimes, so that's a little bit annoying. <span onclick="goVideo(this)" class="timestamp">61:54</span></p>
<p>But PyTorch is newer, there's less existing code, it's still subject to change. <span onclick="goVideo(this)" class="timestamp">61:59</span></p>
<p>So it's a little bit more of an adventure. <span onclick="goVideo(this)" class="timestamp">62:03</span></p>
<p>But at least for me, I kind of prefer, I don't really see much reason for myself to use Torch over PyTorch anymore at this time. <span onclick="goVideo(this)" class="timestamp">62:06</span></p>
<p>So I'm pretty much using PyTorch exclusively for all my work these days. <span onclick="goVideo(this)" class="timestamp">62:13</span></p>
<p>We talked about this a little bit about this idea of static versus dynamic graphs. <span onclick="goVideo(this)" class="timestamp">62:18</span></p>
<p>And this is one of the main distinguishing features between PyTorch and TensorFlow. <span onclick="goVideo(this)" class="timestamp">62:22</span></p>
<p>So we saw in TensorFlow you have these two stages of operation where first you build up this computational graph, then you run the computational graph over and over again many many times reusing that same graph. <span onclick="goVideo(this)" class="timestamp">62:26</span></p>
<p>That's called a static computational graph 'cause there's only one of them. <span onclick="goVideo(this)" class="timestamp">62:38</span></p>
<p>And we saw PyTorch is quite different where we're actually building up this new computational graph, this new fresh thing on every forward pass. <span onclick="goVideo(this)" class="timestamp">62:42</span></p>
<p>That's called a dynamic computational graph. <span onclick="goVideo(this)" class="timestamp">62:48</span></p>
<p>For kind of simple cases, with kind of feed forward neural networks, it doesn't really make a huge difference, the code ends up kind of similarly and they work kind of similarly, but I do want to talk a bit about some of the implications of static versus dynamic. <span onclick="goVideo(this)" class="timestamp">62:52</span></p>
<p>And what are the tradeoffs of those two. <span onclick="goVideo(this)" class="timestamp">63:04</span></p>
<p>So one kind of nice idea with static graphs is that because we're kind of building up one computational graph once, and then reusing it many times, the framework might have the opportunity to go in and do optimizations on that graph. <span onclick="goVideo(this)" class="timestamp">63:07</span></p>
<p>And kind of fuse some operations, reorder some operations, figure out the most efficient way to operate that graph so it can be really efficient. <span onclick="goVideo(this)" class="timestamp">63:19</span></p>
<p>And because we're going to reuse that graph many times, maybe that optimization process is expensive up front, but we can amortize that cost with the speedups that we've gotten when we run the graph many many times. <span onclick="goVideo(this)" class="timestamp">63:26</span></p>
<p>So as kind of a concrete example, maybe if you write some graph which has convolution and relu operations kind of one after another, you might imagine that some fancy graph optimizer could go in and actually output, like emit custom code which has fused operations, fusing the convolution and the relu so now it's computing the same thing as the code you wrote, but now might be able to be executed more efficiently. <span onclick="goVideo(this)" class="timestamp">63:37</span></p>
<p>So I'm not too sure on exactly what the state in practice of TensorFlow graph optimization is right now, but at least in principle, this is one place where static graph really, you can have the potential for doing this optimization in static graphs where maybe it would be not so tractable for dynamic graphs. <span onclick="goVideo(this)" class="timestamp">64:03</span></p>
<p>Another kind of subtle point about static versus dynamic is this idea of serialization. <span onclick="goVideo(this)" class="timestamp">64:25</span></p>
<p>So with a static graph you can imagine that you write this code that builds up the graph and then once you've built the graph, you have this data structure in memory that represents the entire structure of your network. <span onclick="goVideo(this)" class="timestamp">64:28</span></p>
<p>And now you could take that data structure and just serialize it to disk. <span onclick="goVideo(this)" class="timestamp">64:39</span></p>
<p>And now you've got the whole structure of your network saved in some file. <span onclick="goVideo(this)" class="timestamp">64:42</span></p>
<p>And then you could later rear load that thing and then run that computational graph without access to the original code that built it. <span onclick="goVideo(this)" class="timestamp">64:45</span></p>
<p>So this would be kind of nice in a deployment scenario. <span onclick="goVideo(this)" class="timestamp">64:53</span></p>
<p>You might imagine that you might want to train your network in Python because it's maybe easier to work with, but then after you serialize that network and then you could deploy it now in maybe a C++ environment where you don't need to use the original code that built the graph. <span onclick="goVideo(this)" class="timestamp">64:55</span></p>
<p>So that's kind of a nice advantage of static graphs. <span onclick="goVideo(this)" class="timestamp">65:07</span></p>
<p>Whereas with a dynamic graph, because we're interleaving these processes of graph building and graph execution, you kind of need the original code at all times if you want to reuse that model in the future. <span onclick="goVideo(this)" class="timestamp">65:10</span></p>
<p>On the other hand, some advantages for dynamic graphs are that it kind of makes, it just makes your code a lot cleaner and a lot easier in a lot of scenarios. <span onclick="goVideo(this)" class="timestamp">65:22</span></p>
<p>So for example, suppose that we want to do some conditional operation where depending on the value of some variable Z, we want to do different operations to compute Y. <span onclick="goVideo(this)" class="timestamp">65:29</span></p>
<p>Where if Z is positive, we want to use one weight matrix, if Z is negative we want to use a different weight matrix. <span onclick="goVideo(this)" class="timestamp">65:39</span></p>
<p>And we just want to switch off between these two alternatives. <span onclick="goVideo(this)" class="timestamp">65:45</span></p>
<p>In PyTorch because we're using dynamic graphs, it's super simple. <span onclick="goVideo(this)" class="timestamp">65:47</span></p>
<p>Your code kind of looks exactly like you would expect, exactly what you would do in Numpy. <span onclick="goVideo(this)" class="timestamp">65:52</span></p>
<p>You can just use normal Python control flow to handle this thing. <span onclick="goVideo(this)" class="timestamp">65:56</span></p>
<p>And now because we're building up the graph each time, each time we perform this operation will take one of the two paths and build up maybe a different graph on each forward pass, but for any graph that we do end up building up, we can back propagate through it just fine. <span onclick="goVideo(this)" class="timestamp">66:00</span></p>
<p>And the code is very clean, easy to work with. <span onclick="goVideo(this)" class="timestamp">66:14</span></p>
<p>Now in TensorFlow the situations is a little bit more complicated because we build the graph once, this control flow operator kind of needs to be an explicit operator in the TensorFlow graph. <span onclick="goVideo(this)" class="timestamp">66:15</span></p>
<p>And now, so them you can see that we have this tf.cond call which is kind of like a TensorFlow version of an if statement, but now it's baked into the computational graph rather than using sort of Python control flow. <span onclick="goVideo(this)" class="timestamp">66:28</span></p>
<p>And the problem is that because we only build the graph once, all the potential paths of control flow that our program might flow through need to be baked into the graph at the time we construct it before we ever run it. <span onclick="goVideo(this)" class="timestamp">66:40</span></p>
<p>So that means that any kind of control flow operators that you want to have need to be not Python control flow operators, you need to use some kind of magic, special tensor flow operations to do control flow. <span onclick="goVideo(this)" class="timestamp">66:52</span></p>
<p>In this case this tf.cond. <span onclick="goVideo(this)" class="timestamp">67:03</span></p>
<p>Another kind of similar situation happens if you want to have loops. <span onclick="goVideo(this)" class="timestamp">67:06</span></p>
<p>So suppose that we want to compute some kind of recurrent relationships where maybe Y T is equal to Y T minus one plus X T times some weight matrix W and depending on each time we do this, every time we compute this, we might have a different sized sequence of data. <span onclick="goVideo(this)" class="timestamp">67:10</span></p>
<p>And no matter the length of our sequence of data, we just want to compute this same recurrence relation no matter the size of the input sequence. <span onclick="goVideo(this)" class="timestamp">67:26</span></p>
<p>So in PyTorch this is super easy. <span onclick="goVideo(this)" class="timestamp">67:33</span></p>
<p>We can just kind of use a normal for loop in Python to just loop over the number of times that we want to unroll and now depending on the size of the input data, our computational graph will end up as different sizes, but that's fine, we can just back propagate through each one, one at a time. <span onclick="goVideo(this)" class="timestamp">67:35</span></p>
<p>Now in TensorFlow this becomes a little bit uglier. <span onclick="goVideo(this)" class="timestamp">67:51</span></p>
<p>And again, because we need to construct the graph all at once up front, this control flow looping construct again needs to be an explicit node in the TensorFlow graph. <span onclick="goVideo(this)" class="timestamp">67:55</span></p>
<p>So I hope you remember your functional programming because you'll have to use those kinds of operators to implement looping constructs in TensorFlow. <span onclick="goVideo(this)" class="timestamp">68:06</span></p>
<p>So in this case, for this particular recurrence relationship you can use a foldl operation and pass in, sort of implement this particular loop in terms of a foldl. <span onclick="goVideo(this)" class="timestamp">68:13</span></p>
<p>But what this basically means is that you have this sense that TensorFlow is almost building its own entire programming language, using the language of computational graphs. <span onclick="goVideo(this)" class="timestamp">68:24</span></p>
<p>And any kind of control flow operator, or any kind of data structure needs to be rolled into the computational graph so you can't really utilize all your favorite paradigms for working imperatively in Python. <span onclick="goVideo(this)" class="timestamp">68:33</span></p>
<p>You kind of need to relearn a whole separate set of control flow operators. <span onclick="goVideo(this)" class="timestamp">68:44</span></p>
<p>And if you want to do any kinds of control flow inside your computational graph using TensorFlow. <span onclick="goVideo(this)" class="timestamp">68:47</span></p>
<p>So at least for me, I find that kind of confusing, a little bit hard to wrap my head around sometimes, and I kind of like that using PyTorch dynamic graphs, you can just use your favorite imperative programming constructs and it all works just fine. <span onclick="goVideo(this)" class="timestamp">68:52</span></p>
<p>By the way, there actually is some very new library called TensorFlow Fold which is another one of these layers on top of TensorFlow that lets you implement dynamic graphs, you kind of write your own code using TensorFlow Fold that looks kind of like a dynamic graph operation and then TensorFlow Fold does some magic for you and somehow implements that in terms of the static TensorFlow graphs. <span onclick="goVideo(this)" class="timestamp">69:07</span></p>
<p>This is a super new paper that's being presented at ICLR this week in France. <span onclick="goVideo(this)" class="timestamp">69:32</span></p>
<p>So I haven't had the chance to like dive in and play with this yet. <span onclick="goVideo(this)" class="timestamp">69:37</span></p>
<p>But my initial impression was that it does add some amount of dynamic graphs to TensorFlow but it is still a bit more awkward to work with than the sort of native dynamic graphs you have in PyTorch. <span onclick="goVideo(this)" class="timestamp">69:41</span></p>
<p>So then, I thought it might be nice to motivate like why would we care about dynamic graphs in general? <span onclick="goVideo(this)" class="timestamp">69:51</span></p>
<p>So one option is recurrent networks. <span onclick="goVideo(this)" class="timestamp">69:57</span></p>
<p>So you can see that for something like image captioning we use a recurrent network which operates over sequences of different lengths. <span onclick="goVideo(this)" class="timestamp">70:01</span></p>
<p>In this case, the sentence that we want to generate as a caption is a sequence and that sequence can vary depending on our input data. <span onclick="goVideo(this)" class="timestamp">70:07</span></p>
<p>So now you can see that we have this dynamism in the thing where depending on the size of the sentence, our computational graph might need to have more or fewer elements. <span onclick="goVideo(this)" class="timestamp">70:15</span></p>
<p>So that's one kind of common application of dynamic graphs. <span onclick="goVideo(this)" class="timestamp">70:25</span></p>
<p>For those of you who took CS224N last quarter, you saw this idea of recursive networks where sometimes in natural language processing you might, for example, compute a parsed tree of a sentence and then you want to have a neural network kind of operate recursively up this parse tree. <span onclick="goVideo(this)" class="timestamp">70:29</span></p>
<p>So having a neural network that kind of works, it's not just a sequential sequence of layers, but instead it's kind of working over some graph or tree structure instead where now each data point might have a different graph or tree structure so the structure of the computational graph then kind of mirrors the structure of the input data. <span onclick="goVideo(this)" class="timestamp">70:47</span></p>
<p>And it could vary from data point to data point. <span onclick="goVideo(this)" class="timestamp">71:03</span></p>
<p>So this type of thing seems kind of complicated and hairy to implement using TensorFlow, but in PyTorch you can just kind of use like normal Python control flow and it'll work out just fine. <span onclick="goVideo(this)" class="timestamp">71:05</span></p>
<p>Another bit of more researchy application is this really cool idea that I like called neuromodule networks for visual question answering. <span onclick="goVideo(this)" class="timestamp">71:16</span></p>
<p>So here the idea is that we want to ask some questions about images where we maybe input this image of cats and dogs, there's some question, what color is the cat, and then internally the system can read the question and that has these different specialized neural network modules for performing operations like asking for colors and finding cats. <span onclick="goVideo(this)" class="timestamp">71:23</span></p>
<p>And then depending on the text of the question, it can compile this custom architecture for answering the question. <span onclick="goVideo(this)" class="timestamp">71:43</span></p>
<p>And now if we asked a different question, like are there more cats than dogs? <span onclick="goVideo(this)" class="timestamp">71:49</span></p>
<p>Now we have maybe the same basic set of modules for doing things like finding cats and dogs and counting, but they're arranged in a different order. <span onclick="goVideo(this)" class="timestamp">71:55</span></p>
<p>So we get this dynamism again where different data points might give rise to different computational graphs. <span onclick="goVideo(this)" class="timestamp">72:03</span></p>
<p>But this is a bit more of a researchy thing and maybe not so main stream right now. <span onclick="goVideo(this)" class="timestamp">72:07</span></p>
<p>But as kind of a bigger point, I think that there's a lot of cool, creative applications that people could do with dynamic computational graphs and maybe there aren't so many right now, just because it's been so painful to work with them. <span onclick="goVideo(this)" class="timestamp">72:12</span></p>
<p>So I think that there's a lot of opportunity for doing cool, creative things with dynamic computational graphs. <span onclick="goVideo(this)" class="timestamp">72:23</span></p>
<p>And maybe if you come up with cool ideas, we'll feature it in lecture next year. <span onclick="goVideo(this)" class="timestamp">72:30</span></p>
<p>So I wanted to talk very briefly about Caffe which is this framework from Berkeley. <span onclick="goVideo(this)" class="timestamp">72:34</span></p>
<p>Which Caffe is somewhat different from the other deep learning frameworks where you in many cases you can actually train networks without writing any code yourself. <span onclick="goVideo(this)" class="timestamp">72:39</span></p>
<p>You kind of just call into these pre-existing binaries, set up some configuration files and in many cases you can train on data without writing any of your own code. <span onclick="goVideo(this)" class="timestamp">72:48</span></p>
<p>So, you may be first, you convert your data into some format like HDF5 or LMDB and there exists some scripts inside Caffe that can just convert like folders of images and text files into these formats for you. <span onclick="goVideo(this)" class="timestamp">72:56</span></p>
<p>You need to define, now instead of writing code to define the structure of your computational graph, instead you edit some text file called a prototxt which sets up the structure of the computational graph. <span onclick="goVideo(this)" class="timestamp">73:08</span></p>
<p>Here the structure is that we read from some input HDF5 file, we perform some inner product, we compute some loss and the whole structure of the graph is set up in this text file. <span onclick="goVideo(this)" class="timestamp">73:19</span></p>
<p>One kind of downside here is that these files can get really ugly for very large networks. <span onclick="goVideo(this)" class="timestamp">73:30</span></p>
<p>So for something like the 152 layer ResNet model, which by the way was trained in Caffe originally, then this prototxt file ends up almost 7000 lines long. <span onclick="goVideo(this)" class="timestamp">73:35</span></p>
<p>So people are not writing these by hand. <span onclick="goVideo(this)" class="timestamp">73:44</span></p>
<p>People will sometimes will like write python scripts to generate these prototxt files. <span onclick="goVideo(this)" class="timestamp">73:47</span></p>
<p>[laughter] Then you're kind in the realm of rolling your own computational graph abstraction. <span onclick="goVideo(this)" class="timestamp">73:51</span></p>
<p>That's probably not a good idea, but I've seen that before. <span onclick="goVideo(this)" class="timestamp">73:56</span></p>
<p>Then, rather than having some optimizer object, instead there's some solver, you define some solver things inside another prototxt. <span onclick="goVideo(this)" class="timestamp">73:58</span></p>
<p>This defines your learning rate, your optimization algorithm and whatnot. <span onclick="goVideo(this)" class="timestamp">74:07</span></p>
<p>And then once you do all these things, you can just run the Caffe binary with the train command and it all happens magically. <span onclick="goVideo(this)" class="timestamp">74:11</span></p>
<p>Cafee has a model zoo with a bunch of pretrained models, that's pretty useful. <span onclick="goVideo(this)" class="timestamp">74:17</span></p>
<p>Caffe has a Python interface but it's not super well documented. <span onclick="goVideo(this)" class="timestamp">74:21</span></p>
<p>You kind of need to read the source code of the python interface to see what it can do, so that's kind of annoying. <span onclick="goVideo(this)" class="timestamp">74:25</span></p>
<p>But it does work. <span onclick="goVideo(this)" class="timestamp">74:30</span></p>
<p>So, kind of my general thing about Caffe is that it's maybe good for feed forward models, it's maybe good for production scenarios, because it doesn't depend on Python. <span onclick="goVideo(this)" class="timestamp">74:31</span></p>
<p>But probably for research these days, I've seen Caffe being used maybe a little bit less. <span onclick="goVideo(this)" class="timestamp">74:42</span></p>
<p>Although I think it is still pretty commonly used in industry again for production. <span onclick="goVideo(this)" class="timestamp">74:47</span></p>
<p>I promise one slide, one or two slides on Caffe 2. <span onclick="goVideo(this)" class="timestamp">74:51</span></p>
<p>So Caffe 2 is the successor to Caffe which is from Facebook. <span onclick="goVideo(this)" class="timestamp">74:54</span></p>
<p>It's super new, it was only released a week ago. <span onclick="goVideo(this)" class="timestamp">74:58</span></p>
<p>[laughter] So I really haven't had the time to form a super educated opinion about Caffe 2 yet, but it uses static graphs kind of similar to TensorFlow. <span onclick="goVideo(this)" class="timestamp">75:02</span></p>
<p>Kind of like Caffe one the core is written in C++ and they have some Python interface. <span onclick="goVideo(this)" class="timestamp">75:12</span></p>
<p>The difference is that now you no longer need to write your own Python scripts to generate prototxt files. <span onclick="goVideo(this)" class="timestamp">75:17</span></p>
<p>You can kind of define your computational graph structure all in Python, kind of looking with an API that looks kind of like TensorFlow. <span onclick="goVideo(this)" class="timestamp">75:21</span></p>
<p>But then you can spit out, you can serialize this computational graph structure to a prototxt file. <span onclick="goVideo(this)" class="timestamp">75:29</span></p>
<p>And then once your model is trained and whatnot, then we get this benefit that we talked about of static graphs where you can, you don't need the original training code now in order to deploy a trained model. <span onclick="goVideo(this)" class="timestamp">75:34</span></p>
<p>So one interesting thing is that you've seen Google maybe has one major deep running framework, which is TensorFlow, where Facebook has these two, PyTorch and Caffe 2. <span onclick="goVideo(this)" class="timestamp">75:43</span></p>
<p>So these are kind of different philosophies. <span onclick="goVideo(this)" class="timestamp">75:54</span></p>
<p>Google's kind of trying to build one framework to rule them all that maybe works for every possible scenario for deep learning. <span onclick="goVideo(this)" class="timestamp">75:57</span></p>
<p>This is kind of nice because it consolidates all efforts onto one framework. <span onclick="goVideo(this)" class="timestamp">76:02</span></p>
<p>It means you only need to learn one thing and it'll work across many different scenarios including like distributed systems, production, deployment, mobile, research, everything. <span onclick="goVideo(this)" class="timestamp">76:06</span></p>
<p>Only need to learn one framework to do all these things. <span onclick="goVideo(this)" class="timestamp">76:13</span></p>
<p>Whereas Facebook is taking a bit of a different approach. <span onclick="goVideo(this)" class="timestamp">76:15</span></p>
<p>Where PyTorch is really more specialized, more geared towards research so in terms of writing research code and quickly iterating on your ideas, that's super easy in PyTorch, but for things like running in production, running on mobile devices, PyTorch doesn't have a lot of great support. <span onclick="goVideo(this)" class="timestamp">76:18</span></p>
<p>Instead, Caffe 2 is kind of geared toward those more production oriented use cases. <span onclick="goVideo(this)" class="timestamp">76:32</span></p>
<p>So my kind of general study, my general, overall advice about like which framework to use for which problems is kind of that both, I think TensorFlow is a pretty safe bet for just about any project that you want to start new, right? <span onclick="goVideo(this)" class="timestamp">76:39</span></p>
<p>Because it is sort of one framework to rule them all, it can be used for just about any circumstance. <span onclick="goVideo(this)" class="timestamp">76:53</span></p>
<p>However, you probably need to pair it with a higher level wrapper and if you want dynamic graphs, you're maybe out of luck. <span onclick="goVideo(this)" class="timestamp">76:58</span></p>
<p>Some of the code ends up looking a little bit uglier in my opinion, but maybe that's kind of a cosmetic detail and it doesn't really matter that much. <span onclick="goVideo(this)" class="timestamp">77:05</span></p>
<p>I personally think PyTorch is really great for research. <span onclick="goVideo(this)" class="timestamp">77:13</span></p>
<p>If you're focused on just writing research code, I think PyTorch is a great choice. <span onclick="goVideo(this)" class="timestamp">77:15</span></p>
<p>But it's a bit newer, has less community support, less code out there, so it could be a bit of an adventure. <span onclick="goVideo(this)" class="timestamp">77:21</span></p>
<p>If you want more of a well trodden path, TensorFlow might be a better choice. <span onclick="goVideo(this)" class="timestamp">77:25</span></p>
<p>If you're interested in production deployment, you should probably look at Caffe, Caffe 2 or TensorFlow. <span onclick="goVideo(this)" class="timestamp">77:29</span></p>
<p>And if you're really focused on mobile deployment, I think TensorFlow and Caffe 2 both have some built in support for that. <span onclick="goVideo(this)" class="timestamp">77:34</span></p>
<p>So it's kind of unfortunately, there's not just like one global best framework, it kind of depends on what you're actually trying to do, what applications you anticipate but theses are kind of my general advice on those things. <span onclick="goVideo(this)" class="timestamp">77:41</span></p>
<p>So next time we'll talk about some case studies about various CNN architectures. <span onclick="goVideo(this)" class="timestamp">77:53</span></p>
		</div>
	</body>
</html>
